{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ApqIWlIG55aImbl5FIdsAX8ucYx937Hn","authorship_tag":"ABX9TyMLDe7mAxrNjL3UTNWrKcOx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3712d3ab70cb4feeb719be982bc4b489":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c69e5becd1a644e2a59a4f6f450402d0","IPY_MODEL_29daa24a61064a819bccbc713a8b53b2","IPY_MODEL_a46ec26a36c94751918cba61f91ed3cb"],"layout":"IPY_MODEL_3167d938ae1b485ab65658ea9fea3478"}},"c69e5becd1a644e2a59a4f6f450402d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98ab0a6bce894e16a3747ce372a1e7a3","placeholder":"​","style":"IPY_MODEL_5599211bb5254c2d9b9c986f40180555","value":"100%"}},"29daa24a61064a819bccbc713a8b53b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_76fab78e169c49dc8e2c84e2c8227c92","max":111898327,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d80929f634340a2a067d7702eb98bd3","value":111898327}},"a46ec26a36c94751918cba61f91ed3cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0d5aabb9a58407191a9cd61f3cf3c3a","placeholder":"​","style":"IPY_MODEL_5d52d1d1ee3548729247dd39ea5e4103","value":" 107M/107M [00:00&lt;00:00, 232MB/s]"}},"3167d938ae1b485ab65658ea9fea3478":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98ab0a6bce894e16a3747ce372a1e7a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5599211bb5254c2d9b9c986f40180555":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76fab78e169c49dc8e2c84e2c8227c92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d80929f634340a2a067d7702eb98bd3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0d5aabb9a58407191a9cd61f3cf3c3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d52d1d1ee3548729247dd39ea5e4103":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5D1pNR3lwFAV","executionInfo":{"status":"ok","timestamp":1687166212217,"user_tz":-420,"elapsed":6090,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"f9c09d87-eb99-4599-a09d-86d3902ec302"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting facenet-pytorch\n","  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.27.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.15.2+cu118)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (8.4.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.4)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision->facenet-pytorch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision->facenet-pytorch) (1.3.0)\n","Installing collected packages: facenet-pytorch\n","Successfully installed facenet-pytorch-2.5.3\n"]}],"source":["!pip install facenet-pytorch"]},{"cell_type":"code","source":["from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n","import torch\n","from torch.utils.data import DataLoader, SubsetRandomSampler\n","from torch import optim\n","from torch.optim.lr_scheduler import MultiStepLR\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets, transforms\n","import numpy as np\n","import os\n","from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n","from sklearn import svm\n","from PIL import Image\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"4jWAGjtZw1EK","executionInfo":{"status":"ok","timestamp":1687166291937,"user_tz":-420,"elapsed":11864,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["**Mounted Drive**"],"metadata":{"id":"aB5J_BsZw5LB"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_shWa7piw6Tf","executionInfo":{"status":"ok","timestamp":1687166334871,"user_tz":-420,"elapsed":31667,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"db0971de-2231-4332-9f58-62c17d44ac90"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/faceRecognition/dataset/FacenetDataset.zip -d dataset_new"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHbjV9t1w_KY","executionInfo":{"status":"ok","timestamp":1687166340952,"user_tz":-420,"elapsed":3000,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"6b2de84b-891e-4761-b799-48f0e33ab37d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/faceRecognition/dataset/FacenetDataset.zip\n","   creating: dataset_new/FacenetDataset/\n","  inflating: dataset_new/__MACOSX/._FacenetDataset  \n","  inflating: dataset_new/FacenetDataset/.DS_Store  \n","  inflating: dataset_new/__MACOSX/FacenetDataset/._.DS_Store  \n","   creating: dataset_new/FacenetDataset/Phong/\n","  inflating: dataset_new/__MACOSX/FacenetDataset/._Phong  \n","   creating: dataset_new/FacenetDataset/Hellas/\n","  inflating: dataset_new/__MACOSX/FacenetDataset/._Hellas  \n","   creating: dataset_new/FacenetDataset/Nguyen/\n","  inflating: dataset_new/__MACOSX/FacenetDataset/._Nguyen  \n","   creating: dataset_new/FacenetDataset/Huy/\n","  inflating: dataset_new/__MACOSX/FacenetDataset/._Huy  \n","   creating: dataset_new/FacenetDataset/Phu/\n","  inflating: dataset_new/__MACOSX/FacenetDataset/._Phu  \n","  inflating: dataset_new/FacenetDataset/Phong/8.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/9.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/14.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/28.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/29.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/15.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/17.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/script_rename.py  \n","  inflating: dataset_new/__MACOSX/FacenetDataset/Phong/._script_rename.py  \n","  inflating: dataset_new/FacenetDataset/Phong/16.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/12.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/11.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/10.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/21.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/34.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/20.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/22.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/23.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/33.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/27.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/26.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/32.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/18.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/24.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/30.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/31.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/25.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/19.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/4.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/5.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/7.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/6.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/2.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/3.jpg  \n","  inflating: dataset_new/FacenetDataset/Phong/1.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946351201_7b03fe2053ff3d4c519fd693e296431e.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946392970_2fef845d299ec50f9bcd8b061fc8082e.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946395036_69751dce00ad2907e48d2bb567846617.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946337250_e2952e7c6f55d4130924103a4af7d69e.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946345056_1dc53a628d2b5285b1e1706bce282eb0.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946382101_d854b7eb32e3f6840487ed365c4363cd.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946351946_71bf2fcba5ee8c5a340536307add03b3.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946331843_f52d1fdf66443c20221366fe15e684de.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946329487_0bc341657d4cd6e5f8c4a3cdced6a473.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946320295_903d7490fc28499652fb1db63c8139ca.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946370049_69a7fa78763299b3ce96902cd841dddd.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946337997_7cc8336b72385b95bbbe45e76f3850b0.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946339595_6147f2270ceeab8fe6e450bc020eb428.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/script_rename.py  \n","  inflating: dataset_new/__MACOSX/FacenetDataset/Hellas/._script_rename.py  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946364863_64f4fa3b17ba38ca25eac6a04d0a4d76.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946380040_19e4255a062d7b9e8f20b794c1e44109.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946352121_e7b6fa28181f9754e872351bad0612ce.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946344703_e42b358933493ae9cb5865b7cfd3c841.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946388448_a594ff6fd4117380695f4ae5be89229b.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946362560_837363a994db83ca8273f48b8171737f.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946369194_17e954000bef9958dbc01bf188d61370.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946375073_3e74522e72dd953a20d1feb6776864b0.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946357038_65e005498d95bed1ebf6f6923ebfc0a1.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946396871_66139e019ebdc7bd95532720f9e9d852.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946477746_e065bc9a249071241c5e65d37b2f1127.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946363323_952948f29ecda87e075534f1173d164a.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946358065_ce2b05724ed5e25212c73b4cfffd528a.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946322989_55b6073bf74e82bd388e8cc5f61c4928.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946376392_37b9ac19fe447598bd669f2bf0f75b3b.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946330772_f1198bec1275bcdffe6965a8407737ec.jpg  \n","  inflating: dataset_new/FacenetDataset/Hellas/z3340946384632_9d1963df7388b11c2d8de74d8636a968.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/.DS_Store  \n","  inflating: dataset_new/__MACOSX/FacenetDataset/Nguyen/._.DS_Store  \n","  inflating: dataset_new/FacenetDataset/Nguyen/8.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/9.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/14.jpg  \n","  inflating: dataset_new/__MACOSX/FacenetDataset/Nguyen/._14.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/28.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/29.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/15.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/17.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/script_rename.py  \n","  inflating: dataset_new/FacenetDataset/Nguyen/16.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/12.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/13.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/10.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/21.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/20.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/22.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/23.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/27.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/26.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/18.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/24.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/30.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/31.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/25.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/19.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/4.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/5.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/7.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/6.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/2.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/3.jpg  \n","  inflating: dataset_new/FacenetDataset/Nguyen/1.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/8.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/9.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/14.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/28.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/29.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/15.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/17.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/script_rename.py  \n","  inflating: dataset_new/__MACOSX/FacenetDataset/Huy/._script_rename.py  \n","  inflating: dataset_new/FacenetDataset/Huy/12.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/13.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/11.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/10.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/21.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/20.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/22.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/23.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/27.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/26.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/18.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/24.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/30.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/31.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/25.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/19.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/4.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/5.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/7.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/6.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/2.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/3.jpg  \n","  inflating: dataset_new/FacenetDataset/Huy/1.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/8.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/9.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/14.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/28.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/29.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/15.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/17.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/script_rename.py  \n","  inflating: dataset_new/__MACOSX/FacenetDataset/Phu/._script_rename.py  \n","  inflating: dataset_new/FacenetDataset/Phu/16.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/13.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/11.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/10.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/21.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/20.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/22.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/23.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/27.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/26.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/18.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/24.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/30.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/31.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/25.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/19.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/4.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/5.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/7.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/6.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/2.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/3.jpg  \n","  inflating: dataset_new/FacenetDataset/Phu/1.jpg  \n"]}]},{"cell_type":"code","source":["!rm -r /content/dataset_new/__MACOSX\n","!rm -r /content/dataset_new/FacenetDataset/.DS_Store"],"metadata":{"id":"jwD4O_fTxL9O","executionInfo":{"status":"ok","timestamp":1687166361322,"user_tz":-420,"elapsed":346,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","for path in os.listdir('/content/dataset_new/FacenetDataset'):\n","  for path_child in os.listdir('/content/dataset_new/FacenetDataset/' + path):\n","      if '.py' in path_child:\n","        os.remove(f'/content/dataset_new/FacenetDataset/{path}/{path_child}')"],"metadata":{"id":"ce1X8jIcxMJr","executionInfo":{"status":"ok","timestamp":1687166362935,"user_tz":-420,"elapsed":303,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","epochs = 20\n","workers = 0 if os.name == 'nt' else 8"],"metadata":{"id":"rcYBJg9Zy6qK","executionInfo":{"status":"ok","timestamp":1687166364567,"user_tz":-420,"elapsed":1,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["**Facenet + SVM**"],"metadata":{"id":"k-R7SXw74Ub1"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","mtcnn = MTCNN(\n","    image_size=160, margin=0, min_face_size=20,\n","    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n","    device=device\n",")\n","\n","facenet = InceptionResnetV1(pretrained='vggface2').eval()\n","facenet = facenet.to(device)"],"metadata":{"id":"IjUmalVD4L15","executionInfo":{"status":"ok","timestamp":1687166377078,"user_tz":-420,"elapsed":2002,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["3712d3ab70cb4feeb719be982bc4b489","c69e5becd1a644e2a59a4f6f450402d0","29daa24a61064a819bccbc713a8b53b2","a46ec26a36c94751918cba61f91ed3cb","3167d938ae1b485ab65658ea9fea3478","98ab0a6bce894e16a3747ce372a1e7a3","5599211bb5254c2d9b9c986f40180555","76fab78e169c49dc8e2c84e2c8227c92","7d80929f634340a2a067d7702eb98bd3","d0d5aabb9a58407191a9cd61f3cf3c3a","5d52d1d1ee3548729247dd39ea5e4103"]},"outputId":"f79a9bd1-7633-4b99-e59c-3d50942a712f"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/107M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3712d3ab70cb4feeb719be982bc4b489"}},"metadata":{}}]},{"cell_type":"code","source":["def whitens(img):\n","    mean = img.mean()\n","    std = img.std()\n","    std_adj = std.clamp(min=1.0 / (float(img.numel()) ** 0.5))\n","    y = (img - mean) / std_adj\n","    return y\n","\n","def extract_features(mtcnn, facenet, img):\n","    img = img.to(device)\n","    img = transforms.ToPILImage()(img.squeeze_(0))\n","    bbs, _ = mtcnn.detect(img)\n","    if bbs is None:\n","        # if no face is detected\n","        return None, None\n","\n","    faces = torch.stack([extract_face(img, bb) for bb in bbs])\n","    embeddings = facenet(whitens(faces)).detach().numpy()\n","\n","    return bbs, embeddings\n","\n","def dataset_to_embeddings(dataset, mtcnn, facenet):\n","    transform = transforms.Compose([\n","        transforms.Resize(160),\n","        transforms.ToTensor()\n","    ])\n","\n","    embeddings = []\n","    labels = []\n","    for img_path, label in dataset.samples:\n","        print(img_path)\n","\n","        _, embedding = extract_features(mtcnn, facenet, transform(Image.open(img_path).convert('RGB')).unsqueeze_(0))\n","        if embedding is None:\n","            print(\"Could not find face on {}\".format(img_path))\n","            continue\n","        if embedding.shape[0] > 1:\n","            print(\"Multiple faces detected for {}, taking one with highest probability\".format(img_path))\n","            embedding = embedding[0, :]\n","        embeddings.append(embedding.flatten())\n","        labels.append(label)\n","\n","    return np.stack(embeddings), labels\n","\n","def train(embeddings, labels):\n","    clf = svm.SVC(probability=True)\n","    clf.fit(embeddings, labels)\n","    return clf"],"metadata":{"id":"fcRIpT2U4Y2q","executionInfo":{"status":"ok","timestamp":1687166377389,"user_tz":-420,"elapsed":2,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["**Create adapt dataset**"],"metadata":{"id":"VJRgbmMq4cTm"}},{"cell_type":"code","source":["import shutil"],"metadata":{"id":"JMIdVwe94zdS","executionInfo":{"status":"ok","timestamp":1687166380750,"user_tz":-420,"elapsed":306,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["!mkdir /content/dataset\n","!mkdir /content/dataset/train\n","!mkdir /content/dataset/val\n","\n","!mkdir /content/dataset/train/Phu\n","!mkdir /content/dataset/train/Phong\n","!mkdir /content/dataset/train/Nguyen\n","!mkdir /content/dataset/train/Huy\n","!mkdir /content/dataset/train/Hellas\n","\n","!mkdir /content/dataset/val/Phu\n","!mkdir /content/dataset/val/Phong\n","!mkdir /content/dataset/val/Nguyen\n","!mkdir /content/dataset/val/Huy\n","!mkdir /content/dataset/val/Hellas"],"metadata":{"id":"3lFCM1oI8OdM","executionInfo":{"status":"ok","timestamp":1687166384379,"user_tz":-420,"elapsed":1474,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["for idx,path in enumerate(os.listdir('/content/dataset_new/FacenetDataset/Phu')):\n","    if idx < 5:\n","      shutil.move('/content/dataset_new/FacenetDataset/Phu/' + path, '/content/dataset/val/Phu')\n","    else:\n","      shutil.move('/content/dataset_new/FacenetDataset/Phu/' + path, '/content/dataset/train/Phu')"],"metadata":{"id":"0b65PnnC5Tyl","executionInfo":{"status":"ok","timestamp":1687166405645,"user_tz":-420,"elapsed":482,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def move_file(name):\n","  for idx,path in enumerate(os.listdir(f'/content/dataset_new/FacenetDataset/{name}')):\n","    if idx < 5:\n","      shutil.move(f'/content/dataset_new/FacenetDataset/{name}/' + path, f'/content/dataset/val/{name}')\n","    else:\n","      shutil.move(f'/content/dataset_new/FacenetDataset/{name}/' + path, f'/content/dataset/train/{name}')\n","move_file('Hellas')\n","move_file('Phong')\n","move_file('Nguyen')\n","move_file('Huy')"],"metadata":{"id":"0rPqZidc7q_h","executionInfo":{"status":"ok","timestamp":1687166411557,"user_tz":-420,"elapsed":1,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["!rm -r /content/dataset/train/.ipynb_checkpoints\n","!rm -r /content/dataset/val/.ipynb_checkpoints"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxsXvlGX89iR","executionInfo":{"status":"ok","timestamp":1687166414931,"user_tz":-420,"elapsed":317,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"7a9191b3-30f1-496a-e849-9fe310562471"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/dataset/train/.ipynb_checkpoints': No such file or directory\n","rm: cannot remove '/content/dataset/val/.ipynb_checkpoints': No such file or directory\n"]}]},{"cell_type":"code","source":["dataset_train = datasets.ImageFolder(root=\"/content/dataset/train\")\n","dataset_val = datasets.ImageFolder(root=\"/content/dataset/val\")"],"metadata":{"id":"mlJplUec4acH","executionInfo":{"status":"ok","timestamp":1687166416613,"user_tz":-420,"elapsed":2,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["X_train, y_train = dataset_to_embeddings(dataset_train, mtcnn, facenet)\n","X_test, y_test = dataset_to_embeddings(dataset_val, mtcnn, facenet)\n","\n","X_train_class_idx = dataset_train.class_to_idx\n","X_test_class_idx = dataset_val.class_to_idx\n","\n","embeddings, labels, class_to_idx = X_train, y_train, X_train_class_idx"],"metadata":{"id":"cWeGr7tD9U-3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687166474853,"user_tz":-420,"elapsed":56506,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"a63d1af3-4bbe-48c8-ade1-894a2832cd74"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/dataset/train/Hellas/z3340946320295_903d7490fc28499652fb1db63c8139ca.jpg\n","/content/dataset/train/Hellas/z3340946322989_55b6073bf74e82bd388e8cc5f61c4928.jpg\n","/content/dataset/train/Hellas/z3340946329487_0bc341657d4cd6e5f8c4a3cdced6a473.jpg\n","/content/dataset/train/Hellas/z3340946331843_f52d1fdf66443c20221366fe15e684de.jpg\n","/content/dataset/train/Hellas/z3340946337250_e2952e7c6f55d4130924103a4af7d69e.jpg\n","/content/dataset/train/Hellas/z3340946337997_7cc8336b72385b95bbbe45e76f3850b0.jpg\n","/content/dataset/train/Hellas/z3340946339595_6147f2270ceeab8fe6e450bc020eb428.jpg\n","/content/dataset/train/Hellas/z3340946345056_1dc53a628d2b5285b1e1706bce282eb0.jpg\n","/content/dataset/train/Hellas/z3340946351201_7b03fe2053ff3d4c519fd693e296431e.jpg\n","/content/dataset/train/Hellas/z3340946351946_71bf2fcba5ee8c5a340536307add03b3.jpg\n","/content/dataset/train/Hellas/z3340946352121_e7b6fa28181f9754e872351bad0612ce.jpg\n","/content/dataset/train/Hellas/z3340946357038_65e005498d95bed1ebf6f6923ebfc0a1.jpg\n","/content/dataset/train/Hellas/z3340946362560_837363a994db83ca8273f48b8171737f.jpg\n","/content/dataset/train/Hellas/z3340946363323_952948f29ecda87e075534f1173d164a.jpg\n","/content/dataset/train/Hellas/z3340946364863_64f4fa3b17ba38ca25eac6a04d0a4d76.jpg\n","/content/dataset/train/Hellas/z3340946369194_17e954000bef9958dbc01bf188d61370.jpg\n","/content/dataset/train/Hellas/z3340946370049_69a7fa78763299b3ce96902cd841dddd.jpg\n","/content/dataset/train/Hellas/z3340946376392_37b9ac19fe447598bd669f2bf0f75b3b.jpg\n","/content/dataset/train/Hellas/z3340946380040_19e4255a062d7b9e8f20b794c1e44109.jpg\n","/content/dataset/train/Hellas/z3340946382101_d854b7eb32e3f6840487ed365c4363cd.jpg\n","/content/dataset/train/Hellas/z3340946384632_9d1963df7388b11c2d8de74d8636a968.jpg\n","/content/dataset/train/Hellas/z3340946392970_2fef845d299ec50f9bcd8b061fc8082e.jpg\n","/content/dataset/train/Hellas/z3340946395036_69751dce00ad2907e48d2bb567846617.jpg\n","/content/dataset/train/Hellas/z3340946396871_66139e019ebdc7bd95532720f9e9d852.jpg\n","/content/dataset/train/Hellas/z3340946477746_e065bc9a249071241c5e65d37b2f1127.jpg\n","/content/dataset/train/Huy/1.jpg\n","/content/dataset/train/Huy/10.jpg\n","/content/dataset/train/Huy/11.jpg\n","/content/dataset/train/Huy/12.jpg\n","/content/dataset/train/Huy/13.jpg\n","/content/dataset/train/Huy/14.jpg\n","/content/dataset/train/Huy/15.jpg\n","/content/dataset/train/Huy/17.jpg\n","/content/dataset/train/Huy/19.jpg\n","/content/dataset/train/Huy/2.jpg\n","Multiple faces detected for /content/dataset/train/Huy/2.jpg, taking one with highest probability\n","/content/dataset/train/Huy/20.jpg\n","/content/dataset/train/Huy/21.jpg\n","/content/dataset/train/Huy/22.jpg\n","/content/dataset/train/Huy/23.jpg\n","/content/dataset/train/Huy/24.jpg\n","/content/dataset/train/Huy/25.jpg\n","/content/dataset/train/Huy/26.jpg\n","/content/dataset/train/Huy/27.jpg\n","/content/dataset/train/Huy/28.jpg\n","/content/dataset/train/Huy/29.jpg\n","/content/dataset/train/Huy/3.jpg\n","/content/dataset/train/Huy/30.jpg\n","/content/dataset/train/Huy/5.jpg\n","/content/dataset/train/Huy/6.jpg\n","/content/dataset/train/Huy/9.jpg\n","/content/dataset/train/Nguyen/1.jpg\n","/content/dataset/train/Nguyen/10.jpg\n","/content/dataset/train/Nguyen/12.jpg\n","/content/dataset/train/Nguyen/13.jpg\n","/content/dataset/train/Nguyen/14.jpg\n","/content/dataset/train/Nguyen/15.jpg\n","/content/dataset/train/Nguyen/16.jpg\n","/content/dataset/train/Nguyen/17.jpg\n","/content/dataset/train/Nguyen/19.jpg\n","/content/dataset/train/Nguyen/2.jpg\n","/content/dataset/train/Nguyen/20.jpg\n","/content/dataset/train/Nguyen/21.jpg\n","/content/dataset/train/Nguyen/22.jpg\n","/content/dataset/train/Nguyen/23.jpg\n","/content/dataset/train/Nguyen/24.jpg\n","/content/dataset/train/Nguyen/25.jpg\n","/content/dataset/train/Nguyen/26.jpg\n","/content/dataset/train/Nguyen/27.jpg\n","/content/dataset/train/Nguyen/28.jpg\n","/content/dataset/train/Nguyen/29.jpg\n","/content/dataset/train/Nguyen/3.jpg\n","/content/dataset/train/Nguyen/30.jpg\n","/content/dataset/train/Nguyen/5.jpg\n","/content/dataset/train/Nguyen/6.jpg\n","/content/dataset/train/Nguyen/9.jpg\n","/content/dataset/train/Phong/1.jpg\n","/content/dataset/train/Phong/10.jpg\n","/content/dataset/train/Phong/11.jpg\n","/content/dataset/train/Phong/12.jpg\n","/content/dataset/train/Phong/14.jpg\n","/content/dataset/train/Phong/15.jpg\n","Could not find face on /content/dataset/train/Phong/15.jpg\n","/content/dataset/train/Phong/16.jpg\n","/content/dataset/train/Phong/17.jpg\n","/content/dataset/train/Phong/19.jpg\n","/content/dataset/train/Phong/2.jpg\n","/content/dataset/train/Phong/20.jpg\n","/content/dataset/train/Phong/21.jpg\n","Multiple faces detected for /content/dataset/train/Phong/21.jpg, taking one with highest probability\n","/content/dataset/train/Phong/22.jpg\n","/content/dataset/train/Phong/23.jpg\n","/content/dataset/train/Phong/24.jpg\n","/content/dataset/train/Phong/25.jpg\n","/content/dataset/train/Phong/26.jpg\n","/content/dataset/train/Phong/27.jpg\n","/content/dataset/train/Phong/28.jpg\n","/content/dataset/train/Phong/29.jpg\n","/content/dataset/train/Phong/3.jpg\n","/content/dataset/train/Phong/30.jpg\n","/content/dataset/train/Phong/32.jpg\n","/content/dataset/train/Phong/33.jpg\n","/content/dataset/train/Phong/34.jpg\n","/content/dataset/train/Phong/5.jpg\n","/content/dataset/train/Phong/6.jpg\n","/content/dataset/train/Phong/9.jpg\n","/content/dataset/train/Phu/1.jpg\n","/content/dataset/train/Phu/10.jpg\n","/content/dataset/train/Phu/11.jpg\n","/content/dataset/train/Phu/13.jpg\n","/content/dataset/train/Phu/14.jpg\n","/content/dataset/train/Phu/15.jpg\n","/content/dataset/train/Phu/16.jpg\n","/content/dataset/train/Phu/17.jpg\n","/content/dataset/train/Phu/19.jpg\n","/content/dataset/train/Phu/2.jpg\n","/content/dataset/train/Phu/20.jpg\n","/content/dataset/train/Phu/21.jpg\n","/content/dataset/train/Phu/22.jpg\n","/content/dataset/train/Phu/23.jpg\n","/content/dataset/train/Phu/24.jpg\n","/content/dataset/train/Phu/25.jpg\n","/content/dataset/train/Phu/26.jpg\n","/content/dataset/train/Phu/27.jpg\n","/content/dataset/train/Phu/28.jpg\n","/content/dataset/train/Phu/29.jpg\n","/content/dataset/train/Phu/3.jpg\n","/content/dataset/train/Phu/30.jpg\n","/content/dataset/train/Phu/5.jpg\n","/content/dataset/train/Phu/6.jpg\n","/content/dataset/train/Phu/9.jpg\n","/content/dataset/val/Hellas/z3340946330772_f1198bec1275bcdffe6965a8407737ec.jpg\n","/content/dataset/val/Hellas/z3340946344703_e42b358933493ae9cb5865b7cfd3c841.jpg\n","/content/dataset/val/Hellas/z3340946358065_ce2b05724ed5e25212c73b4cfffd528a.jpg\n","/content/dataset/val/Hellas/z3340946375073_3e74522e72dd953a20d1feb6776864b0.jpg\n","/content/dataset/val/Hellas/z3340946388448_a594ff6fd4117380695f4ae5be89229b.jpg\n","/content/dataset/val/Huy/18.jpg\n","/content/dataset/val/Huy/31.jpg\n","/content/dataset/val/Huy/4.jpg\n","/content/dataset/val/Huy/7.jpg\n","/content/dataset/val/Huy/8.jpg\n","/content/dataset/val/Nguyen/18.jpg\n","/content/dataset/val/Nguyen/31.jpg\n","/content/dataset/val/Nguyen/4.jpg\n","/content/dataset/val/Nguyen/7.jpg\n","/content/dataset/val/Nguyen/8.jpg\n","/content/dataset/val/Phong/18.jpg\n","/content/dataset/val/Phong/31.jpg\n","/content/dataset/val/Phong/4.jpg\n","/content/dataset/val/Phong/7.jpg\n","/content/dataset/val/Phong/8.jpg\n","/content/dataset/val/Phu/18.jpg\n","/content/dataset/val/Phu/31.jpg\n","/content/dataset/val/Phu/4.jpg\n","/content/dataset/val/Phu/7.jpg\n","/content/dataset/val/Phu/8.jpg\n"]}]},{"cell_type":"code","source":["clf = train(embeddings, labels)"],"metadata":{"id":"-56-1opO90YD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx_to_class = {v: k for k, v in class_to_idx.items()}\n","print(idx_to_class)\n","\n","target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(labels, clf.predict(embeddings), target_names=target_names))\n","\n","# Predict labels for validation set and calculate accuracy\n","y_val_pred = clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_val_pred)\n","print('Validation Accuracy: {:.2f}%'.format(accuracy*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DHx-vt0M92iz","executionInfo":{"status":"ok","timestamp":1687116280659,"user_tz":-420,"elapsed":3,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"c64c9980-7b57-4e1e-98dc-4614b8d19213"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'Hellas', 1: 'Huy', 2: 'Nguyen', 3: 'Phong', 4: 'Phu'}\n","              precision    recall  f1-score   support\n","\n","      Hellas       1.00      1.00      1.00        25\n","         Huy       1.00      1.00      1.00        25\n","      Nguyen       1.00      1.00      1.00        25\n","       Phong       1.00      1.00      1.00        27\n","         Phu       1.00      1.00      1.00        25\n","\n","    accuracy                           1.00       127\n","   macro avg       1.00      1.00      1.00       127\n","weighted avg       1.00      1.00      1.00       127\n","\n","Validation Accuracy: 100.00%\n"]}]},{"cell_type":"markdown","source":["**Try again facenet only**"],"metadata":{"id":"r7U3-gCM99IL"}},{"cell_type":"code","source":["resnet = InceptionResnetV1(\n","    classify=True,\n","    pretrained='vggface2',\n","    num_classes=len(idx_to_class)\n",").to(device)"],"metadata":{"id":"4C9Vh7hq9-xq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/dataset/train'\n","dataset_train = datasets.ImageFolder(data_dir, transform=transforms.Resize((512,512)))\n","dataset_train.samples = [\n","    (p, p.replace(data_dir, data_dir + '_cropped'))\n","        for p, _ in dataset_train.samples\n","]"],"metadata":{"id":"KwZCSWqj-ApG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","epochs = 20\n","workers = 0 if os.name == 'nt' else 8"],"metadata":{"id":"maOXW6T4-CP9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(\n","    dataset_train,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    collate_fn=training.collate_pil\n",")\n","\n","for i, (x, y) in enumerate(train_loader):\n","    mtcnn(x, save_path=y)\n","    print('\\rBatch {} of {}'.format(i + 1, len(train_loader)), end='')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOv6ZLy_-EGQ","executionInfo":{"status":"ok","timestamp":1687116389641,"user_tz":-420,"elapsed":38927,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"92e5c49f-0614-4040-d81c-1d45bb54160e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Batch 4 of 4"]}]},{"cell_type":"code","source":["data_dir = '/content/dataset/val'\n","dataset_val = datasets.ImageFolder(data_dir, transform=transforms.Resize((512,512)))\n","dataset_val.samples = [\n","    (p, p.replace(data_dir, data_dir + '_cropped'))\n","        for p, _ in dataset_val.samples\n","]\n","\n","val_loader = DataLoader(\n","    dataset_val,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    collate_fn=training.collate_pil\n",")\n","\n","for i, (x, y) in enumerate(val_loader):\n","    mtcnn(x, save_path=y)\n","    print('\\rBatch {} of {}'.format(i + 1, len(val_loader)), end='')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZ64UB5x-JAa","executionInfo":{"status":"ok","timestamp":1687116402555,"user_tz":-420,"elapsed":9110,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"e4309f38-4c2a-49c0-af0f-89094059724f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\rBatch 1 of 1"]}]},{"cell_type":"code","source":["optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n","scheduler = MultiStepLR(optimizer, [5, 10])\n","\n","trans = transforms.Compose([\n","    np.float32,\n","    transforms.ToTensor(),\n","    fixed_image_standardization\n","])\n","\n","train_dataset = datasets.ImageFolder('/content/dataset/train_cropped', transform=trans)\n","img_inds_train = np.arange(len(train_dataset))\n","np.random.shuffle(img_inds_train)\n","\n","val_dataset = datasets.ImageFolder('/content/dataset/val_cropped', transform=trans)\n","img_inds_val = np.arange(len(val_dataset))\n","np.random.shuffle(img_inds_val)\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    sampler=SubsetRandomSampler(img_inds_train)\n",")\n","val_loader = DataLoader(\n","    val_dataset,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    sampler=SubsetRandomSampler(img_inds_val)\n",")"],"metadata":{"id":"NGfnEZ7X-UqG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training**"],"metadata":{"id":"50zv2GwL-W7O"}},{"cell_type":"code","source":["loss_fn = torch.nn.CrossEntropyLoss()\n","metrics = {\n","    'fps': training.BatchTimer(),\n","    'acc': training.accuracy\n","}"],"metadata":{"id":"1LIiDZbL-XvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["writer = SummaryWriter()\n","writer.iteration, writer.interval = 0, 10\n","\n","print('\\n\\nInitial')\n","print('-' * 10)\n","resnet.eval()\n","training.pass_epoch(\n","    resnet, loss_fn, val_loader,\n","    batch_metrics=metrics, show_running=True, device=device,\n","    writer=writer\n",")\n","\n","for epoch in range(epochs):\n","    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n","    print('-' * 10)\n","\n","    resnet.train()\n","    training.pass_epoch(\n","        resnet, loss_fn, train_loader, optimizer, scheduler,\n","        batch_metrics=metrics, show_running=True, device=device,\n","        writer=writer\n","    )\n","\n","    resnet.eval()\n","    training.pass_epoch(\n","        resnet, loss_fn, val_loader,\n","        batch_metrics=metrics, show_running=True, device=device,\n","        writer=writer\n","    )\n","\n","writer.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrj4TTjK-ZVV","executionInfo":{"status":"ok","timestamp":1687117137302,"user_tz":-420,"elapsed":709908,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"3362ccee-9108-4db1-bcf7-50cdeca02255"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Initial\n","----------\n","Valid |     1/1    | loss:    1.6447 | fps:    2.1037 | acc:    0.1600   \n","\n","Epoch 1/20\n","----------\n","Train |     4/4    | loss:    0.6742 | fps:    4.4303 | acc:    0.7422   \n","Valid |     1/1    | loss:    7.9410 | fps:    9.3435 | acc:    0.3600   \n","\n","Epoch 2/20\n","----------\n","Train |     4/4    | loss:    0.0415 | fps:    4.4672 | acc:    0.9922   \n","Valid |     1/1    | loss:   23.0485 | fps:    9.3879 | acc:    0.2000   \n","\n","Epoch 3/20\n","----------\n","Train |     4/4    | loss:    0.0332 | fps:    4.3358 | acc:    0.9922   \n","Valid |     1/1    | loss:    8.2651 | fps:    9.2996 | acc:    0.2800   \n","\n","Epoch 4/20\n","----------\n","Train |     4/4    | loss:    0.1698 | fps:    3.8115 | acc:    0.9531   \n","Valid |     1/1    | loss:    5.0914 | fps:    5.1374 | acc:    0.4800   \n","\n","Epoch 5/20\n","----------\n","Train |     4/4    | loss:    0.0694 | fps:    3.8056 | acc:    0.9766   \n","Valid |     1/1    | loss:    0.5074 | fps:    5.0890 | acc:    0.8400   \n","\n","Epoch 6/20\n","----------\n","Train |     4/4    | loss:    0.0169 | fps:    3.7362 | acc:    0.9922   \n","Valid |     1/1    | loss:    0.2370 | fps:    7.8181 | acc:    0.9600   \n","\n","Epoch 7/20\n","----------\n","Train |     4/4    | loss:    0.0085 | fps:    4.5105 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.1549 | fps:    7.5881 | acc:    0.9600   \n","\n","Epoch 8/20\n","----------\n","Train |     4/4    | loss:    0.0039 | fps:    4.4720 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.1098 | fps:    7.2173 | acc:    0.9600   \n","\n","Epoch 9/20\n","----------\n","Train |     4/4    | loss:    0.0075 | fps:    4.2113 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0743 | fps:    5.4795 | acc:    0.9600   \n","\n","Epoch 10/20\n","----------\n","Train |     4/4    | loss:    0.0042 | fps:    4.0965 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0527 | fps:    3.9426 | acc:    0.9600   \n","\n","Epoch 11/20\n","----------\n","Train |     4/4    | loss:    0.0101 | fps:    3.5207 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0511 | fps:    9.3217 | acc:    0.9600   \n","\n","Epoch 12/20\n","----------\n","Train |     4/4    | loss:    0.0019 | fps:    3.4255 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0453 | fps:    8.9832 | acc:    0.9600   \n","\n","Epoch 13/20\n","----------\n","Train |     4/4    | loss:    0.0024 | fps:    4.4525 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0467 | fps:    9.1492 | acc:    0.9600   \n","\n","Epoch 14/20\n","----------\n","Train |     4/4    | loss:    0.0015 | fps:    4.5142 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0489 | fps:    9.2769 | acc:    0.9600   \n","\n","Epoch 15/20\n","----------\n","Train |     4/4    | loss:    0.0024 | fps:    4.5155 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0499 | fps:    4.3966 | acc:    0.9600   \n","\n","Epoch 16/20\n","----------\n","Train |     4/4    | loss:    0.0017 | fps:    3.4953 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0481 | fps:    8.7360 | acc:    0.9600   \n","\n","Epoch 17/20\n","----------\n","Train |     4/4    | loss:    0.0019 | fps:    4.3526 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0478 | fps:    6.1530 | acc:    0.9600   \n","\n","Epoch 18/20\n","----------\n","Train |     4/4    | loss:    0.0008 | fps:    4.3906 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0459 | fps:    9.1723 | acc:    0.9600   \n","\n","Epoch 19/20\n","----------\n","Train |     4/4    | loss:    0.0435 | fps:    4.4515 | acc:    0.9844   \n","Valid |     1/1    | loss:    0.0396 | fps:    7.7976 | acc:    0.9600   \n","\n","Epoch 20/20\n","----------\n","Train |     4/4    | loss:    0.0014 | fps:    4.4730 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.0384 | fps:    9.1287 | acc:    0.9600   \n"]}]},{"cell_type":"code","source":["resnet.eval()\n","predict = []\n","for x,y in val_loader:\n","  pred = resnet(x).detach().cpu()\n","  label = y"],"metadata":{"id":"h2mTiDHT-1DI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = np.argmax(pred,axis = 1)"],"metadata":{"id":"1RA-U2GB-1ip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import metrics\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"bfJTRltS-3Zp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(label, pred, target_names=target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0ETywcO-8QH","executionInfo":{"status":"ok","timestamp":1687117150725,"user_tz":-420,"elapsed":2,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"084f6d0b-3e18-4928-cff7-2b3c43d15512"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","      Hellas       1.00      0.80      0.89         5\n","         Huy       0.83      1.00      0.91         5\n","      Nguyen       1.00      1.00      1.00         5\n","       Phong       1.00      1.00      1.00         5\n","         Phu       1.00      1.00      1.00         5\n","\n","    accuracy                           0.96        25\n","   macro avg       0.97      0.96      0.96        25\n","weighted avg       0.97      0.96      0.96        25\n","\n"]}]},{"cell_type":"markdown","source":["**AutoFaiss - CLIP + Faiss**"],"metadata":{"id":"P-7TIC6c-8zk"}},{"cell_type":"code","source":["!pip install clip-retrieval autofaiss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8dqPBnWX-9vS","executionInfo":{"status":"ok","timestamp":1687117361382,"user_tz":-420,"elapsed":199664,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"a01aa4e4-d3cd-4c8e-d992-27064d86382a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting clip-retrieval\n","  Downloading clip_retrieval-2.37.0-py3-none-any.whl (343 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.4/343.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting autofaiss\n","  Downloading autofaiss-2.15.8-py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting img2dataset<2,>=1.25.5 (from clip-retrieval)\n","  Downloading img2dataset-1.41.0-py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting clip-anytorch<3,>=2.5.0 (from clip-retrieval)\n","  Downloading clip_anytorch-2.5.2-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm<5,>=4.62.3 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (4.65.0)\n","Collecting fire<0.5.0,>=0.4.0 (from clip-retrieval)\n","  Downloading fire-0.4.0.tar.gz (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch<2,>=1.7.1 (from clip-retrieval)\n","  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m862.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (0.15.2+cu118)\n","Requirement already satisfied: numpy<2,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (1.22.4)\n","Collecting faiss-cpu<2,>=1.7.2 (from clip-retrieval)\n","  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flask<3,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (2.2.4)\n","Collecting flask-restful<1,>=0.3.9 (from clip-retrieval)\n","  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n","Collecting flask-cors<4,>=3.0.10 (from clip-retrieval)\n","  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n","Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (1.5.3)\n","Collecting pyarrow<8,>=6.0.1 (from clip-retrieval)\n","  Downloading pyarrow-7.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting webdataset<0.3,>=0.2 (from clip-retrieval)\n","  Downloading webdataset-0.2.48-py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: h5py<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (3.8.0)\n","Requirement already satisfied: prometheus-client<1,>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (0.16.0)\n","Collecting fsspec==2022.11.0 (from clip-retrieval)\n","  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentence-transformers<3,>=2.2.0 (from clip-retrieval)\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting wandb<0.13,>=0.12.10 (from clip-retrieval)\n","  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting open-clip-torch<3.0.0,>=2.0.0 (from clip-retrieval)\n","  Downloading open_clip_torch-2.20.0-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (2.27.1)\n","Collecting aiohttp<4,>=3.8.1 (from clip-retrieval)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multilingual-clip<2,>=1.0.10 (from clip-retrieval)\n","  Downloading multilingual_clip-1.0.10-py3-none-any.whl (20 kB)\n","Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (1.26.15)\n","Collecting scipy<1.9.2 (from clip-retrieval)\n","  Downloading scipy-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting embedding-reader<2,>=1.5.1 (from autofaiss)\n","  Downloading embedding_reader-1.5.1-py3-none-any.whl (18 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting ftfy (from clip-anytorch<3,>=2.5.0->clip-retrieval)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip-anytorch<3,>=2.5.0->clip-retrieval) (2022.10.31)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire<0.5.0,>=0.4.0->clip-retrieval) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire<0.5.0,>=0.4.0->clip-retrieval) (2.3.0)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask<3,>=2.0.3->clip-retrieval) (2.3.0)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask<3,>=2.0.3->clip-retrieval) (3.1.2)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask<3,>=2.0.3->clip-retrieval) (2.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask<3,>=2.0.3->clip-retrieval) (8.1.3)\n","Collecting aniso8601>=0.82 (from flask-restful<1,>=0.3.9->clip-retrieval)\n","  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from flask-restful<1,>=0.3.9->clip-retrieval) (2022.7.1)\n","Requirement already satisfied: opencv-python-headless<5,>=4.5.5.62 in /usr/local/lib/python3.10/dist-packages (from img2dataset<2,>=1.25.5->clip-retrieval) (4.7.0.72)\n","Collecting exifread-nocycle<4,>=3.0.1 (from img2dataset<2,>=1.25.5->clip-retrieval)\n","  Downloading ExifRead_nocycle-3.0.1-py3-none-any.whl (39 kB)\n","Requirement already satisfied: albumentations<2,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from img2dataset<2,>=1.25.5->clip-retrieval) (1.2.1)\n","Collecting dataclasses<1.0.0,>=0.6 (from img2dataset<2,>=1.25.5->clip-retrieval)\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Collecting transformers (from multilingual-clip<2,>=1.0.10->clip-retrieval)\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub (from open-clip-torch<3.0.0,>=2.0.0->clip-retrieval)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece (from open-clip-torch<3.0.0,>=2.0.0->clip-retrieval)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf<4 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch<3.0.0,>=2.0.0->clip-retrieval) (3.20.3)\n","Collecting timm (from open-clip-torch<3.0.0,>=2.0.0->clip-retrieval)\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->clip-retrieval) (2.8.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.1->clip-retrieval) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.1->clip-retrieval) (3.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3,>=2.2.0->clip-retrieval) (1.2.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3,>=2.2.0->clip-retrieval) (3.8.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2,>=1.7.1->clip-retrieval) (4.5.0)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2,>=1.7.1->clip-retrieval)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2,>=1.7.1->clip-retrieval)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2,>=1.7.1->clip-retrieval)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2,>=1.7.1->clip-retrieval)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7.1->clip-retrieval) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7.1->clip-retrieval) (0.40.0)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision<2,>=0.10.1 (from clip-retrieval)\n","  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<2,>=0.10.1->clip-retrieval) (8.4.0)\n","Collecting GitPython>=1.0.0 (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (2.3)\n","Collecting shortuuid>=0.5.0 (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading sentry_sdk-1.25.1-py2.py3-none-any.whl (206 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (6.0)\n","Collecting pathtools (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Collecting braceexpand (from webdataset<0.3,>=0.2->clip-retrieval)\n","  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (0.19.3)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (0.0.4)\n","Collecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->clip-retrieval) (3.12.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->clip-retrieval) (23.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask<3,>=2.0.3->clip-retrieval) (2.1.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->multilingual-clip<2,>=1.0.10->clip-retrieval)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->multilingual-clip<2,>=1.0.10->clip-retrieval)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip-anytorch<3,>=2.5.0->clip-retrieval) (0.2.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers<3,>=2.2.0->clip-retrieval) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3,>=2.2.0->clip-retrieval) (3.1.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (3.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (2.25.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (2023.4.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (1.4.1)\n","Building wheels for collected packages: fire, sentence-transformers, pathtools\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115927 sha256=c245c64c1c97140283b6363b7ebc32d3b708bd7af7a686eeedc0776703ccc1b6\n","  Stored in directory: /root/.cache/pip/wheels/26/9a/dd/2818b1b023daf077ec3e625c47ae446aca587a5abe48e05212\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=dca5e94664f672de34d9ba8bfe35065391319a3d049b0a88d80cf2693b4aa49c\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=b6572ecab26573f797f15b051ade35e68d11df92f77e1a89ced7b89bd5b5d4a0\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built fire sentence-transformers pathtools\n","Installing collected packages: tokenizers, sentencepiece, safetensors, pathtools, faiss-cpu, exifread-nocycle, dataclasses, braceexpand, aniso8601, webdataset, smmap, shortuuid, setproctitle, sentry-sdk, scipy, pyarrow, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, multidict, ftfy, fsspec, frozenlist, fire, docker-pycreds, async-timeout, yarl, nvidia-cudnn-cu11, huggingface-hub, gitdb, aiosignal, transformers, torch, GitPython, flask-restful, flask-cors, embedding-reader, aiohttp, wandb, torchvision, multilingual-clip, autofaiss, timm, sentence-transformers, img2dataset, clip-anytorch, open-clip-torch, clip-retrieval\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.10.1\n","    Uninstalling scipy-1.10.1:\n","      Successfully uninstalled scipy-1.10.1\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 9.0.0\n","    Uninstalling pyarrow-9.0.0:\n","      Successfully uninstalled pyarrow-9.0.0\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2023.4.0\n","    Uninstalling fsspec-2023.4.0:\n","      Successfully uninstalled fsspec-2023.4.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.1+cu118\n","    Uninstalling torch-2.0.1+cu118:\n","      Successfully uninstalled torch-2.0.1+cu118\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.15.2+cu118\n","    Uninstalling torchvision-0.15.2+cu118:\n","      Successfully uninstalled torchvision-0.15.2+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.31 aiohttp-3.8.4 aiosignal-1.3.1 aniso8601-9.0.1 async-timeout-4.0.2 autofaiss-2.15.8 braceexpand-0.1.7 clip-anytorch-2.5.2 clip-retrieval-2.37.0 dataclasses-0.6 docker-pycreds-0.4.0 embedding-reader-1.5.1 exifread-nocycle-3.0.1 faiss-cpu-1.7.4 fire-0.4.0 flask-cors-3.0.10 flask-restful-0.3.10 frozenlist-1.3.3 fsspec-2022.11.0 ftfy-6.1.1 gitdb-4.0.10 huggingface-hub-0.15.1 img2dataset-1.41.0 multidict-6.0.4 multilingual-clip-1.0.10 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 open-clip-torch-2.20.0 pathtools-0.1.2 pyarrow-7.0.0 safetensors-0.3.1 scipy-1.9.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 sentry-sdk-1.25.1 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 timm-0.9.2 tokenizers-0.13.3 torch-1.13.1 torchvision-0.14.1 transformers-4.30.2 wandb-0.12.21 webdataset-0.2.48 yarl-1.9.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses","fsspec","pyarrow","scipy","torch","torchvision"]}}},"metadata":{}}]},{"cell_type":"code","source":["!clip-retrieval inference --input_dataset /content/dataset/train_cropped --output_folder /content/dataset/train_embedding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5lqHI-1c_lKu","executionInfo":{"status":"ok","timestamp":1687117597906,"user_tz":-420,"elapsed":215616,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"8ee44bdd-7fb3-4162-b9c2-6a3bbdf4c570"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples has been estimated to be 54\n","Starting the worker\n","dataset is 30\n","Starting work on task 0\n","100%|████████████████████████████████████████| 354M/354M [00:02<00:00, 137MiB/s]\n","warming up with batch size 256 on cpu\n","done warming up in 182.14397382736206s\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"," sample_per_sec 4 ; sample_count 54 "]}]},{"cell_type":"code","source":["!autofaiss build_index --embeddings=\"/content/dataset/train_embedding/img_emb\" \\\n","                    --index_path=\"/content/knn.index\" \\\n","                    --index_infos_path=\"/content/infos.json\" \\\n","                    --metric_type=\"ip\" \\\n","                    --max_index_query_time_ms=10 \\\n","                    --max_index_memory_usage=\"4GB\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X8kY36vy_oBL","executionInfo":{"status":"ok","timestamp":1687117612388,"user_tz":-420,"elapsed":1158,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"e6be1377-1659-4ba6-e70b-5a76006885c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-18 19:46:52,435 [INFO]: Using 2 omp threads (processes), consider increasing --nb_cores if you have more\n","2023-06-18 19:46:52,436 [INFO]: Launching the whole pipeline 06/18/2023, 19:46:52\n","2023-06-18 19:46:52,436 [INFO]: Reading total number of vectors and dimension 06/18/2023, 19:46:52\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 15827.56it/s]\n","2023-06-18 19:46:52,465 [INFO]: There are 54 embeddings of dim 512\n","2023-06-18 19:46:52,465 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0284 secs\n","2023-06-18 19:46:52,465 [INFO]: \tCompute estimated construction time of the index 06/18/2023, 19:46:52\n","2023-06-18 19:46:52,465 [INFO]: \t\t-> Train: 16.7 minutes\n","2023-06-18 19:46:52,465 [INFO]: \t\t-> Add: 0.0 seconds\n","2023-06-18 19:46:52,465 [INFO]: \t\tTotal: 16.7 minutes\n","2023-06-18 19:46:52,465 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0004 secs\n","2023-06-18 19:46:52,465 [INFO]: \tChecking that your have enough memory available to create the index 06/18/2023, 19:46:52\n","2023-06-18 19:46:52,466 [INFO]: 118.8KB of memory will be needed to build the index (more might be used if you have more)\n","2023-06-18 19:46:52,466 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0007 secs\n","2023-06-18 19:46:52,466 [INFO]: \tSelecting most promising index types given data characteristics 06/18/2023, 19:46:52\n","2023-06-18 19:46:52,466 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n","2023-06-18 19:46:52,466 [INFO]: \tCreating the index 06/18/2023, 19:46:52\n","2023-06-18 19:46:52,467 [INFO]: \t\t-> Instanciate the index Flat 06/18/2023, 19:46:52\n","2023-06-18 19:46:52,475 [INFO]: \t\t>>> Finished \"-> Instanciate the index Flat\" in 0.0080 secs\n","2023-06-18 19:46:52,475 [INFO]: \t\t-> Adding the vectors to the index 06/18/2023, 19:46:52\n","2023-06-18 19:46:52,475 [INFO]: The memory available for adding the vectors is 32.0GB(total available - used by the index)\n","2023-06-18 19:46:52,475 [INFO]: Using a batch size of 488281 (memory overhead 953.7MB)\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 86.12it/s]\n","2023-06-18 19:46:52,490 [INFO]: \tComputing best hyperparameters for index /content/knn.index 06/18/2023, 19:46:52\n","2023-06-18 19:46:52,490 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /content/knn.index\" in 0.0000 secs\n","2023-06-18 19:46:52,492 [INFO]: The best hyperparameters are: \n","2023-06-18 19:46:52,492 [INFO]: \tCompute fast metrics 06/18/2023, 19:46:52\n","\r  0% 0/1 [00:00<?, ?it/s]\r  0% 0/1 [00:00<?, ?it/s]\n","2023-06-18 19:46:52,547 [INFO]: \t>>> Finished \"Compute fast metrics\" in 0.0543 secs\n","2023-06-18 19:46:52,547 [INFO]: \tSaving the index on local disk 06/18/2023, 19:46:52\n","2023-06-18 19:46:52,548 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.0011 secs\n","2023-06-18 19:46:52,548 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 0.0735 secs\n","2023-06-18 19:46:52,548 [INFO]: {\n","2023-06-18 19:46:52,548 [INFO]: \tindex_key: Flat\n","2023-06-18 19:46:52,548 [INFO]: \tindex_param: \n","2023-06-18 19:46:52,548 [INFO]: \tindex_path: /content/knn.index\n","2023-06-18 19:46:52,548 [INFO]: \tsize in bytes: 110637\n","2023-06-18 19:46:52,549 [INFO]: \tavg_search_speed_ms: 0.018394568227889457\n","2023-06-18 19:46:52,549 [INFO]: \t99p_search_speed_ms: 0.06427944921597374\n","2023-06-18 19:46:52,549 [INFO]: \treconstruction error %: 0.0\n","2023-06-18 19:46:52,549 [INFO]: \tnb vectors: 54\n","2023-06-18 19:46:52,549 [INFO]: \tvectors dimension: 512\n","2023-06-18 19:46:52,549 [INFO]: \tcompression ratio: 0.9995932644594485\n","2023-06-18 19:46:52,549 [INFO]: }\n","2023-06-18 19:46:52,549 [INFO]: \t>>> Finished \"Creating the index\" in 0.0823 secs\n","2023-06-18 19:46:52,549 [INFO]: >>> Finished \"Launching the whole pipeline\" in 0.1126 secs\n","(<faiss.swigfaiss_avx2.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x7f681c243db0> >, {'index_key': 'Flat', 'index_param': '', 'index_path': '/content/knn.index', 'size in bytes': 110637, 'avg_search_speed_ms': 0.018394568227889457, '99p_search_speed_ms': 0.06427944921597374, 'reconstruction error %': 0.0, 'nb vectors': 54, 'vectors dimension': 512, 'compression ratio': 0.9995932644594485})\n"]}]},{"cell_type":"code","source":["import faiss\n","import torch\n","import clip\n","import os\n","import pandas as pd"],"metadata":{"id":"ugaWIAkc_vRm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_parquet(\"/content/dataset/train_embedding/metadata/metadata_0.parquet\")\n","image_list = df[\"image_path\"].tolist()\n","ind = faiss.read_index(\"/content/knn.index\")"],"metadata":{"id":"ksXYzWSC_v3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)"],"metadata":{"id":"lhpbM-PP_xlM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trans = transforms.Compose([\n","    np.float32,\n","    # transforms.ToTensor(),\n","    fixed_image_standardization\n","])\n","\n","val_dataset = datasets.ImageFolder('/content/dataset/val_cropped')\n","test_class_idx = val_dataset.class_to_idx"],"metadata":{"id":"Anx_cx-A_y5i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx_to_class = {v: k for k, v in test_class_idx.items()}\n","print(idx_to_class)\n","\n","target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(target_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yv9Hwjgj_0op","executionInfo":{"status":"ok","timestamp":1687117700107,"user_tz":-420,"elapsed":4,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"53ed2917-cbee-452f-e8ea-4e41fc503c1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'Hellas', 1: 'Huy', 2: 'Nguyen', 3: 'Phong', 4: 'Phu'}\n","['Hellas', 'Huy', 'Nguyen', 'Phong', 'Phu']\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","from collections import defaultdict"],"metadata":{"id":"kjzCp3Yt_21X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = []\n","preds = []\n","k = 5\n","for x,y in val_dataset:\n","    image_tensor = preprocess(x)\n","    image_features = model.encode_image(torch.unsqueeze(image_tensor.to(device), dim=0))\n","    image_features /= image_features.norm(dim=-1, keepdim=True)\n","    image_embeddings = image_features.cpu().detach().numpy().astype('float32')\n","    D, I = ind.search(image_embeddings, k)\n","    print(D,I)\n","    i_candidate = defaultdict(int)\n","    for D_ele,I_ele in zip(D[0],I[0]):\n","      if D_ele > 0.7:\n","        name = image_list[I_ele].split('/')[-2]\n","        i_candidate[test_class_idx[name]] += 1\n","    key_with_max_value = max(i_candidate, key=lambda k: i_candidate[k])\n","\n","    preds.append(key_with_max_value)\n","    labels.append(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JGbOieCW_4ZR","executionInfo":{"status":"ok","timestamp":1687117927649,"user_tz":-420,"elapsed":13612,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"676ed3c9-6f35-49a8-9eb8-17630d3c5269"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.9814776 0.9811022 0.9768701 0.9695312 0.968061 ]] [[33 31 30 46 39]]\n","[[0.9602302  0.9559616  0.9500785  0.946908   0.94595134]] [[40 37 36 25  7]]\n","[[0.98577493 0.9793645  0.9723014  0.9697337  0.96467674]] [[35 39 36 33 40]]\n","[[0.9912002  0.98894477 0.98893046 0.98604625 0.9849465 ]] [[44 43 47 45 46]]\n","[[0.97098    0.95289326 0.9502202  0.9474362  0.94551396]] [[51 42 34 50 11]]\n","[[0.95729834 0.95626354 0.95396113 0.9523598  0.94224334]] [[21 14 22 10 13]]\n","[[0.92248875 0.91735256 0.9145962  0.91353023 0.9114625 ]] [[33 36 34 32 22]]\n","[[0.9472196  0.9436085  0.94320464 0.94121546 0.9362284 ]] [[ 2  8 26 18 19]]\n","[[0.93213534 0.92167145 0.9194058  0.91939265 0.9167016 ]] [[23 29 48 46 33]]\n","[[0.9298672  0.9270016  0.92661995 0.92609537 0.92108256]] [[43 13 19 44 23]]\n","[[0.9462588  0.94098294 0.9320184  0.9314344  0.9266585 ]] [[ 9  1 27 16 15]]\n","[[0.9419172  0.9397419  0.93809783 0.936774   0.9321478 ]] [[18  5 26 27 11]]\n","[[0.9620621  0.95556676 0.9519692  0.9475317  0.9452912 ]] [[18 16  2  7  5]]\n","[[0.94862103 0.9347883  0.9316882  0.9278764  0.9276496 ]] [[13 34  8  4 21]]\n","[[0.95112747 0.9458168  0.93172646 0.9294636  0.9227019 ]] [[16 12  4 15 11]]\n","[[0.92150164 0.9193933  0.9155615  0.9142921  0.9129405 ]] [[ 7  3  0  2 18]]\n","[[0.9433408  0.92321247 0.9192086  0.91354907 0.911008  ]] [[24  6 23 50 17]]\n","[[0.9584919  0.95471525 0.95005405 0.94917464 0.9484136 ]] [[ 0  7  8  4 14]]\n","[[0.9653878 0.9608245 0.9584384 0.9522573 0.950386 ]] [[25 24 34 31 33]]\n","[[0.9403978  0.8973509  0.8866277  0.8850551  0.88371706]] [[ 3 50 23 25 34]]\n","[[0.9766918 0.972208  0.9706632 0.9679667 0.9662408]] [[ 8 21 14 13 34]]\n","[[0.9426316 0.9420114 0.9406257 0.9363454 0.9343334]] [[ 0  9 41 27 47]]\n","[[0.9746093  0.9697921  0.96414495 0.96345603 0.96244466]] [[ 4 16  0 14 21]]\n","[[0.9461427  0.93923646 0.9382409  0.9372639  0.93692124]] [[ 0 41 21 37  9]]\n","[[0.95486647 0.94757104 0.94446963 0.9414358  0.9384909 ]] [[ 4 22 21 12  0]]\n"]}]},{"cell_type":"code","source":["from sklearn import metrics\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"DcRDdlMW__KF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(preds)\n","print(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqexikKXDV42","executionInfo":{"status":"ok","timestamp":1687117932144,"user_tz":-420,"elapsed":317,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"2ae96dcb-b3d9-4973-8d98-264a502bbecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 4, 4, 4, 4, 4, 4, 3, 4, 0, 3, 4, 4, 4, 4, 4]\n","[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4]\n"]}]},{"cell_type":"code","source":["target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(labels, preds, target_names=target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Piy_iHHAA7Q","executionInfo":{"status":"ok","timestamp":1687117933891,"user_tz":-420,"elapsed":3,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"4f14f34a-4184-4f67-f65d-88047b8ba274"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","      Hellas       0.56      1.00      0.71         5\n","         Huy       0.00      0.00      0.00         5\n","      Nguyen       0.00      0.00      0.00         5\n","       Phong       1.00      0.40      0.57         5\n","         Phu       0.36      1.00      0.53         5\n","\n","    accuracy                           0.48        25\n","   macro avg       0.38      0.48      0.36        25\n","weighted avg       0.38      0.48      0.36        25\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["**Facenet + Faiss**"],"metadata":{"id":"Kx70qhTLADb1"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","mtcnn = MTCNN(\n","    image_size=160, margin=0, min_face_size=20,\n","    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n","    device=device\n",")\n","\n","facenet = InceptionResnetV1(pretrained='vggface2').eval()\n","facenet = facenet.to(device)"],"metadata":{"id":"LSVelwW3AEoq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings, labels, class_to_idx = X_train, y_train, X_train_class_idx"],"metadata":{"id":"ToKKuo_uAeZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -r facenet_emb\n","!mkdir facenet_emb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acY0rAy4Agf-","executionInfo":{"status":"ok","timestamp":1687118052031,"user_tz":-420,"elapsed":353,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"0b22e9b2-95b5-4d41-b1a1-cc62360d8f31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'facenet_emb': No such file or directory\n"]}]},{"cell_type":"code","source":["np.save('/content/facenet_emb/facenet.npy', embeddings)"],"metadata":{"id":"VcWmIuYyAhNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!autofaiss build_index --embeddings=\"/content/facenet_emb\" \\\n","                    --index_path=\"/content/knn_facenet.index\" \\\n","                    --index_infos_path=\"/content/infos_facenet.json\" \\\n","                    --metric_type=\"ip\" \\\n","                    --max_index_query_time_ms=10 \\\n","                    --max_index_memory_usage=\"4GB\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayBds-ySAi32","executionInfo":{"status":"ok","timestamp":1687118058463,"user_tz":-420,"elapsed":1506,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"a3d87e7e-b184-4be1-f5a0-c2631a62beb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-18 19:54:18,417 [INFO]: Using 2 omp threads (processes), consider increasing --nb_cores if you have more\n","2023-06-18 19:54:18,417 [INFO]: Launching the whole pipeline 06/18/2023, 19:54:18\n","2023-06-18 19:54:18,417 [INFO]: Reading total number of vectors and dimension 06/18/2023, 19:54:18\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 13189.64it/s]\n","2023-06-18 19:54:18,482 [INFO]: There are 127 embeddings of dim 512\n","2023-06-18 19:54:18,482 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0650 secs\n","2023-06-18 19:54:18,482 [INFO]: \tCompute estimated construction time of the index 06/18/2023, 19:54:18\n","2023-06-18 19:54:18,482 [INFO]: \t\t-> Train: 16.7 minutes\n","2023-06-18 19:54:18,482 [INFO]: \t\t-> Add: 0.0 seconds\n","2023-06-18 19:54:18,482 [INFO]: \t\tTotal: 16.7 minutes\n","2023-06-18 19:54:18,482 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0002 secs\n","2023-06-18 19:54:18,483 [INFO]: \tChecking that your have enough memory available to create the index 06/18/2023, 19:54:18\n","2023-06-18 19:54:18,483 [INFO]: 279.4KB of memory will be needed to build the index (more might be used if you have more)\n","2023-06-18 19:54:18,483 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0007 secs\n","2023-06-18 19:54:18,483 [INFO]: \tSelecting most promising index types given data characteristics 06/18/2023, 19:54:18\n","2023-06-18 19:54:18,483 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n","2023-06-18 19:54:18,483 [INFO]: \tCreating the index 06/18/2023, 19:54:18\n","2023-06-18 19:54:18,484 [INFO]: \t\t-> Instanciate the index Flat 06/18/2023, 19:54:18\n","2023-06-18 19:54:18,484 [INFO]: \t\t>>> Finished \"-> Instanciate the index Flat\" in 0.0002 secs\n","2023-06-18 19:54:18,484 [INFO]: \t\t-> Adding the vectors to the index 06/18/2023, 19:54:18\n","2023-06-18 19:54:18,484 [INFO]: The memory available for adding the vectors is 32.0GB(total available - used by the index)\n","2023-06-18 19:54:18,484 [INFO]: Using a batch size of 488281 (memory overhead 953.7MB)\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 59.54it/s]\n","2023-06-18 19:54:18,503 [INFO]: \tComputing best hyperparameters for index /content/knn_facenet.index 06/18/2023, 19:54:18\n","2023-06-18 19:54:18,503 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /content/knn_facenet.index\" in 0.0000 secs\n","2023-06-18 19:54:18,503 [INFO]: The best hyperparameters are: \n","2023-06-18 19:54:18,503 [INFO]: \tCompute fast metrics 06/18/2023, 19:54:18\n","\r  0% 0/1 [00:00<?, ?it/s]\r  0% 0/1 [00:00<?, ?it/s]\n","2023-06-18 19:54:18,583 [INFO]: \t>>> Finished \"Compute fast metrics\" in 0.0791 secs\n","2023-06-18 19:54:18,583 [INFO]: \tSaving the index on local disk 06/18/2023, 19:54:18\n","2023-06-18 19:54:18,588 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.0050 secs\n","2023-06-18 19:54:18,588 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 0.1043 secs\n","2023-06-18 19:54:18,588 [INFO]: {\n","2023-06-18 19:54:18,588 [INFO]: \tindex_key: Flat\n","2023-06-18 19:54:18,588 [INFO]: \tindex_param: \n","2023-06-18 19:54:18,588 [INFO]: \tindex_path: /content/knn_facenet.index\n","2023-06-18 19:54:18,588 [INFO]: \tsize in bytes: 260141\n","2023-06-18 19:54:18,588 [INFO]: \tavg_search_speed_ms: 0.03140248622465037\n","2023-06-18 19:54:18,589 [INFO]: \t99p_search_speed_ms: 0.05304984999384033\n","2023-06-18 19:54:18,589 [INFO]: \treconstruction error %: 0.0\n","2023-06-18 19:54:18,589 [INFO]: \tnb vectors: 127\n","2023-06-18 19:54:18,589 [INFO]: \tvectors dimension: 512\n","2023-06-18 19:54:18,589 [INFO]: \tcompression ratio: 0.9998270168869959\n","2023-06-18 19:54:18,589 [INFO]: }\n","2023-06-18 19:54:18,589 [INFO]: \t>>> Finished \"Creating the index\" in 0.1053 secs\n","2023-06-18 19:54:18,589 [INFO]: >>> Finished \"Launching the whole pipeline\" in 0.1718 secs\n","(<faiss.swigfaiss_avx2.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x7f9eac21c780> >, {'index_key': 'Flat', 'index_param': '', 'index_path': '/content/knn_facenet.index', 'size in bytes': 260141, 'avg_search_speed_ms': 0.03140248622465037, '99p_search_speed_ms': 0.05304984999384033, 'reconstruction error %': 0.0, 'nb vectors': 127, 'vectors dimension': 512, 'compression ratio': 0.9998270168869959})\n"]}]},{"cell_type":"code","source":["ind = faiss.read_index(\"/content/knn_facenet.index\")"],"metadata":{"id":"smI4fKmpAlnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = []\n","k = 1\n","for image_emb in X_test:\n","    D, I = ind.search(image_emb[np.newaxis, :], k)\n","    print(D,I)\n","    i_candidate = defaultdict(int)\n","    for D_ele,I_ele in zip(D[0],I[0]):\n","      # if D_ele > 0.7:\n","        cls = labels[I_ele]\n","        i_candidate[cls] += 1\n","    # try:\n","    key_with_max_value = max(i_candidate, key=lambda k: i_candidate[k])\n","    # except:\n","    #   print(\"error\")\n","    #   key_with_max_value = 0\n","\n","    preds.append(key_with_max_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZ-D1fjUAn1I","executionInfo":{"status":"ok","timestamp":1687118106146,"user_tz":-420,"elapsed":3,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"c2ad7aa6-f25d-440d-e6a0-c746d9b9cbe5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.94171274]] [[2]]\n","[[0.93268883]] [[7]]\n","[[0.94907737]] [[6]]\n","[[0.98267686]] [[18]]\n","[[0.88385683]] [[21]]\n","[[0.72854424]] [[28]]\n","[[0.871317]] [[37]]\n","[[0.8147012]] [[29]]\n","[[0.7699375]] [[46]]\n","[[0.91810703]] [[46]]\n","[[0.90323716]] [[74]]\n","[[0.74952936]] [[71]]\n","[[0.9538892]] [[51]]\n","[[0.9163799]] [[72]]\n","[[0.9418812]] [[53]]\n","[[0.9404378]] [[83]]\n","[[0.9845713]] [[99]]\n","[[0.9340857]] [[96]]\n","[[0.86662614]] [[98]]\n","[[0.965695]] [[76]]\n","[[0.90524125]] [[108]]\n","[[0.9515755]] [[110]]\n","[[0.83670175]] [[109]]\n","[[0.9521544]] [[120]]\n","[[0.9633839]] [[122]]\n"]}]},{"cell_type":"code","source":["target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(y_test, preds, target_names=target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cI4uZBgeAoNG","executionInfo":{"status":"ok","timestamp":1687118110249,"user_tz":-420,"elapsed":304,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"910b1ef2-57d7-4afa-fae2-f57916290f8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","      Hellas       1.00      1.00      1.00         5\n","         Huy       1.00      1.00      1.00         5\n","      Nguyen       1.00      1.00      1.00         5\n","       Phong       1.00      1.00      1.00         5\n","         Phu       1.00      1.00      1.00         5\n","\n","    accuracy                           1.00        25\n","   macro avg       1.00      1.00      1.00        25\n","weighted avg       1.00      1.00      1.00        25\n","\n"]}]},{"cell_type":"markdown","source":["**Alignment**"],"metadata":{"id":"QWBXBMTL9Osw"}},{"cell_type":"code","source":["!pip install --upgrade imutils"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-JOr0FI9PqO","executionInfo":{"status":"ok","timestamp":1687166510082,"user_tz":-420,"elapsed":7228,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"5c82a468-f9b3-4889-883c-a4fdb42f09b5"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\n"]}]},{"cell_type":"code","source":["!pip install dlib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_kDqF5I9cre","executionInfo":{"status":"ok","timestamp":1687166515455,"user_tz":-420,"elapsed":5377,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"ad7d29b2-e2dd-44f9-a484-6a9f440d4576"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.1)\n"]}]},{"cell_type":"code","source":["from imutils import face_utils\n","import numpy as np\n","import argparse\n","import imutils\n","import dlib\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from imutils.face_utils import FaceAligner\n","from imutils.face_utils import rect_to_bb\n","import math\n","import matplotlib.pyplot as plt"],"metadata":{"id":"XC9QxqyW9ehX","executionInfo":{"status":"ok","timestamp":1687166516317,"user_tz":-420,"elapsed":369,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["detector = dlib.get_frontal_face_detector()\n","predictor = dlib.shape_predictor('/content/drive/MyDrive/faceRecognition/dlib/shape_predictor_68_face_landmarks.dat')\n","predictor2 = dlib.shape_predictor('/content/drive/MyDrive/faceRecognition/dlib/shape_predictor_81_face_landmarks.dat')"],"metadata":{"id":"w2bM-QUr9fx5","executionInfo":{"status":"ok","timestamp":1687166527656,"user_tz":-420,"elapsed":4131,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def facial_landmarks(image):\n","    try:\n","        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    except:\n","        grayscale_image = image\n","\n","    # array of rectangles surrounding faces detected\n","    rectangles = detector(grayscale_image, 1)\n","\n","    # If at least one face is detected, find its landmarks\n","    if len(rectangles) > 0:\n","        # Get 68 landmark points\n","        faceLandmarks = predictor(grayscale_image, rectangles[0])\n","        faceLandmarks = face_utils.shape_to_np(faceLandmarks)\n","        return faceLandmarks,rectangles\n","    else:\n","        return None"],"metadata":{"id":"7EWgahdx9hsd","executionInfo":{"status":"ok","timestamp":1687166529497,"user_tz":-420,"elapsed":2,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from skimage import transform as trans\n","\n","__file__ = 'test'\n","\n","# reference facial points, a list of coordinates (x,y)\n","REFERENCE_FACIAL_POINTS = [\n","    [30.29459953, 51.69630051],\n","    [65.53179932, 51.50139999],\n","    [48.02519989, 71.73660278],\n","    [33.54930115, 92.3655014],\n","    [62.72990036, 92.20410156]\n","]\n","\n","DEFAULT_CROP_SIZE = (96, 112)\n","\n","\n","class FaceWarpException(Exception):\n","    def __str__(self):\n","        return 'In File {}:{}'.format(\n","            __file__, super.__str__(self))\n","\n","\n","def get_reference_facial_points(output_size=None,\n","                                inner_padding_factor=0.0,\n","                                outer_padding=(0, 0),\n","                                default_square=False):\n","    tmp_5pts = np.array(REFERENCE_FACIAL_POINTS)\n","    tmp_crop_size = np.array(DEFAULT_CROP_SIZE)\n","\n","    # 0) make the inner region a square\n","    if default_square:\n","        size_diff = max(tmp_crop_size) - tmp_crop_size\n","        tmp_5pts += size_diff / 2\n","        tmp_crop_size += size_diff\n","\n","    # print('---> default:')\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              reference_5pts = ', tmp_5pts)\n","\n","    if (output_size and\n","            output_size[0] == tmp_crop_size[0] and\n","            output_size[1] == tmp_crop_size[1]):\n","        print('output_size == DEFAULT_CROP_SIZE {}: return default reference points'.format(tmp_crop_size))\n","        return tmp_5pts\n","\n","    if (inner_padding_factor == 0 and\n","            outer_padding == (0, 0)):\n","        if output_size is None:\n","            print('No paddings to do: return default reference points')\n","            return tmp_5pts\n","        else:\n","            raise FaceWarpException(\n","                'No paddings to do, output_size must be None or {}'.format(tmp_crop_size))\n","\n","    # check output size\n","    if not (0 <= inner_padding_factor <= 1.0):\n","        raise FaceWarpException('Not (0 <= inner_padding_factor <= 1.0)')\n","\n","    if ((inner_padding_factor > 0 or outer_padding[0] > 0 or outer_padding[1] > 0)\n","            and output_size is None):\n","        output_size = tmp_crop_size * \\\n","                      (1 + inner_padding_factor * 2).astype(np.int32)\n","        output_size += np.array(outer_padding)\n","        print('              deduced from paddings, output_size = ', output_size)\n","\n","    if not (outer_padding[0] < output_size[0]\n","            and outer_padding[1] < output_size[1]):\n","        raise FaceWarpException('Not (outer_padding[0] < output_size[0]'\n","                                'and outer_padding[1] < output_size[1])')\n","\n","    # 1) pad the inner region according inner_padding_factor\n","    # print('---> STEP1: pad the inner region according inner_padding_factor')\n","    if inner_padding_factor > 0:\n","        size_diff = tmp_crop_size * inner_padding_factor * 2\n","        tmp_5pts += size_diff / 2\n","        tmp_crop_size += np.round(size_diff).astype(np.int32)\n","\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              reference_5pts = ', tmp_5pts)\n","\n","    # 2) resize the padded inner region\n","    # print('---> STEP2: resize the padded inner region')\n","    size_bf_outer_pad = np.array(output_size) - np.array(outer_padding) * 2\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              size_bf_outer_pad = ', size_bf_outer_pad)\n","\n","    if size_bf_outer_pad[0] * tmp_crop_size[1] != size_bf_outer_pad[1] * tmp_crop_size[0]:\n","        raise FaceWarpException('Must have (output_size - outer_padding)'\n","                                '= some_scale * (crop_size * (1.0 + inner_padding_factor)')\n","\n","    scale_factor = size_bf_outer_pad[0].astype(np.float32) / tmp_crop_size[0]\n","    # print('              resize scale_factor = ', scale_factor)\n","    tmp_5pts = tmp_5pts * scale_factor\n","    #    size_diff = tmp_crop_size * (scale_factor - min(scale_factor))\n","    #    tmp_5pts = tmp_5pts + size_diff / 2\n","    tmp_crop_size = size_bf_outer_pad\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              reference_5pts = ', tmp_5pts)\n","\n","    # 3) add outer_padding to make output_size\n","    reference_5point = tmp_5pts + np.array(outer_padding)\n","    tmp_crop_size = output_size\n","    # print('---> STEP3: add outer_padding to make output_size')\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              reference_5pts = ', tmp_5pts)\n","    #\n","    # print('===> end get_reference_facial_points\\n')\n","\n","    return reference_5point\n","\n","\n","def get_affine_transform_matrix(src_pts, dst_pts):\n","    tfm = np.float32([[1, 0, 0], [0, 1, 0]])\n","    n_pts = src_pts.shape[0]\n","    ones = np.ones((n_pts, 1), src_pts.dtype)\n","    src_pts_ = np.hstack([src_pts, ones])\n","    dst_pts_ = np.hstack([dst_pts, ones])\n","\n","    A, res, rank, s = np.linalg.lstsq(src_pts_, dst_pts_)\n","\n","    if rank == 3:\n","        tfm = np.float32([\n","            [A[0, 0], A[1, 0], A[2, 0]],\n","            [A[0, 1], A[1, 1], A[2, 1]]\n","        ])\n","    elif rank == 2:\n","        tfm = np.float32([\n","            [A[0, 0], A[1, 0], 0],\n","            [A[0, 1], A[1, 1], 0]\n","        ])\n","\n","    return tfm\n","\n","\n","def warp_and_crop_face(src_img,\n","                       facial_pts,\n","                       reference_pts=None,\n","                       crop_size=(96, 112),\n","                       align_type='smilarity'):\n","    if reference_pts is None:\n","        if crop_size[0] == 96 and crop_size[1] == 112:\n","            reference_pts = REFERENCE_FACIAL_POINTS\n","        else:\n","            default_square = False\n","            inner_padding_factor = 0\n","            outer_padding = (0, 0)\n","            output_size = crop_size\n","\n","            reference_pts = get_reference_facial_points(output_size,\n","                                                        inner_padding_factor,\n","                                                        outer_padding,\n","                                                        default_square)\n","\n","    ref_pts = np.float32(reference_pts)\n","    ref_pts_shp = ref_pts.shape\n","    if max(ref_pts_shp) < 3 or min(ref_pts_shp) != 2:\n","        raise FaceWarpException(\n","            'reference_pts.shape must be (K,2) or (2,K) and K>2')\n","\n","    if ref_pts_shp[0] == 2:\n","        ref_pts = ref_pts.T\n","\n","    src_pts = np.float32(facial_pts)\n","    src_pts_shp = src_pts.shape\n","    if max(src_pts_shp) < 3 or min(src_pts_shp) != 2:\n","        raise FaceWarpException(\n","            'facial_pts.shape must be (K,2) or (2,K) and K>2')\n","\n","    if src_pts_shp[0] == 2:\n","        src_pts = src_pts.T\n","\n","    if src_pts.shape != ref_pts.shape:\n","        raise FaceWarpException(\n","            'facial_pts and reference_pts must have the same shape')\n","\n","    if align_type == 'cv2_affine':\n","        tfm = cv2.getAffineTransform(src_pts[0:3], ref_pts[0:3])\n","    #        print('cv2.getAffineTransform() returns tfm=\\n' + str(tfm))\n","    elif align_type == 'affine':\n","        tfm = get_affine_transform_matrix(src_pts, ref_pts)\n","    #        print('get_affine_transform_matrix() returns tfm=\\n' + str(tfm))\n","    else:\n","        # tfm = get_similarity_transform_for_cv2(src_pts, ref_pts)\n","        tform = trans.SimilarityTransform()\n","        tform.estimate(src_pts, ref_pts)\n","        tfm = tform.params[0:2, :]\n","\n","    face_img = cv2.warpAffine(src_img, tfm, (crop_size[0], crop_size[1]))\n","\n","    return face_img"],"metadata":{"id":"k_y9YTyi9kwf","executionInfo":{"status":"ok","timestamp":1687166543016,"user_tz":-420,"elapsed":942,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["!mkdir dataset/train_crop\n","!mkdir dataset/val_crop"],"metadata":{"id":"KJvmGgfa9ngI","executionInfo":{"status":"ok","timestamp":1687166549878,"user_tz":-420,"elapsed":357,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["for path in os.listdir('/content/dataset/train'):\n","  # try:\n","  #   os.chdir(f'/content/dataset/train_crop/{path}')\n","  # except:\n","  os.mkdir(f'/content/dataset/train_crop/{path}')\n","  os.chdir(f'/content/dataset/train_crop/{path}')\n","  for link in os.listdir('/content/dataset/train/' + path):\n","    try:\n","      originalImage = cv2.imread(f'/content/dataset/train/{path}/{link}')\n","      landmarks,rec = facial_landmarks(originalImage)\n","      (x, y, w, h) = face_utils.rect_to_bb(rec[0])\n","      a = [landmarks[17],landmarks[26],landmarks[33],landmarks[4],landmarks[12]]\n","      img = warp_and_crop_face(originalImage,a,reference_pts=None,crop_size=(96,112),align_type='')\n","      cv2.imwrite(f'{link}', img)\n","    except:\n","      continue"],"metadata":{"id":"7vmqRINx9pW5","executionInfo":{"status":"ok","timestamp":1687166920544,"user_tz":-420,"elapsed":363460,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["for path in os.listdir('/content/dataset/val'):\n","  # try:\n","  #   os.chdir(f'/content/dataset/train_crop/{path}')\n","  # except:\n","  os.mkdir(f'/content/dataset/val_crop/{path}')\n","  os.chdir(f'/content/dataset/val_crop/{path}')\n","  for link in os.listdir('/content/dataset/val/' + path):\n","    try:\n","      originalImage = cv2.imread(f'/content/dataset/val/{path}/{link}')\n","      landmarks,rec = facial_landmarks(originalImage)\n","      (x, y, w, h) = face_utils.rect_to_bb(rec[0])\n","      a = [landmarks[17],landmarks[26],landmarks[33],landmarks[4],landmarks[12]]\n","      img = warp_and_crop_face(originalImage,a,reference_pts=None,crop_size=(96,112),align_type='')\n","      cv2.imwrite(f'{link}', img)\n","    except:\n","      continue"],"metadata":{"id":"Gg_3a_zR9qGG","executionInfo":{"status":"ok","timestamp":1687166983272,"user_tz":-420,"elapsed":62731,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["dataset_train = datasets.ImageFolder(root=\"/content/dataset/train_crop\")\n","dataset_val = datasets.ImageFolder(root=\"/content/dataset/val_crop\")\n","\n","X_train, y_train = dataset_to_embeddings(dataset_train, mtcnn, facenet)\n","X_test, y_test = dataset_to_embeddings(dataset_val, mtcnn, facenet)\n","\n","X_train_class_idx = dataset_train.class_to_idx\n","X_test_class_idx = dataset_val.class_to_idx\n","\n","embeddings, labels, class_to_idx = X_train, y_train, X_train_class_idx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2uhju1j9sD0","executionInfo":{"status":"ok","timestamp":1687167008333,"user_tz":-420,"elapsed":25075,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"0fd635f7-dd29-46cd-d55a-d478dedf5e77"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/dataset/train_crop/Hellas/z3340946320295_903d7490fc28499652fb1db63c8139ca.jpg\n","/content/dataset/train_crop/Hellas/z3340946322989_55b6073bf74e82bd388e8cc5f61c4928.jpg\n","/content/dataset/train_crop/Hellas/z3340946329487_0bc341657d4cd6e5f8c4a3cdced6a473.jpg\n","/content/dataset/train_crop/Hellas/z3340946337250_e2952e7c6f55d4130924103a4af7d69e.jpg\n","/content/dataset/train_crop/Hellas/z3340946337997_7cc8336b72385b95bbbe45e76f3850b0.jpg\n","/content/dataset/train_crop/Hellas/z3340946351201_7b03fe2053ff3d4c519fd693e296431e.jpg\n","/content/dataset/train_crop/Hellas/z3340946352121_e7b6fa28181f9754e872351bad0612ce.jpg\n","/content/dataset/train_crop/Hellas/z3340946357038_65e005498d95bed1ebf6f6923ebfc0a1.jpg\n","/content/dataset/train_crop/Hellas/z3340946362560_837363a994db83ca8273f48b8171737f.jpg\n","/content/dataset/train_crop/Hellas/z3340946363323_952948f29ecda87e075534f1173d164a.jpg\n","/content/dataset/train_crop/Hellas/z3340946364863_64f4fa3b17ba38ca25eac6a04d0a4d76.jpg\n","/content/dataset/train_crop/Hellas/z3340946369194_17e954000bef9958dbc01bf188d61370.jpg\n","/content/dataset/train_crop/Hellas/z3340946370049_69a7fa78763299b3ce96902cd841dddd.jpg\n","/content/dataset/train_crop/Hellas/z3340946376392_37b9ac19fe447598bd669f2bf0f75b3b.jpg\n","/content/dataset/train_crop/Hellas/z3340946380040_19e4255a062d7b9e8f20b794c1e44109.jpg\n","/content/dataset/train_crop/Hellas/z3340946382101_d854b7eb32e3f6840487ed365c4363cd.jpg\n","/content/dataset/train_crop/Hellas/z3340946384632_9d1963df7388b11c2d8de74d8636a968.jpg\n","/content/dataset/train_crop/Hellas/z3340946392970_2fef845d299ec50f9bcd8b061fc8082e.jpg\n","/content/dataset/train_crop/Hellas/z3340946395036_69751dce00ad2907e48d2bb567846617.jpg\n","/content/dataset/train_crop/Hellas/z3340946396871_66139e019ebdc7bd95532720f9e9d852.jpg\n","/content/dataset/train_crop/Hellas/z3340946477746_e065bc9a249071241c5e65d37b2f1127.jpg\n","/content/dataset/train_crop/Huy/1.jpg\n","/content/dataset/train_crop/Huy/10.jpg\n","/content/dataset/train_crop/Huy/11.jpg\n","/content/dataset/train_crop/Huy/12.jpg\n","/content/dataset/train_crop/Huy/13.jpg\n","/content/dataset/train_crop/Huy/14.jpg\n","/content/dataset/train_crop/Huy/15.jpg\n","/content/dataset/train_crop/Huy/17.jpg\n","/content/dataset/train_crop/Huy/19.jpg\n","/content/dataset/train_crop/Huy/2.jpg\n","/content/dataset/train_crop/Huy/21.jpg\n","/content/dataset/train_crop/Huy/22.jpg\n","/content/dataset/train_crop/Huy/23.jpg\n","/content/dataset/train_crop/Huy/24.jpg\n","/content/dataset/train_crop/Huy/25.jpg\n","/content/dataset/train_crop/Huy/26.jpg\n","/content/dataset/train_crop/Huy/27.jpg\n","/content/dataset/train_crop/Huy/29.jpg\n","/content/dataset/train_crop/Huy/3.jpg\n","/content/dataset/train_crop/Huy/30.jpg\n","/content/dataset/train_crop/Huy/6.jpg\n","/content/dataset/train_crop/Huy/9.jpg\n","/content/dataset/train_crop/Nguyen/1.jpg\n","/content/dataset/train_crop/Nguyen/10.jpg\n","/content/dataset/train_crop/Nguyen/12.jpg\n","/content/dataset/train_crop/Nguyen/13.jpg\n","/content/dataset/train_crop/Nguyen/14.jpg\n","/content/dataset/train_crop/Nguyen/15.jpg\n","/content/dataset/train_crop/Nguyen/16.jpg\n","/content/dataset/train_crop/Nguyen/17.jpg\n","/content/dataset/train_crop/Nguyen/19.jpg\n","/content/dataset/train_crop/Nguyen/20.jpg\n","/content/dataset/train_crop/Nguyen/21.jpg\n","/content/dataset/train_crop/Nguyen/22.jpg\n","/content/dataset/train_crop/Nguyen/23.jpg\n","/content/dataset/train_crop/Nguyen/24.jpg\n","/content/dataset/train_crop/Nguyen/25.jpg\n","/content/dataset/train_crop/Nguyen/26.jpg\n","/content/dataset/train_crop/Nguyen/27.jpg\n","/content/dataset/train_crop/Nguyen/28.jpg\n","/content/dataset/train_crop/Nguyen/29.jpg\n","/content/dataset/train_crop/Nguyen/30.jpg\n","/content/dataset/train_crop/Nguyen/5.jpg\n","/content/dataset/train_crop/Nguyen/6.jpg\n","/content/dataset/train_crop/Nguyen/9.jpg\n","/content/dataset/train_crop/Phong/1.jpg\n","/content/dataset/train_crop/Phong/10.jpg\n","/content/dataset/train_crop/Phong/11.jpg\n","/content/dataset/train_crop/Phong/12.jpg\n","/content/dataset/train_crop/Phong/14.jpg\n","/content/dataset/train_crop/Phong/2.jpg\n","/content/dataset/train_crop/Phong/21.jpg\n","Could not find face on /content/dataset/train_crop/Phong/21.jpg\n","/content/dataset/train_crop/Phong/22.jpg\n","/content/dataset/train_crop/Phong/23.jpg\n","/content/dataset/train_crop/Phong/24.jpg\n","/content/dataset/train_crop/Phong/25.jpg\n","/content/dataset/train_crop/Phong/26.jpg\n","/content/dataset/train_crop/Phong/27.jpg\n","/content/dataset/train_crop/Phong/28.jpg\n","/content/dataset/train_crop/Phong/3.jpg\n","/content/dataset/train_crop/Phong/30.jpg\n","/content/dataset/train_crop/Phong/32.jpg\n","/content/dataset/train_crop/Phong/33.jpg\n","/content/dataset/train_crop/Phong/34.jpg\n","/content/dataset/train_crop/Phong/5.jpg\n","/content/dataset/train_crop/Phong/6.jpg\n","/content/dataset/train_crop/Phong/9.jpg\n","/content/dataset/train_crop/Phu/1.jpg\n","/content/dataset/train_crop/Phu/10.jpg\n","/content/dataset/train_crop/Phu/11.jpg\n","/content/dataset/train_crop/Phu/13.jpg\n","/content/dataset/train_crop/Phu/14.jpg\n","/content/dataset/train_crop/Phu/15.jpg\n","/content/dataset/train_crop/Phu/16.jpg\n","/content/dataset/train_crop/Phu/17.jpg\n","/content/dataset/train_crop/Phu/19.jpg\n","/content/dataset/train_crop/Phu/2.jpg\n","/content/dataset/train_crop/Phu/21.jpg\n","/content/dataset/train_crop/Phu/22.jpg\n","/content/dataset/train_crop/Phu/23.jpg\n","/content/dataset/train_crop/Phu/24.jpg\n","/content/dataset/train_crop/Phu/25.jpg\n","/content/dataset/train_crop/Phu/26.jpg\n","/content/dataset/train_crop/Phu/27.jpg\n","/content/dataset/train_crop/Phu/28.jpg\n","/content/dataset/train_crop/Phu/3.jpg\n","/content/dataset/train_crop/Phu/30.jpg\n","/content/dataset/train_crop/Phu/5.jpg\n","/content/dataset/train_crop/Phu/6.jpg\n","/content/dataset/train_crop/Phu/9.jpg\n","/content/dataset/val_crop/Hellas/z3340946330772_f1198bec1275bcdffe6965a8407737ec.jpg\n","/content/dataset/val_crop/Hellas/z3340946375073_3e74522e72dd953a20d1feb6776864b0.jpg\n","/content/dataset/val_crop/Hellas/z3340946388448_a594ff6fd4117380695f4ae5be89229b.jpg\n","/content/dataset/val_crop/Huy/18.jpg\n","/content/dataset/val_crop/Huy/31.jpg\n","/content/dataset/val_crop/Huy/4.jpg\n","/content/dataset/val_crop/Huy/7.jpg\n","/content/dataset/val_crop/Huy/8.jpg\n","/content/dataset/val_crop/Nguyen/18.jpg\n","/content/dataset/val_crop/Nguyen/31.jpg\n","/content/dataset/val_crop/Nguyen/4.jpg\n","/content/dataset/val_crop/Nguyen/7.jpg\n","/content/dataset/val_crop/Nguyen/8.jpg\n","/content/dataset/val_crop/Phong/18.jpg\n","/content/dataset/val_crop/Phong/31.jpg\n","/content/dataset/val_crop/Phong/4.jpg\n","/content/dataset/val_crop/Phong/7.jpg\n","/content/dataset/val_crop/Phong/8.jpg\n","/content/dataset/val_crop/Phu/18.jpg\n","/content/dataset/val_crop/Phu/31.jpg\n","/content/dataset/val_crop/Phu/4.jpg\n","/content/dataset/val_crop/Phu/7.jpg\n","/content/dataset/val_crop/Phu/8.jpg\n"]}]},{"cell_type":"code","source":["!rm -r /content/facenet_emb\n","!mkdir /content/facenet_emb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Bzl5S0l96a5","executionInfo":{"status":"ok","timestamp":1687167008843,"user_tz":-420,"elapsed":523,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"914d16f1-8249-435a-f178-d27ff8833239"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/facenet_emb': No such file or directory\n"]}]},{"cell_type":"code","source":["np.save('/content/facenet_emb/facenet.npy', embeddings)"],"metadata":{"id":"a2SZiKpX98vk","executionInfo":{"status":"ok","timestamp":1687167008845,"user_tz":-420,"elapsed":4,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["!pip install autofaiss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CXlDmUNy9-X6","executionInfo":{"status":"ok","timestamp":1687167018392,"user_tz":-420,"elapsed":9552,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"03adcfa7-f293-44e4-ebcb-f93c5af6b9ee"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting autofaiss\n","  Downloading autofaiss-2.15.8-py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fire<0.5.0,>=0.4.0 (from autofaiss)\n","  Downloading fire-0.4.0.tar.gz (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy<2,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (1.22.4)\n","Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (1.5.3)\n","Requirement already satisfied: pyarrow<13,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (9.0.0)\n","Requirement already satisfied: tqdm<5,>=4.62.3 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (4.65.0)\n","Requirement already satisfied: fsspec>=2022.1.0 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (2023.4.0)\n","Collecting embedding-reader<2,>=1.5.1 (from autofaiss)\n","  Downloading embedding_reader-1.5.1-py3-none-any.whl (18 kB)\n","Collecting faiss-cpu<2,>=1 (from autofaiss)\n","  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire<0.5.0,>=0.4.0->autofaiss) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire<0.5.0,>=0.4.0->autofaiss) (2.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->autofaiss) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->autofaiss) (2022.7.1)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115927 sha256=2138b8c21399ad39e75ba8a8cf0537fa126123f5856f60a12add3ac07f5451fa\n","  Stored in directory: /root/.cache/pip/wheels/26/9a/dd/2818b1b023daf077ec3e625c47ae446aca587a5abe48e05212\n","Successfully built fire\n","Installing collected packages: faiss-cpu, fire, embedding-reader, autofaiss\n","Successfully installed autofaiss-2.15.8 embedding-reader-1.5.1 faiss-cpu-1.7.4 fire-0.4.0\n"]}]},{"cell_type":"code","source":["!autofaiss build_index --embeddings=\"/content/facenet_emb\" \\\n","                    --index_path=\"/content/knn_facenet.index\" \\\n","                    --index_infos_path=\"/content/infos_facenet.json\" \\\n","                    --metric_type=\"ip\" \\\n","                    --max_index_query_time_ms=10 \\\n","                    --max_index_memory_usage=\"4GB\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J1BAf4uk-ARL","executionInfo":{"status":"ok","timestamp":1687167019737,"user_tz":-420,"elapsed":1349,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"0327e7e6-8def-4a5e-b20a-41c1af6f56f0"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-19 09:30:18,395 [INFO]: Using 2 omp threads (processes), consider increasing --nb_cores if you have more\n","2023-06-19 09:30:18,395 [INFO]: Launching the whole pipeline 06/19/2023, 09:30:18\n","2023-06-19 09:30:18,395 [INFO]: Reading total number of vectors and dimension 06/19/2023, 09:30:18\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 10565.00it/s]\n","2023-06-19 09:30:18,465 [INFO]: There are 110 embeddings of dim 512\n","2023-06-19 09:30:18,466 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0704 secs\n","2023-06-19 09:30:18,466 [INFO]: \tCompute estimated construction time of the index 06/19/2023, 09:30:18\n","2023-06-19 09:30:18,466 [INFO]: \t\t-> Train: 16.7 minutes\n","2023-06-19 09:30:18,466 [INFO]: \t\t-> Add: 0.0 seconds\n","2023-06-19 09:30:18,466 [INFO]: \t\tTotal: 16.7 minutes\n","2023-06-19 09:30:18,466 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0002 secs\n","2023-06-19 09:30:18,466 [INFO]: \tChecking that your have enough memory available to create the index 06/19/2023, 09:30:18\n","2023-06-19 09:30:18,467 [INFO]: 242.0KB of memory will be needed to build the index (more might be used if you have more)\n","2023-06-19 09:30:18,467 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0012 secs\n","2023-06-19 09:30:18,467 [INFO]: \tSelecting most promising index types given data characteristics 06/19/2023, 09:30:18\n","2023-06-19 09:30:18,467 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n","2023-06-19 09:30:18,467 [INFO]: \tCreating the index 06/19/2023, 09:30:18\n","2023-06-19 09:30:18,468 [INFO]: \t\t-> Instanciate the index Flat 06/19/2023, 09:30:18\n","2023-06-19 09:30:18,468 [INFO]: \t\t>>> Finished \"-> Instanciate the index Flat\" in 0.0003 secs\n","2023-06-19 09:30:18,468 [INFO]: \t\t-> Adding the vectors to the index 06/19/2023, 09:30:18\n","2023-06-19 09:30:18,468 [INFO]: The memory available for adding the vectors is 32.0GB(total available - used by the index)\n","2023-06-19 09:30:18,468 [INFO]: Using a batch size of 488281 (memory overhead 953.7MB)\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 70.47it/s]\n","2023-06-19 09:30:18,492 [INFO]: \tComputing best hyperparameters for index /content/knn_facenet.index 06/19/2023, 09:30:18\n","2023-06-19 09:30:18,492 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /content/knn_facenet.index\" in 0.0000 secs\n","2023-06-19 09:30:18,492 [INFO]: The best hyperparameters are: \n","2023-06-19 09:30:18,492 [INFO]: \tCompute fast metrics 06/19/2023, 09:30:18\n","\r  0% 0/1 [00:00<?, ?it/s]\r  0% 0/1 [00:00<?, ?it/s]\n","2023-06-19 09:30:18,587 [INFO]: \t>>> Finished \"Compute fast metrics\" in 0.0948 secs\n","2023-06-19 09:30:18,587 [INFO]: \tSaving the index on local disk 06/19/2023, 09:30:18\n","2023-06-19 09:30:18,589 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.0015 secs\n","2023-06-19 09:30:18,589 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 0.1210 secs\n","2023-06-19 09:30:18,589 [INFO]: {\n","2023-06-19 09:30:18,589 [INFO]: \tindex_key: Flat\n","2023-06-19 09:30:18,589 [INFO]: \tindex_param: \n","2023-06-19 09:30:18,589 [INFO]: \tindex_path: /content/knn_facenet.index\n","2023-06-19 09:30:18,589 [INFO]: \tsize in bytes: 225325\n","2023-06-19 09:30:18,589 [INFO]: \tavg_search_speed_ms: 0.03527732153156975\n","2023-06-19 09:30:18,590 [INFO]: \t99p_search_speed_ms: 0.06568287992763558\n","2023-06-19 09:30:18,590 [INFO]: \treconstruction error %: 0.0\n","2023-06-19 09:30:18,590 [INFO]: \tnb vectors: 110\n","2023-06-19 09:30:18,590 [INFO]: \tvectors dimension: 512\n","2023-06-19 09:30:18,590 [INFO]: \tcompression ratio: 0.9998002884722068\n","2023-06-19 09:30:18,590 [INFO]: }\n","2023-06-19 09:30:18,590 [INFO]: \t>>> Finished \"Creating the index\" in 0.1223 secs\n","2023-06-19 09:30:18,590 [INFO]: >>> Finished \"Launching the whole pipeline\" in 0.1948 secs\n","(<faiss.swigfaiss_avx2.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x7ff418568840> >, {'index_key': 'Flat', 'index_param': '', 'index_path': '/content/knn_facenet.index', 'size in bytes': 225325, 'avg_search_speed_ms': 0.03527732153156975, '99p_search_speed_ms': 0.06568287992763558, 'reconstruction error %': 0.0, 'nb vectors': 110, 'vectors dimension': 512, 'compression ratio': 0.9998002884722068})\n"]}]},{"cell_type":"code","source":["import faiss\n","import torch\n","import os\n","import pandas as pd\n","from collections import defaultdict"],"metadata":{"id":"UXGopoU1-B_N","executionInfo":{"status":"ok","timestamp":1687167019738,"user_tz":-420,"elapsed":13,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["ind = faiss.read_index(\"/content/knn_facenet.index\")"],"metadata":{"id":"QupuAwp7-Dda","executionInfo":{"status":"ok","timestamp":1687167019738,"user_tz":-420,"elapsed":12,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["preds = []\n","k = 3\n","for image_emb in X_test:\n","    D, I = ind.search(image_emb[np.newaxis, :], k)\n","    print(D,I)\n","    i_candidate = defaultdict(int)\n","    for D_ele,I_ele in zip(D[0],I[0]):\n","      # if D_ele > 0.7:\n","        cls = labels[I_ele]\n","        i_candidate[cls] += 1\n","    # try:\n","    key_with_max_value = max(i_candidate, key=lambda k: i_candidate[k])\n","    # except:\n","    #   print(\"error\")\n","    #   key_with_max_value = 0\n","\n","    preds.append(key_with_max_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYiohlK_-Fo_","executionInfo":{"status":"ok","timestamp":1687167174949,"user_tz":-420,"elapsed":3,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"b5ae105e-39df-49f2-d392-d828601260a1"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.9244436  0.910566   0.90784436]] [[3 5 2]]\n","[[0.95380974 0.9394542  0.9340483 ]] [[11 13 14]]\n","[[0.81213397 0.81093276 0.80566835]] [[ 8 20 17]]\n","[[0.7024101  0.69576365 0.67275786]] [[82 96 44]]\n","[[0.8273859  0.8075385  0.77624667]] [[32 29 22]]\n","[[0.89393556 0.73590124 0.72063905]] [[25 33 36]]\n","[[0.79905057 0.78677183 0.7855057 ]] [[40 23 28]]\n","[[0.8883059 0.8100925 0.7761458]] [[40 23 28]]\n","[[0.9178004  0.89162153 0.8780757 ]] [[65 54 50]]\n","[[0.80750763 0.8058837  0.7815418 ]] [[60 62 54]]\n","[[0.89923334 0.8953867  0.8267541 ]] [[47 58 55]]\n","[[0.8883485 0.8808952 0.8447958]] [[63 54 50]]\n","[[0.884149   0.8391694  0.83591574]] [[46 48 45]]\n","[[0.9574333  0.95743054 0.9489771 ]] [[69 77 71]]\n","[[0.9649584  0.96062565 0.9598875 ]] [[85 86 76]]\n","[[0.90813273 0.8930341  0.8795388 ]] [[81 80 68]]\n","[[0.825261   0.8249568  0.75583285]] [[70 83 78]]\n","[[0.94777375 0.94063264 0.935001  ]] [[67 79 69]]\n","[[0.88629234 0.8796291  0.84141845]] [[90 93 94]]\n","[[0.95361906 0.9443839  0.90837586]] [[109  95  92]]\n","[[0.89733374 0.8052831  0.80099434]] [[ 96 100  93]]\n","[[0.94364667 0.9113654  0.9057312 ]] [[104  95 108]]\n","[[0.88673025 0.87885433 0.8769703 ]] [[101  94 105]]\n"]}]},{"cell_type":"code","source":["from sklearn import metrics\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"31S62ppb-HUd","executionInfo":{"status":"ok","timestamp":1687167019738,"user_tz":-420,"elapsed":6,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["test_class_idx = dataset_val.class_to_idx\n","\n","idx_to_class = {v: k for k, v in test_class_idx.items()}\n","print(idx_to_class)\n","\n","target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(target_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"svY8Y5K1-JL5","executionInfo":{"status":"ok","timestamp":1687167179420,"user_tz":-420,"elapsed":292,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"d0cb6c91-df95-4dbc-8ee1-9531e8033475"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'Hellas', 1: 'Huy', 2: 'Nguyen', 3: 'Phong', 4: 'Phu'}\n","['Hellas', 'Huy', 'Nguyen', 'Phong', 'Phu']\n"]}]},{"cell_type":"code","source":["target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(y_test, preds, target_names=target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhfWDAZ5-L8a","executionInfo":{"status":"ok","timestamp":1687167181477,"user_tz":-420,"elapsed":3,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"f825db3a-3652-42fd-cb31-937b3f15cb93"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","      Hellas       1.00      1.00      1.00         3\n","         Huy       1.00      0.80      0.89         5\n","      Nguyen       1.00      1.00      1.00         5\n","       Phong       0.83      1.00      0.91         5\n","         Phu       1.00      1.00      1.00         5\n","\n","    accuracy                           0.96        23\n","   macro avg       0.97      0.96      0.96        23\n","weighted avg       0.96      0.96      0.96        23\n","\n"]}]}]}