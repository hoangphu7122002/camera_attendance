{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"20633b60d4e343faa8ba78bb38083c6d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2baf95c753846a7b66689ea2c7c5e65","IPY_MODEL_24d8ca1f975244569a111879f7af2014","IPY_MODEL_1f369a488fd44918a151efa201b2a9e1"],"layout":"IPY_MODEL_e1322d9ddf4840a7aadbea1936e68951"}},"f2baf95c753846a7b66689ea2c7c5e65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec334799a89f4c14a3b19ffe6c20eaa3","placeholder":"​","style":"IPY_MODEL_5cbe0ef580624a0190954b51f88fb564","value":"100%"}},"24d8ca1f975244569a111879f7af2014":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2788ef9efe8b49158530d69f0f14cac6","max":111898327,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6bfbc04ea00e40e8bb7283965d5307cd","value":111898327}},"1f369a488fd44918a151efa201b2a9e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bfbacb98f824b9cbe149de6457760ec","placeholder":"​","style":"IPY_MODEL_7b160be922124e9aa2706a8935117c89","value":" 107M/107M [00:00&lt;00:00, 218MB/s]"}},"e1322d9ddf4840a7aadbea1936e68951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec334799a89f4c14a3b19ffe6c20eaa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cbe0ef580624a0190954b51f88fb564":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2788ef9efe8b49158530d69f0f14cac6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bfbc04ea00e40e8bb7283965d5307cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1bfbacb98f824b9cbe149de6457760ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b160be922124e9aa2706a8935117c89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install facenet-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkpronfxmWkC","executionInfo":{"status":"ok","timestamp":1687163568329,"user_tz":-420,"elapsed":4693,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"ef4be772-1b7e-40ba-d08f-692e6843fd74"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting facenet-pytorch\n","  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.27.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.15.2+cu118)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (8.4.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.4)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision->facenet-pytorch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision->facenet-pytorch) (1.3.0)\n","Installing collected packages: facenet-pytorch\n","Successfully installed facenet-pytorch-2.5.3\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YVH7ny4T-0qq","executionInfo":{"status":"ok","timestamp":1687163592182,"user_tz":-420,"elapsed":13756,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"outputs":[],"source":["from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n","import torch\n","from torch.utils.data import DataLoader, SubsetRandomSampler\n","from torch import optim\n","from torch.optim.lr_scheduler import MultiStepLR\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets, transforms\n","import numpy as np\n","import os\n","from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n","from sklearn import svm\n","from PIL import Image\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","source":["**Mounted Drive**"],"metadata":{"id":"t5psElipmY38"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WvV__wRXmZ3s","executionInfo":{"status":"ok","timestamp":1687163620518,"user_tz":-420,"elapsed":18679,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"6e2ecfc0-c424-446a-b7ca-f68139e377fe"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/faceRecognition/dataset/5_celeb.zip -d dataset"],"metadata":{"id":"wbohwWKCoeiV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Util Function**"],"metadata":{"id":"zLjX8Ot8nC9d"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","mtcnn = MTCNN(\n","    image_size=160, margin=0, min_face_size=20,\n","    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n","    device=device\n",")\n","\n","facenet = InceptionResnetV1(pretrained='vggface2').eval()\n","facenet = facenet.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["20633b60d4e343faa8ba78bb38083c6d","f2baf95c753846a7b66689ea2c7c5e65","24d8ca1f975244569a111879f7af2014","1f369a488fd44918a151efa201b2a9e1","e1322d9ddf4840a7aadbea1936e68951","ec334799a89f4c14a3b19ffe6c20eaa3","5cbe0ef580624a0190954b51f88fb564","2788ef9efe8b49158530d69f0f14cac6","6bfbc04ea00e40e8bb7283965d5307cd","1bfbacb98f824b9cbe149de6457760ec","7b160be922124e9aa2706a8935117c89"]},"id":"w5JUzMa8pOxv","executionInfo":{"status":"ok","timestamp":1687163645290,"user_tz":-420,"elapsed":1798,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"6a6eda88-5dcf-47bd-fd13-de37bcb97a90"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/107M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20633b60d4e343faa8ba78bb38083c6d"}},"metadata":{}}]},{"cell_type":"code","source":["def whitens(img):\n","    mean = img.mean()\n","    std = img.std()\n","    std_adj = std.clamp(min=1.0 / (float(img.numel()) ** 0.5))\n","    y = (img - mean) / std_adj\n","    return y\n","\n","def extract_features(mtcnn, facenet, img):\n","    img = img.to(device)\n","    img = transforms.ToPILImage()(img.squeeze_(0))\n","    bbs, _ = mtcnn.detect(img)\n","    if bbs is None:\n","        # if no face is detected\n","        return None, None\n","\n","    faces = torch.stack([extract_face(img, bb) for bb in bbs])\n","    embeddings = facenet(whitens(faces)).detach().numpy()\n","\n","    return bbs, embeddings\n","\n","def dataset_to_embeddings(dataset, mtcnn, facenet):\n","    transform = transforms.Compose([\n","        transforms.Resize(160),\n","        transforms.ToTensor()\n","    ])\n","\n","    embeddings = []\n","    labels = []\n","    for img_path, label in dataset.samples:\n","        print(img_path)\n","\n","        _, embedding = extract_features(mtcnn, facenet, transform(Image.open(img_path).convert('RGB')).unsqueeze_(0))\n","        if embedding is None:\n","            print(\"Could not find face on {}\".format(img_path))\n","            continue\n","        if embedding.shape[0] > 1:\n","            print(\"Multiple faces detected for {}, taking one with highest probability\".format(img_path))\n","            embedding = embedding[0, :]\n","        embeddings.append(embedding.flatten())\n","        labels.append(label)\n","\n","    return np.stack(embeddings), labels\n","\n","def train(embeddings, labels):\n","    clf = svm.SVC(probability=True)\n","    clf.fit(embeddings, labels)\n","    return clf"],"metadata":{"id":"2HfxdO8vmrqM","executionInfo":{"status":"ok","timestamp":1687163647041,"user_tz":-420,"elapsed":599,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**Dataset**"],"metadata":{"id":"v_VAxHGVpjER"}},{"cell_type":"code","source":["# Create the ImageFolder dataset\n","dataset_train = datasets.ImageFolder(root=\"/content/dataset/train\")\n","dataset_val = datasets.ImageFolder(root=\"/content/dataset/val\")"],"metadata":{"id":"TE-zkaOqpD6J","executionInfo":{"status":"ok","timestamp":1687163651143,"user_tz":-420,"elapsed":3,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize(160),\n","    transforms.ToTensor()\n","])\n","\n","test = extract_features(mtcnn, facenet, transform(dataset_train[30][0]))"],"metadata":{"id":"byr1v_26pkXq","executionInfo":{"status":"ok","timestamp":1687163653080,"user_tz":-420,"elapsed":639,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["test[1].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BRfFLICGqwcQ","executionInfo":{"status":"ok","timestamp":1687163654739,"user_tz":-420,"elapsed":2,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"e0e8520a-7893-46bd-f575-9e88df6b123e"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 512)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","transform = transforms.Compose([\n","    transforms.Resize(160),\n","    # transforms.ToTensor()\n","])\n","\n","def plot_img_bbox(img,target):\n","    # plot the image and bboxes\n","    # Bounding boxes are defined as follows: x-min y-min width height\n","    fig, a = plt.subplots(1,1)\n","    fig.set_size_inches(5,5)\n","    a.imshow(img)\n","    for box in target:\n","        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n","        rect = patches.Rectangle((x, y),\n","                                 width, height,\n","                                 linewidth = 2,\n","                                 edgecolor = 'r',\n","                                 facecolor = 'none')\n","\n","        # Draw the bounding box on top of the image\n","        a.add_patch(rect)\n","    plt.show()\n","\n","# plotting the image with bboxes. Feel free to change the index\n","img = transform(dataset_train[30][0])\n","target = test[0]\n","print(test[1])\n","plot_img_bbox(img,target)"],"metadata":{"id":"cXi3t9QerM7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Facenet + SVM**"],"metadata":{"id":"8dmTus5psmTT"}},{"cell_type":"code","source":["X_train, y_train = dataset_to_embeddings(dataset_train, mtcnn, facenet)\n","X_test, y_test = dataset_to_embeddings(dataset_val, mtcnn, facenet)\n","\n","X_train_class_idx = dataset_train.class_to_idx\n","X_test_class_idx = dataset_val.class_to_idx\n","\n","embeddings, labels, class_to_idx = X_train, y_train, X_train_class_idx"],"metadata":{"id":"Xypk5_VXsqxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf = train(embeddings, labels)"],"metadata":{"id":"0U2rHfN7uAvf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2X1lY50BuYC9","executionInfo":{"status":"ok","timestamp":1687095449958,"user_tz":-420,"elapsed":312,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"71b0ceb7-164d-4dd1-e446-83e8b53aef56"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["idx_to_class = {v: k for k, v in class_to_idx.items()}\n","print(idx_to_class)\n","\n","target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(labels, clf.predict(embeddings), target_names=target_names))\n","\n","# Predict labels for validation set and calculate accuracy\n","y_val_pred = clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_val_pred)\n","print('Validation Accuracy: {:.2f}%'.format(accuracy*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUXtZTAMuKpc","executionInfo":{"status":"ok","timestamp":1687095427468,"user_tz":-420,"elapsed":292,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"1913bca8-31eb-4aca-e57b-87f75922a861"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'ben_afflek', 1: 'elton_john', 2: 'jerry_seinfeld', 3: 'madonna', 4: 'mindy_kaling'}\n","                precision    recall  f1-score   support\n","\n","    ben_afflek       1.00      1.00      1.00        14\n","    elton_john       1.00      1.00      1.00        16\n","jerry_seinfeld       1.00      1.00      1.00        21\n","       madonna       1.00      1.00      1.00        19\n","  mindy_kaling       1.00      1.00      1.00        22\n","\n","      accuracy                           1.00        92\n","     macro avg       1.00      1.00      1.00        92\n","  weighted avg       1.00      1.00      1.00        92\n","\n","Validation Accuracy: 100.00%\n"]}]},{"cell_type":"markdown","source":["#Facenet Only"],"metadata":{"id":"KDSYI97Cvywe"}},{"cell_type":"code","source":["print(len(idx_to_class))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEWWRxpWwosi","executionInfo":{"status":"ok","timestamp":1687096047683,"user_tz":-420,"elapsed":3,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"10e8af2f-e44f-4594-dbab-51bb724d9cc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n"]}]},{"cell_type":"code","source":["resnet = InceptionResnetV1(\n","    classify=True,\n","    pretrained='vggface2',\n","    num_classes=len(idx_to_class)\n",").to(device)"],"metadata":{"id":"eH-E33fjv0P7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train**"],"metadata":{"id":"qqwBVTsEy0BZ"}},{"cell_type":"code","source":["data_dir = '/content/dataset/train'\n","dataset_train = datasets.ImageFolder(data_dir, transform=transforms.Resize((512,512)))\n","dataset_train.samples = [\n","    (p, p.replace(data_dir, data_dir + '_cropped'))\n","        for p, _ in dataset_train.samples\n","]"],"metadata":{"id":"59Z7wg1pxRER","executionInfo":{"status":"ok","timestamp":1687163703032,"user_tz":-420,"elapsed":404,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","epochs = 20\n","workers = 0 if os.name == 'nt' else 8"],"metadata":{"id":"_s9xJGQNyX8_","executionInfo":{"status":"ok","timestamp":1687163704803,"user_tz":-420,"elapsed":2,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(\n","    dataset_train,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    collate_fn=training.collate_pil\n",")\n","\n","for i, (x, y) in enumerate(train_loader):\n","    print(x)\n","    print(mtcnn(x))\n","    break\n","    # mtcnn(x, save_path=y)\n","    # print('\\rBatch {} of {}'.format(i + 1, len(train_loader)), end='')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvPrcZ47xtCn","executionInfo":{"status":"ok","timestamp":1687163728688,"user_tz":-420,"elapsed":6119,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"7bfd0ad7-ab5e-4ce4-c44b-86105adc94e4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[<PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DE9BA90>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEE30>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEECE0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEECB0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEDA0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCED270>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEC50>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEED10>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEE60>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEE00>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEDD0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEED70>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEED40>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEE90>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEEC0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEEF0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEF20>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEF50>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEF80>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEFB0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEEFE0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF010>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF040>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF070>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF0A0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF0D0>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF100>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF130>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF160>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF190>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF490>, <PIL.Image.Image image mode=RGB size=512x512 at 0x7F6F6DCEF4C0>]\n","[tensor([[[-0.7695, -0.8008, -0.8477,  ..., -0.3867, -0.5195, -0.6133],\n","         [-0.8008, -0.8320, -0.8711,  ..., -0.1680, -0.3320, -0.4648],\n","         [-0.8242, -0.8477, -0.8789,  ..., -0.2617, -0.2773, -0.3008],\n","         ...,\n","         [ 0.7773,  0.7695,  0.7539,  ..., -0.8242, -0.8164, -0.8086],\n","         [ 0.8242,  0.8164,  0.8164,  ..., -0.8555, -0.8477, -0.8477],\n","         [ 0.8633,  0.8555,  0.8633,  ..., -0.8242, -0.8164, -0.8164]],\n","\n","        [[-0.7461, -0.7773, -0.8242,  ..., -0.4258, -0.5664, -0.6602],\n","         [-0.7695, -0.8008, -0.8398,  ..., -0.2070, -0.3711, -0.5117],\n","         [-0.7930, -0.8164, -0.8398,  ..., -0.3008, -0.3164, -0.3320],\n","         ...,\n","         [ 0.7773,  0.7617,  0.7461,  ..., -0.7617, -0.7539, -0.7461],\n","         [ 0.8242,  0.8164,  0.8086,  ..., -0.7930, -0.7852, -0.7852],\n","         [ 0.8555,  0.8477,  0.8477,  ..., -0.7617, -0.7539, -0.7539]],\n","\n","        [[-0.7070, -0.7383, -0.7852,  ..., -0.4336, -0.5664, -0.6602],\n","         [-0.7383, -0.7695, -0.8086,  ..., -0.2070, -0.3711, -0.5039],\n","         [-0.7617, -0.7852, -0.8164,  ..., -0.2930, -0.3086, -0.3242],\n","         ...,\n","         [ 0.5508,  0.5352,  0.5117,  ..., -0.7383, -0.7305, -0.7227],\n","         [ 0.6055,  0.5898,  0.5742,  ..., -0.7695, -0.7617, -0.7617],\n","         [ 0.6367,  0.6211,  0.6133,  ..., -0.7383, -0.7305, -0.7305]]]), tensor([[[ 0.9180,  0.9180,  0.8633,  ..., -0.8398, -0.8633, -0.8867],\n","         [ 0.8867,  0.8242,  0.6445,  ..., -0.8398, -0.8555, -0.8789],\n","         [ 0.8164,  0.5586,  0.1680,  ..., -0.8320, -0.8242, -0.8242],\n","         ...,\n","         [ 0.1914,  0.1055,  0.0117,  ...,  0.8477,  0.8789,  0.8477],\n","         [-0.5508, -0.5820, -0.6367,  ...,  0.8398,  0.8867,  0.7539],\n","         [-0.7617, -0.7773, -0.8008,  ...,  0.7930,  0.7070,  0.4336]],\n","\n","        [[ 0.7227,  0.7305,  0.6836,  ..., -0.8477, -0.8633, -0.8867],\n","         [ 0.7148,  0.6445,  0.4727,  ..., -0.8477, -0.8633, -0.8789],\n","         [ 0.6445,  0.3867,  0.0117,  ..., -0.8398, -0.8320, -0.8320],\n","         ...,\n","         [ 0.0586, -0.0195, -0.1133,  ...,  0.8398,  0.8711,  0.8398],\n","         [-0.6211, -0.6445, -0.6914,  ...,  0.8320,  0.8789,  0.7461],\n","         [-0.7695, -0.7773, -0.8008,  ...,  0.7852,  0.6992,  0.4258]],\n","\n","        [[ 0.6367,  0.6367,  0.5898,  ..., -0.8867, -0.9023, -0.9258],\n","         [ 0.6133,  0.5430,  0.3711,  ..., -0.8867, -0.9023, -0.9180],\n","         [ 0.5430,  0.2852, -0.0898,  ..., -0.8867, -0.8711, -0.8711],\n","         ...,\n","         [-0.0195, -0.0977, -0.1914,  ...,  0.8789,  0.9023,  0.8789],\n","         [-0.6914, -0.7148, -0.7539,  ...,  0.8711,  0.9023,  0.7695],\n","         [-0.8242, -0.8320, -0.8477,  ...,  0.8242,  0.7383,  0.4570]]]), tensor([[[-0.4102, -0.4102, -0.4727,  ..., -0.5898, -0.5898, -0.5898],\n","         [-0.4102, -0.4102, -0.4727,  ..., -0.5977, -0.5977, -0.5977],\n","         [-0.3945, -0.3945, -0.4570,  ..., -0.6914, -0.6914, -0.6914],\n","         ...,\n","         [ 0.5195,  0.5195,  0.5273,  ..., -0.8164, -0.8320, -0.8320],\n","         [ 0.7227,  0.7227,  0.7227,  ..., -0.8633, -0.8867, -0.8867],\n","         [ 0.7461,  0.7461,  0.7461,  ..., -0.8711, -0.8945, -0.8945]],\n","\n","        [[-0.5273, -0.5273, -0.5898,  ..., -0.6758, -0.6758, -0.6758],\n","         [-0.5273, -0.5273, -0.5898,  ..., -0.6836, -0.6836, -0.6836],\n","         [-0.5195, -0.5195, -0.5820,  ..., -0.7773, -0.7773, -0.7773],\n","         ...,\n","         [ 0.2695,  0.2695,  0.2773,  ..., -0.8242, -0.8320, -0.8320],\n","         [ 0.4492,  0.4492,  0.4414,  ..., -0.8789, -0.8945, -0.8945],\n","         [ 0.4727,  0.4727,  0.4648,  ..., -0.8867, -0.9023, -0.9023]],\n","\n","        [[-0.5352, -0.5352, -0.5977,  ..., -0.6211, -0.6133, -0.6133],\n","         [-0.5352, -0.5352, -0.5977,  ..., -0.6289, -0.6211, -0.6211],\n","         [-0.5195, -0.5195, -0.5898,  ..., -0.7227, -0.7148, -0.7148],\n","         ...,\n","         [ 0.1680,  0.1680,  0.1680,  ..., -0.8164, -0.8398, -0.8398],\n","         [ 0.3398,  0.3398,  0.3320,  ..., -0.8789, -0.9023, -0.9023],\n","         [ 0.3633,  0.3633,  0.3555,  ..., -0.8867, -0.9102, -0.9102]]]), tensor([[[-0.7227, -0.7227, -0.7227,  ..., -0.2930, -0.2930, -0.2930],\n","         [-0.6914, -0.6914, -0.6914,  ..., -0.2227, -0.2305, -0.2383],\n","         [-0.6602, -0.6602, -0.6602,  ..., -0.1289, -0.1445, -0.1602],\n","         ...,\n","         [-0.0742, -0.0742, -0.0820,  ...,  0.9102,  0.9180,  0.9180],\n","         [-0.0820, -0.0820, -0.0898,  ...,  0.9180,  0.9258,  0.9258],\n","         [-0.0664, -0.0664, -0.0742,  ...,  0.9180,  0.9180,  0.9180]],\n","\n","        [[-0.8320, -0.8398, -0.8477,  ..., -0.6289, -0.6211, -0.6211],\n","         [-0.8086, -0.8086, -0.8164,  ..., -0.5664, -0.5664, -0.5664],\n","         [-0.7852, -0.7852, -0.7930,  ..., -0.4883, -0.4961, -0.5039],\n","         ...,\n","         [-0.3008, -0.3008, -0.3086,  ...,  0.8242,  0.8320,  0.8320],\n","         [-0.3008, -0.3086, -0.3164,  ...,  0.8320,  0.8398,  0.8398],\n","         [-0.2773, -0.2852, -0.2930,  ...,  0.8320,  0.8320,  0.8320]],\n","\n","        [[-0.8555, -0.8633, -0.8789,  ..., -0.7539, -0.7539, -0.7539],\n","         [-0.8398, -0.8477, -0.8555,  ..., -0.7070, -0.7148, -0.7148],\n","         [-0.8164, -0.8242, -0.8320,  ..., -0.6445, -0.6523, -0.6602],\n","         ...,\n","         [-0.3867, -0.3867, -0.4023,  ...,  0.7773,  0.7852,  0.7852],\n","         [-0.3867, -0.3945, -0.4102,  ...,  0.7852,  0.7930,  0.7930],\n","         [-0.3633, -0.3711, -0.3867,  ...,  0.7852,  0.7852,  0.7852]]]), tensor([[[-0.4414, -0.4570, -0.4805,  ..., -0.3086, -0.2773, -0.1914],\n","         [-0.4570, -0.4727, -0.4727,  ..., -0.3164, -0.3633, -0.3242],\n","         [-0.4648, -0.4805, -0.4727,  ..., -0.3789, -0.4570, -0.4258],\n","         ...,\n","         [-0.4961, -0.5039, -0.5273,  ...,  0.3477,  0.3242,  0.2930],\n","         [-0.4961, -0.5039, -0.5273,  ...,  0.2852,  0.2852,  0.2773],\n","         [-0.4961, -0.4961, -0.5117,  ...,  0.3008,  0.3008,  0.2930]],\n","\n","        [[-0.2617, -0.2695, -0.2852,  ..., -0.3398, -0.3086, -0.2227],\n","         [-0.2773, -0.2852, -0.2852,  ..., -0.3477, -0.3945, -0.3555],\n","         [-0.2930, -0.3008, -0.2852,  ..., -0.4102, -0.4883, -0.4570],\n","         ...,\n","         [-0.3164, -0.3242, -0.3477,  ...,  0.0273,  0.0117, -0.0195],\n","         [-0.3164, -0.3242, -0.3477,  ..., -0.0352, -0.0352, -0.0430],\n","         [-0.3164, -0.3164, -0.3320,  ..., -0.0273, -0.0195, -0.0273]],\n","\n","        [[-0.4805, -0.4883, -0.5117,  ..., -0.3477, -0.3164, -0.2305],\n","         [-0.4961, -0.5039, -0.5117,  ..., -0.3555, -0.4023, -0.3633],\n","         [-0.5117, -0.5195, -0.5117,  ..., -0.4180, -0.4961, -0.4648],\n","         ...,\n","         [-0.5352, -0.5430, -0.5664,  ..., -0.1133, -0.1289, -0.1602],\n","         [-0.5352, -0.5430, -0.5664,  ..., -0.1758, -0.1758, -0.1836],\n","         [-0.5352, -0.5352, -0.5508,  ..., -0.1602, -0.1602, -0.1680]]]), tensor([[[-0.9570, -0.9570, -0.9492,  ..., -0.8320, -0.8477, -0.8633],\n","         [-0.9570, -0.9570, -0.9492,  ..., -0.8398, -0.8555, -0.8711],\n","         [-0.9570, -0.9570, -0.9492,  ..., -0.8398, -0.8711, -0.8867],\n","         ...,\n","         [ 0.8320,  0.8398,  0.8477,  ..., -0.8633, -0.8633, -0.8633],\n","         [ 0.7930,  0.8086,  0.8242,  ..., -0.8633, -0.8633, -0.8633],\n","         [ 0.7461,  0.7461,  0.7383,  ..., -0.8711, -0.8633, -0.8633]],\n","\n","        [[-0.9570, -0.9570, -0.9492,  ..., -0.8789, -0.9023, -0.9258],\n","         [-0.9492, -0.9492, -0.9414,  ..., -0.8867, -0.9102, -0.9336],\n","         [-0.9492, -0.9492, -0.9414,  ..., -0.8945, -0.9258, -0.9492],\n","         ...,\n","         [ 0.7852,  0.7773,  0.7617,  ..., -0.9102, -0.9023, -0.9023],\n","         [ 0.7773,  0.7773,  0.7852,  ..., -0.9102, -0.9023, -0.9023],\n","         [ 0.7539,  0.7461,  0.7383,  ..., -0.9023, -0.8945, -0.8945]],\n","\n","        [[-0.9727, -0.9727, -0.9648,  ..., -0.8867, -0.9180, -0.9414],\n","         [-0.9805, -0.9805, -0.9727,  ..., -0.8945, -0.9258, -0.9492],\n","         [-0.9883, -0.9883, -0.9805,  ..., -0.9023, -0.9336, -0.9570],\n","         ...,\n","         [ 0.6758,  0.6680,  0.6523,  ..., -0.9023, -0.9102, -0.9102],\n","         [ 0.6758,  0.6836,  0.6836,  ..., -0.9023, -0.9102, -0.9102],\n","         [ 0.6680,  0.6602,  0.6445,  ..., -0.8945, -0.9023, -0.9023]]]), tensor([[[-0.5195, -0.5117, -0.5039,  ...,  0.9570,  0.9648,  0.9727],\n","         [-0.5117, -0.5039, -0.4961,  ...,  0.9570,  0.9727,  0.9805],\n","         [-0.5117, -0.5039, -0.4961,  ...,  0.9570,  0.9727,  0.9805],\n","         ...,\n","         [ 0.2227,  0.2148,  0.1992,  ..., -0.5898, -0.5586, -0.5508],\n","         [ 0.2070,  0.1992,  0.1836,  ..., -0.6211, -0.6055, -0.5977],\n","         [ 0.1992,  0.1914,  0.1758,  ..., -0.6523, -0.6445, -0.6367]],\n","\n","        [[-0.6445, -0.6445, -0.6367,  ...,  0.9727,  0.9727,  0.9727],\n","         [-0.6523, -0.6445, -0.6367,  ...,  0.9805,  0.9805,  0.9805],\n","         [-0.6523, -0.6445, -0.6367,  ...,  0.9805,  0.9805,  0.9805],\n","         ...,\n","         [-0.0273, -0.0430, -0.0664,  ..., -0.6680, -0.6367, -0.6211],\n","         [-0.0430, -0.0586, -0.0820,  ..., -0.6992, -0.6758, -0.6680],\n","         [-0.0508, -0.0664, -0.0898,  ..., -0.7227, -0.7148, -0.7070]],\n","\n","        [[-0.6680, -0.6680, -0.6602,  ...,  0.9570,  0.9570,  0.9570],\n","         [-0.6680, -0.6680, -0.6602,  ...,  0.9648,  0.9648,  0.9648],\n","         [-0.6680, -0.6680, -0.6602,  ...,  0.9648,  0.9648,  0.9648],\n","         ...,\n","         [-0.0820, -0.0977, -0.1211,  ..., -0.7070, -0.6445, -0.6133],\n","         [-0.0898, -0.1055, -0.1367,  ..., -0.7383, -0.6836, -0.6602],\n","         [-0.0977, -0.1133, -0.1445,  ..., -0.7617, -0.7227, -0.6992]]]), tensor([[[-0.1602, -0.1445, -0.2227,  ...,  0.3633,  0.3633,  0.3633],\n","         [-0.2617, -0.2930, -0.3711,  ...,  0.3711,  0.3711,  0.3711],\n","         [-0.4727, -0.4961, -0.5039,  ...,  0.3789,  0.3789,  0.3789],\n","         ...,\n","         [-0.8789, -0.8789, -0.8789,  ...,  0.0430,  0.0352,  0.0273],\n","         [-0.8320, -0.8398, -0.8398,  ...,  0.0586,  0.0430,  0.0273],\n","         [-0.8086, -0.8164, -0.8242,  ...,  0.0430,  0.0352,  0.0195]],\n","\n","        [[-0.2227, -0.2148, -0.2852,  ...,  0.6602,  0.6602,  0.6602],\n","         [-0.3164, -0.3555, -0.4258,  ...,  0.6680,  0.6680,  0.6680],\n","         [-0.5195, -0.5430, -0.5508,  ...,  0.6758,  0.6758,  0.6758],\n","         ...,\n","         [-0.8320, -0.8477, -0.8477,  ...,  0.0430,  0.0352,  0.0273],\n","         [-0.7852, -0.8008, -0.8008,  ...,  0.0586,  0.0430,  0.0273],\n","         [-0.7539, -0.7695, -0.7695,  ...,  0.0430,  0.0352,  0.0195]],\n","\n","        [[-0.2227, -0.2148, -0.2930,  ...,  0.9648,  0.9648,  0.9648],\n","         [-0.3086, -0.3398, -0.4180,  ...,  0.9727,  0.9727,  0.9727],\n","         [-0.4961, -0.5195, -0.5195,  ...,  0.9805,  0.9805,  0.9805],\n","         ...,\n","         [-0.7383, -0.7539, -0.7539,  ...,  0.0273,  0.0195,  0.0117],\n","         [-0.6914, -0.7070, -0.7148,  ...,  0.0430,  0.0273,  0.0117],\n","         [-0.6523, -0.6680, -0.6758,  ...,  0.0273,  0.0195,  0.0039]]]), tensor([[[ 0.7773,  0.7773,  0.7773,  ..., -0.8477, -0.8477, -0.8477],\n","         [ 0.7773,  0.7773,  0.7773,  ..., -0.8711, -0.8711, -0.8789],\n","         [ 0.7617,  0.7617,  0.7695,  ..., -0.8789, -0.8711, -0.8711],\n","         ...,\n","         [ 0.7617,  0.7617,  0.7617,  ...,  0.3164,  0.3008,  0.2852],\n","         [ 0.7617,  0.7617,  0.7617,  ...,  0.2695,  0.2617,  0.2617],\n","         [ 0.7617,  0.7617,  0.7539,  ...,  0.2695,  0.2539,  0.2383]],\n","\n","        [[ 0.8164,  0.8164,  0.8164,  ..., -0.8867, -0.8789, -0.8789],\n","         [ 0.8164,  0.8164,  0.8164,  ..., -0.9102, -0.9102, -0.9102],\n","         [ 0.8164,  0.8164,  0.8164,  ..., -0.9258, -0.9180, -0.9023],\n","         ...,\n","         [ 0.8633,  0.8633,  0.8633,  ...,  0.3008,  0.3008,  0.3008],\n","         [ 0.8633,  0.8633,  0.8633,  ...,  0.2773,  0.2773,  0.2773],\n","         [ 0.8633,  0.8633,  0.8633,  ...,  0.2773,  0.2695,  0.2617]],\n","\n","        [[ 0.7617,  0.7617,  0.7617,  ..., -0.9180, -0.8945, -0.8867],\n","         [ 0.7617,  0.7617,  0.7617,  ..., -0.9414, -0.9258, -0.9180],\n","         [ 0.7539,  0.7539,  0.7539,  ..., -0.9648, -0.9492, -0.9336],\n","         ...,\n","         [ 0.9258,  0.9258,  0.9258,  ...,  0.2383,  0.2461,  0.2539],\n","         [ 0.9258,  0.9258,  0.9258,  ...,  0.2227,  0.2227,  0.2305],\n","         [ 0.9258,  0.9258,  0.9258,  ...,  0.2383,  0.2227,  0.2070]]]), tensor([[[-0.9961, -0.9961, -0.9961,  ..., -0.9727, -0.9883, -0.9883],\n","         [-0.9961, -0.9961, -0.9961,  ..., -0.9727, -0.9805, -0.9883],\n","         [-0.9961, -0.9961, -0.9961,  ..., -0.9570, -0.9648, -0.9805],\n","         ...,\n","         [-0.9883, -0.9961, -0.9805,  ..., -0.5586, -0.7148, -0.5117],\n","         [-0.9961, -0.9883, -0.9648,  ..., -0.5430, -0.7383, -0.5742],\n","         [-0.9961, -0.9805, -0.9570,  ..., -0.5117, -0.7383, -0.6133]],\n","\n","        [[-0.9961, -0.9961, -0.9961,  ..., -0.9258, -0.9414, -0.9414],\n","         [-0.9961, -0.9961, -0.9961,  ..., -0.9023, -0.9102, -0.9102],\n","         [-0.9961, -0.9961, -0.9961,  ..., -0.8867, -0.8945, -0.8945],\n","         ...,\n","         [-0.9805, -0.9961, -0.9883,  ..., -0.5820, -0.7305, -0.5195],\n","         [-0.9727, -0.9883, -0.9805,  ..., -0.5586, -0.7539, -0.5820],\n","         [-0.9648, -0.9805, -0.9648,  ..., -0.5273, -0.7539, -0.6211]],\n","\n","        [[-0.9961, -0.9883, -0.9883,  ..., -0.9805, -0.9805, -0.9883],\n","         [-0.9961, -0.9883, -0.9883,  ..., -0.9727, -0.9727, -0.9805],\n","         [-0.9961, -0.9961, -0.9961,  ..., -0.9648, -0.9570, -0.9727],\n","         ...,\n","         [-0.9883, -0.9961, -0.9961,  ..., -0.7148, -0.8711, -0.6523],\n","         [-0.9961, -0.9883, -0.9883,  ..., -0.6758, -0.8711, -0.7070],\n","         [-0.9961, -0.9883, -0.9805,  ..., -0.6289, -0.8555, -0.7383]]]), tensor([[[-0.9492, -0.9492, -0.9492,  ..., -0.9805, -0.9805, -0.9727],\n","         [-0.9492, -0.9492, -0.9492,  ..., -0.9805, -0.9805, -0.9727],\n","         [-0.9492, -0.9492, -0.9492,  ..., -0.9805, -0.9805, -0.9727],\n","         ...,\n","         [-0.2070, -0.2227, -0.2383,  ..., -0.8867, -0.8477, -0.8242],\n","         [-0.2227, -0.2305, -0.2539,  ..., -0.8633, -0.8555, -0.8398],\n","         [-0.2383, -0.2383, -0.2617,  ..., -0.8633, -0.8555, -0.8555]],\n","\n","        [[-0.9414, -0.9414, -0.9414,  ..., -0.9805, -0.9805, -0.9727],\n","         [-0.9414, -0.9414, -0.9414,  ..., -0.9805, -0.9805, -0.9727],\n","         [-0.9414, -0.9414, -0.9414,  ..., -0.9805, -0.9805, -0.9727],\n","         ...,\n","         [-0.6445, -0.6445, -0.6523,  ..., -0.9258, -0.8945, -0.8789],\n","         [-0.6445, -0.6445, -0.6602,  ..., -0.9102, -0.9023, -0.8945],\n","         [-0.6523, -0.6445, -0.6602,  ..., -0.9023, -0.9023, -0.9102]],\n","\n","        [[-0.9258, -0.9258, -0.9258,  ..., -0.9805, -0.9805, -0.9727],\n","         [-0.9258, -0.9258, -0.9258,  ..., -0.9805, -0.9805, -0.9727],\n","         [-0.9258, -0.9258, -0.9258,  ..., -0.9805, -0.9805, -0.9727],\n","         ...,\n","         [-0.5742, -0.5742, -0.5898,  ..., -0.8789, -0.8477, -0.8242],\n","         [-0.5742, -0.5820, -0.6055,  ..., -0.8789, -0.8711, -0.8633],\n","         [-0.5898, -0.5898, -0.6055,  ..., -0.8789, -0.8867, -0.8945]]]), tensor([[[-0.8398, -0.8945, -0.9258,  ..., -0.8477, -0.8555, -0.8242],\n","         [-0.8555, -0.8867, -0.9102,  ..., -0.7773, -0.8164, -0.8242],\n","         [-0.8555, -0.8477, -0.8555,  ..., -0.7227, -0.7773, -0.8164],\n","         ...,\n","         [-0.0977, -0.0508,  0.0117,  ..., -0.8164, -0.8164, -0.8164],\n","         [-0.1836, -0.1602, -0.1133,  ..., -0.8164, -0.8164, -0.8164],\n","         [-0.2305, -0.2070, -0.1758,  ..., -0.8164, -0.8164, -0.8164]],\n","\n","        [[-0.8789, -0.9336, -0.9648,  ..., -0.9102, -0.9180, -0.8867],\n","         [-0.8867, -0.9180, -0.9414,  ..., -0.8477, -0.8789, -0.8867],\n","         [-0.8867, -0.8789, -0.8867,  ..., -0.7852, -0.8398, -0.8711],\n","         ...,\n","         [-0.1211, -0.1367, -0.1367,  ..., -0.6602, -0.6602, -0.6602],\n","         [-0.0977, -0.1367, -0.1602,  ..., -0.6602, -0.6602, -0.6602],\n","         [-0.0820, -0.1133, -0.1445,  ..., -0.6602, -0.6602, -0.6602]],\n","\n","        [[-0.9180, -0.9570, -0.9805,  ..., -0.9336, -0.9414, -0.9102],\n","         [-0.9336, -0.9570, -0.9727,  ..., -0.8711, -0.9023, -0.9102],\n","         [-0.9336, -0.9336, -0.9336,  ..., -0.8086, -0.8633, -0.8945],\n","         ...,\n","         [ 0.3164,  0.1992,  0.1055,  ..., -0.6055, -0.6055, -0.6055],\n","         [ 0.5117,  0.3867,  0.2617,  ..., -0.6055, -0.6055, -0.6055],\n","         [ 0.6289,  0.5195,  0.4102,  ..., -0.6055, -0.6055, -0.6055]]]), tensor([[[-0.7539, -0.7383, -0.6992,  ..., -0.6758, -0.6523, -0.6602],\n","         [-0.7070, -0.6758, -0.6523,  ..., -0.6992, -0.6602, -0.6523],\n","         [-0.6758, -0.6602, -0.6523,  ..., -0.7070, -0.6602, -0.6523],\n","         ...,\n","         [-0.1367, -0.0898, -0.0273,  ..., -0.3242, -0.4570, -0.5352],\n","         [-0.1523, -0.1289, -0.0898,  ..., -0.5039, -0.5508, -0.5664],\n","         [-0.1836, -0.1836, -0.1523,  ..., -0.5508, -0.5742, -0.5977]],\n","\n","        [[-0.6836, -0.6602, -0.6211,  ..., -0.5664, -0.5508, -0.5664],\n","         [-0.6445, -0.6133, -0.5898,  ..., -0.5820, -0.5508, -0.5508],\n","         [-0.6211, -0.6055, -0.5977,  ..., -0.5820, -0.5430, -0.5352],\n","         ...,\n","         [-0.3711, -0.3320, -0.2773,  ..., -0.4492, -0.5430, -0.5820],\n","         [-0.3789, -0.3633, -0.3320,  ..., -0.5898, -0.5977, -0.5742],\n","         [-0.4023, -0.4102, -0.3867,  ..., -0.5977, -0.5742, -0.5820]],\n","\n","        [[-0.7539, -0.7305, -0.6914,  ..., -0.6680, -0.6602, -0.6836],\n","         [-0.7070, -0.6758, -0.6602,  ..., -0.6836, -0.6680, -0.6758],\n","         [-0.6836, -0.6680, -0.6602,  ..., -0.6836, -0.6602, -0.6680],\n","         ...,\n","         [-0.5977, -0.5508, -0.5039,  ..., -0.6367, -0.7070, -0.7383],\n","         [-0.5977, -0.5820, -0.5586,  ..., -0.7617, -0.7461, -0.7227],\n","         [-0.6211, -0.6289, -0.6133,  ..., -0.7617, -0.7227, -0.7070]]]), tensor([[[-0.6211, -0.6211, -0.6289,  ...,  0.0898,  0.0820,  0.0820],\n","         [-0.6133, -0.6133, -0.6289,  ...,  0.1055,  0.1133,  0.1211],\n","         [-0.6055, -0.6133, -0.6289,  ...,  0.1289,  0.1523,  0.1602],\n","         ...,\n","         [ 0.2227,  0.2227,  0.2148,  ...,  0.9727,  0.9727,  0.9727],\n","         [ 0.2148,  0.2148,  0.2070,  ...,  0.9727,  0.9727,  0.9727],\n","         [ 0.2227,  0.2227,  0.2148,  ...,  0.9727,  0.9727,  0.9727]],\n","\n","        [[-0.7461, -0.7461, -0.7539,  ..., -0.0664, -0.0586, -0.0586],\n","         [-0.7383, -0.7461, -0.7617,  ..., -0.0508, -0.0352, -0.0273],\n","         [-0.7305, -0.7383, -0.7539,  ..., -0.0430, -0.0117, -0.0039],\n","         ...,\n","         [ 0.1523,  0.1523,  0.1445,  ...,  0.8398,  0.8789,  0.8945],\n","         [ 0.1445,  0.1445,  0.1367,  ...,  0.8164,  0.8633,  0.8867],\n","         [ 0.1523,  0.1523,  0.1445,  ...,  0.8008,  0.8477,  0.8711]],\n","\n","        [[-0.7695, -0.7695, -0.7695,  ..., -0.0742, -0.0430, -0.0352],\n","         [-0.7617, -0.7617, -0.7773,  ..., -0.0586, -0.0273, -0.0117],\n","         [-0.7539, -0.7617, -0.7773,  ..., -0.0508, -0.0195, -0.0039],\n","         ...,\n","         [ 0.1055,  0.0977,  0.0898,  ...,  0.6914,  0.7461,  0.7695],\n","         [ 0.0977,  0.0977,  0.0898,  ...,  0.6523,  0.7148,  0.7461],\n","         [ 0.0977,  0.0977,  0.0898,  ...,  0.6289,  0.6914,  0.7227]]]), tensor([[[ 0.8711,  0.8633,  0.8398,  ...,  0.6680,  0.6602,  0.6602],\n","         [ 0.8242,  0.8164,  0.7930,  ...,  0.6680,  0.6602,  0.6602],\n","         [ 0.7539,  0.7461,  0.7148,  ...,  0.6758,  0.6680,  0.6680],\n","         ...,\n","         [ 0.8711,  0.8789,  0.9023,  ..., -0.5352, -0.6523, -0.6914],\n","         [ 0.8633,  0.8711,  0.8945,  ..., -0.5586, -0.6680, -0.6992],\n","         [ 0.8633,  0.8711,  0.8945,  ..., -0.5664, -0.6758, -0.7070]],\n","\n","        [[ 0.7773,  0.7695,  0.7383,  ...,  0.5508,  0.5430,  0.5430],\n","         [ 0.7227,  0.7148,  0.6836,  ...,  0.5508,  0.5430,  0.5430],\n","         [ 0.6367,  0.6289,  0.5898,  ...,  0.5508,  0.5508,  0.5508],\n","         ...,\n","         [ 0.4883,  0.4961,  0.5039,  ..., -0.5352, -0.6523, -0.6836],\n","         [ 0.4805,  0.4883,  0.5117,  ..., -0.5586, -0.6602, -0.6914],\n","         [ 0.4805,  0.4883,  0.5117,  ..., -0.5742, -0.6680, -0.6992]],\n","\n","        [[ 0.7852,  0.7773,  0.7539,  ...,  0.5273,  0.5273,  0.5273],\n","         [ 0.7305,  0.7227,  0.6914,  ...,  0.5273,  0.5273,  0.5273],\n","         [ 0.6367,  0.6289,  0.5898,  ...,  0.5352,  0.5273,  0.5273],\n","         ...,\n","         [ 0.6992,  0.7070,  0.7305,  ..., -0.4336, -0.5586, -0.5977],\n","         [ 0.6914,  0.6992,  0.7305,  ..., -0.4648, -0.5820, -0.6211],\n","         [ 0.6914,  0.6992,  0.7305,  ..., -0.4805, -0.5977, -0.6289]]]), tensor([[[ 0.0742,  0.0977,  0.1133,  ...,  0.1602,  0.1523,  0.1445],\n","         [ 0.0664,  0.0977,  0.1211,  ...,  0.0898,  0.1133,  0.1211],\n","         [ 0.0977,  0.1211,  0.1289,  ...,  0.0273,  0.0664,  0.0898],\n","         ...,\n","         [-0.0352, -0.0273, -0.0352,  ..., -0.7852, -0.8008, -0.8086],\n","         [-0.0352, -0.0273, -0.0273,  ..., -0.7852, -0.8008, -0.8086],\n","         [-0.0273, -0.0195, -0.0117,  ..., -0.7773, -0.7930, -0.8008]],\n","\n","        [[-0.0977, -0.0977, -0.0898,  ..., -0.1055, -0.0898, -0.0898],\n","         [-0.1133, -0.0977, -0.0820,  ..., -0.1602, -0.1367, -0.1211],\n","         [-0.0898, -0.0820, -0.0977,  ..., -0.2383, -0.1992, -0.1602],\n","         ...,\n","         [-0.9336, -0.9414, -0.9492,  ..., -0.8086, -0.8242, -0.8320],\n","         [-0.9336, -0.9414, -0.9492,  ..., -0.8086, -0.8242, -0.8320],\n","         [-0.9414, -0.9492, -0.9570,  ..., -0.8086, -0.8242, -0.8320]],\n","\n","        [[-0.2773, -0.2773, -0.2852,  ..., -0.3242, -0.3086, -0.3008],\n","         [-0.2930, -0.2852, -0.2930,  ..., -0.3789, -0.3555, -0.3398],\n","         [-0.2773, -0.2773, -0.3008,  ..., -0.4570, -0.4180, -0.3789],\n","         ...,\n","         [-0.7227, -0.7305, -0.7461,  ..., -0.7148, -0.7305, -0.7461],\n","         [-0.7227, -0.7305, -0.7461,  ..., -0.7070, -0.7227, -0.7383],\n","         [-0.7227, -0.7305, -0.7461,  ..., -0.6992, -0.7148, -0.7227]]]), tensor([[[-0.2383, -0.1992, -0.3320,  ...,  0.4492,  0.3398,  0.1914],\n","         [-0.2539, -0.2617, -0.3711,  ...,  0.4805,  0.3633,  0.2148],\n","         [-0.2852, -0.3789, -0.4805,  ...,  0.4727,  0.3477,  0.2461],\n","         ...,\n","         [-0.9336, -0.9336, -0.9336,  ..., -0.0430, -0.5742, -0.9023],\n","         [-0.9336, -0.9336, -0.9336,  ...,  0.1445, -0.4336, -0.8867],\n","         [-0.9336, -0.9336, -0.9336,  ...,  0.3242, -0.3008, -0.8711]],\n","\n","        [[-0.3555, -0.3477, -0.4727,  ...,  0.1992,  0.0898, -0.0664],\n","         [-0.3867, -0.4102, -0.5117,  ...,  0.2227,  0.1055, -0.0508],\n","         [-0.4180, -0.5195, -0.6055,  ...,  0.2070,  0.0820, -0.0273],\n","         ...,\n","         [-0.9180, -0.9180, -0.9180,  ..., -0.1523, -0.6289, -0.9180],\n","         [-0.9180, -0.9180, -0.9180,  ...,  0.0352, -0.4805, -0.9023],\n","         [-0.9180, -0.9180, -0.9180,  ...,  0.2227, -0.3320, -0.8711]],\n","\n","        [[-0.4805, -0.4648, -0.5820,  ..., -0.0195, -0.1523, -0.3242],\n","         [-0.5117, -0.5273, -0.6289,  ..., -0.0039, -0.1523, -0.3164],\n","         [-0.5352, -0.6367, -0.7227,  ..., -0.0195, -0.1836, -0.3008],\n","         ...,\n","         [-0.9492, -0.9492, -0.9492,  ..., -0.1992, -0.6211, -0.8945],\n","         [-0.9414, -0.9414, -0.9414,  ..., -0.0117, -0.4805, -0.8789],\n","         [-0.9414, -0.9414, -0.9414,  ...,  0.1758, -0.3320, -0.8477]]]), tensor([[[-0.1367, -0.0586,  0.0273,  ...,  0.4102,  0.3633,  0.3320],\n","         [-0.0820, -0.0195,  0.0430,  ...,  0.4023,  0.3789,  0.3633],\n","         [-0.0273,  0.0195,  0.0664,  ...,  0.4023,  0.3945,  0.3945],\n","         ...,\n","         [-0.8086, -0.8320, -0.8555,  ..., -0.9336, -0.9492, -0.9648],\n","         [-0.7930, -0.8164, -0.8477,  ..., -0.9180, -0.9414, -0.9648],\n","         [-0.8008, -0.8164, -0.8320,  ..., -0.9336, -0.9492, -0.9570]],\n","\n","        [[-0.3633, -0.2773, -0.1758,  ...,  0.0742,  0.0430,  0.0352],\n","         [-0.3086, -0.2383, -0.1680,  ...,  0.1133,  0.0820,  0.0742],\n","         [-0.2617, -0.2070, -0.1602,  ...,  0.1523,  0.1211,  0.1055],\n","         ...,\n","         [-0.9570, -0.9570, -0.9648,  ..., -0.9648, -0.9805, -0.9883],\n","         [-0.9414, -0.9492, -0.9570,  ..., -0.9414, -0.9648, -0.9883],\n","         [-0.9492, -0.9492, -0.9492,  ..., -0.9648, -0.9805, -0.9883]],\n","\n","        [[-0.4570, -0.3867, -0.3008,  ..., -0.0117, -0.0586, -0.0820],\n","         [-0.3945, -0.3477, -0.2930,  ...,  0.0195, -0.0195, -0.0430],\n","         [-0.3477, -0.3086, -0.2773,  ...,  0.0508,  0.0117, -0.0117],\n","         ...,\n","         [-0.9414, -0.9492, -0.9648,  ..., -0.9570, -0.9727, -0.9883],\n","         [-0.9258, -0.9336, -0.9492,  ..., -0.9336, -0.9648, -0.9883],\n","         [-0.9336, -0.9414, -0.9492,  ..., -0.9570, -0.9727, -0.9883]]]), tensor([[[-0.7930, -0.7930, -0.7852,  ...,  0.2695,  0.3477,  0.4258],\n","         [-0.7852, -0.7852, -0.7773,  ...,  0.1289,  0.1992,  0.3398],\n","         [-0.7695, -0.7695, -0.7695,  ...,  0.0820,  0.0977,  0.2227],\n","         ...,\n","         [-0.2070, -0.1914, -0.1602,  ..., -0.8242, -0.8398, -0.8555],\n","         [-0.1914, -0.1680, -0.1445,  ..., -0.8164, -0.8164, -0.8242],\n","         [-0.1758, -0.1445, -0.1445,  ..., -0.8398, -0.8477, -0.8320]],\n","\n","        [[-0.7695, -0.7695, -0.7617,  ...,  0.2383,  0.3242,  0.3945],\n","         [-0.7617, -0.7617, -0.7539,  ...,  0.0820,  0.1602,  0.2930],\n","         [-0.7461, -0.7461, -0.7461,  ...,  0.0195,  0.0430,  0.1758],\n","         ...,\n","         [-0.3711, -0.3633, -0.3398,  ..., -0.7852, -0.8008, -0.8164],\n","         [-0.3555, -0.3320, -0.3164,  ..., -0.7539, -0.7539, -0.7617],\n","         [-0.3398, -0.3164, -0.3242,  ..., -0.7539, -0.7539, -0.7539]],\n","\n","        [[-0.7305, -0.7305, -0.7227,  ...,  0.1055,  0.1914,  0.2773],\n","         [-0.7227, -0.7227, -0.7148,  ..., -0.0664,  0.0117,  0.1523],\n","         [-0.7070, -0.7070, -0.7070,  ..., -0.1367, -0.1055,  0.0352],\n","         ...,\n","         [-0.4805, -0.4648, -0.4336,  ..., -0.7695, -0.7773, -0.7852],\n","         [-0.4648, -0.4414, -0.4180,  ..., -0.6992, -0.6914, -0.6914],\n","         [-0.4414, -0.4180, -0.4258,  ..., -0.6914, -0.6836, -0.6836]]]), tensor([[[ 0.8320,  0.7852,  0.7305,  ..., -0.0352, -0.1211, -0.0820],\n","         [ 0.8008,  0.6914,  0.5898,  ..., -0.0352, -0.1055, -0.0508],\n","         [ 0.5820,  0.3945,  0.2852,  ...,  0.0352, -0.0117,  0.0195],\n","         ...,\n","         [-0.8242, -0.8086, -0.8008,  ...,  0.6289,  0.6211,  0.6055],\n","         [-0.8320, -0.8164, -0.8086,  ...,  0.6523,  0.6211,  0.5977],\n","         [-0.8320, -0.8242, -0.8242,  ...,  0.6602,  0.6445,  0.6445]],\n","\n","        [[ 0.6992,  0.5977,  0.4883,  ..., -0.5039, -0.5820, -0.5430],\n","         [ 0.6055,  0.4492,  0.2852,  ..., -0.5039, -0.5664, -0.5117],\n","         [ 0.2695,  0.0664, -0.0898,  ..., -0.4336, -0.4805, -0.4336],\n","         ...,\n","         [-0.8711, -0.8633, -0.8555,  ...,  0.5195,  0.5430,  0.5664],\n","         [-0.8711, -0.8633, -0.8633,  ...,  0.5742,  0.5664,  0.5430],\n","         [-0.8711, -0.8633, -0.8633,  ...,  0.5742,  0.5742,  0.5664]],\n","\n","        [[ 0.5273,  0.4023,  0.2539,  ..., -0.7852, -0.8711, -0.8320],\n","         [ 0.3945,  0.2070,  0.0195,  ..., -0.7852, -0.8555, -0.8008],\n","         [ 0.0273, -0.2148, -0.3789,  ..., -0.7148, -0.7617, -0.7227],\n","         ...,\n","         [-0.9023, -0.8945, -0.8867,  ...,  0.4336,  0.4805,  0.5117],\n","         [-0.9023, -0.8945, -0.8867,  ...,  0.5195,  0.5117,  0.5039],\n","         [-0.9023, -0.8945, -0.8945,  ...,  0.5430,  0.5273,  0.5117]]]), tensor([[[ 0.0977,  0.1211,  0.1602,  ..., -0.0117, -0.0195, -0.0273],\n","         [ 0.0898,  0.1133,  0.1445,  ..., -0.0508, -0.0195, -0.0039],\n","         [ 0.0898,  0.1133,  0.1445,  ..., -0.0664, -0.0195,  0.0039],\n","         ...,\n","         [ 0.3633,  0.3633,  0.3555,  ..., -0.8320, -0.8398, -0.8398],\n","         [ 0.3398,  0.3398,  0.3320,  ..., -0.8086, -0.8164, -0.8164],\n","         [ 0.3320,  0.3320,  0.3242,  ..., -0.7930, -0.8008, -0.8008]],\n","\n","        [[-0.3398, -0.3164, -0.2852,  ..., -0.4883, -0.4961, -0.5039],\n","         [-0.3555, -0.3320, -0.3008,  ..., -0.5273, -0.4961, -0.4805],\n","         [-0.3633, -0.3398, -0.3008,  ..., -0.5430, -0.4961, -0.4727],\n","         ...,\n","         [-0.7148, -0.7148, -0.7070,  ..., -0.8242, -0.8320, -0.8320],\n","         [-0.7070, -0.7070, -0.6992,  ..., -0.8008, -0.8086, -0.8086],\n","         [-0.6992, -0.6992, -0.6914,  ..., -0.7773, -0.7852, -0.7852]],\n","\n","        [[-0.9258, -0.9023, -0.8711,  ..., -0.8398, -0.8477, -0.8477],\n","         [-0.9414, -0.9180, -0.8867,  ..., -0.8633, -0.8398, -0.8242],\n","         [-0.9414, -0.9180, -0.8867,  ..., -0.8711, -0.8398, -0.8164],\n","         ...,\n","         [-0.8633, -0.8555, -0.8477,  ..., -0.8633, -0.8711, -0.8789],\n","         [-0.8555, -0.8555, -0.8477,  ..., -0.8398, -0.8477, -0.8477],\n","         [-0.8477, -0.8477, -0.8398,  ..., -0.8242, -0.8242, -0.8242]]]), tensor([[[-0.3320, -0.3555, -0.3086,  ..., -0.2852, -0.2930, -0.2852],\n","         [-0.3164, -0.3086, -0.2617,  ..., -0.2539, -0.2695, -0.2617],\n","         [-0.3008, -0.2695, -0.2383,  ..., -0.1836, -0.2070, -0.2305],\n","         ...,\n","         [-0.7461, -0.7227, -0.6758,  ..., -0.8086, -0.7695, -0.7227],\n","         [-0.6680, -0.6445, -0.6211,  ..., -0.7695, -0.7695, -0.7852],\n","         [-0.6758, -0.6602, -0.6445,  ..., -0.7539, -0.7852, -0.8398]],\n","\n","        [[-0.5742, -0.6055, -0.5586,  ..., -0.5742, -0.5820, -0.5742],\n","         [-0.5664, -0.5586, -0.5195,  ..., -0.5430, -0.5586, -0.5508],\n","         [-0.5586, -0.5273, -0.4961,  ..., -0.4727, -0.4961, -0.5195],\n","         ...,\n","         [-0.7461, -0.7227, -0.6758,  ..., -0.8086, -0.7695, -0.7227],\n","         [-0.6680, -0.6445, -0.6211,  ..., -0.7695, -0.7695, -0.7852],\n","         [-0.6758, -0.6602, -0.6445,  ..., -0.7539, -0.7852, -0.8398]],\n","\n","        [[-0.6914, -0.7227, -0.6758,  ..., -0.7773, -0.7852, -0.7773],\n","         [-0.6836, -0.6758, -0.6367,  ..., -0.7461, -0.7617, -0.7539],\n","         [-0.6758, -0.6367, -0.5977,  ..., -0.6758, -0.6992, -0.7227],\n","         ...,\n","         [-0.7461, -0.7148, -0.6758,  ..., -0.8086, -0.7695, -0.7227],\n","         [-0.6602, -0.6367, -0.6133,  ..., -0.7695, -0.7695, -0.7852],\n","         [-0.6602, -0.6445, -0.6289,  ..., -0.7539, -0.7852, -0.8398]]]), tensor([[[-0.6914, -0.6914, -0.6992,  ..., -0.1680, -0.0820, -0.0430],\n","         [-0.6914, -0.6914, -0.6914,  ..., -0.2617, -0.1758, -0.1289],\n","         [-0.6914, -0.6836, -0.6836,  ..., -0.3008, -0.1914, -0.1445],\n","         ...,\n","         [ 0.2930,  0.2773,  0.2695,  ...,  0.1680,  0.1367,  0.1211],\n","         [ 0.3086,  0.3008,  0.2852,  ...,  0.1992,  0.1680,  0.1367],\n","         [ 0.3398,  0.3477,  0.3164,  ...,  0.2305,  0.2148,  0.1680]],\n","\n","        [[-0.9102, -0.9258, -0.9336,  ..., -0.4414, -0.3555, -0.3164],\n","         [-0.9102, -0.9180, -0.9180,  ..., -0.5352, -0.4492, -0.4023],\n","         [-0.9102, -0.9023, -0.9023,  ..., -0.5664, -0.4648, -0.4258],\n","         ...,\n","         [-0.1445, -0.1680, -0.1758,  ..., -0.2617, -0.2930, -0.3008],\n","         [-0.1367, -0.1445, -0.1523,  ..., -0.2148, -0.2461, -0.2773],\n","         [-0.1055, -0.1055, -0.1211,  ..., -0.1836, -0.1992, -0.2461]],\n","\n","        [[-0.9805, -0.9883, -0.9961,  ..., -0.6758, -0.6055, -0.5820],\n","         [-0.9805, -0.9805, -0.9805,  ..., -0.7617, -0.6758, -0.6523],\n","         [-0.9805, -0.9727, -0.9727,  ..., -0.7773, -0.6836, -0.6602],\n","         ...,\n","         [-0.2461, -0.2617, -0.2695,  ..., -0.3789, -0.4180, -0.4258],\n","         [-0.2383, -0.2461, -0.2539,  ..., -0.3398, -0.3711, -0.4023],\n","         [-0.2070, -0.1992, -0.2227,  ..., -0.3086, -0.3242, -0.3711]]]), tensor([[[-0.2930, -0.3086, -0.3633,  ..., -0.1523, -0.1680, -0.1680],\n","         [-0.3086, -0.3164, -0.3711,  ..., -0.1445, -0.1602, -0.1602],\n","         [-0.3320, -0.3477, -0.3867,  ..., -0.1211, -0.1367, -0.1445],\n","         ...,\n","         [ 0.0664,  0.0586,  0.0430,  ..., -0.9961, -0.9961, -0.9961],\n","         [ 0.0586,  0.0508,  0.0352,  ..., -0.9961, -0.9961, -0.9961],\n","         [ 0.0586,  0.0508,  0.0352,  ..., -0.9961, -0.9961, -0.9961]],\n","\n","        [[-0.2930, -0.3086, -0.3633,  ..., -0.1523, -0.1680, -0.1680],\n","         [-0.3086, -0.3164, -0.3711,  ..., -0.1445, -0.1602, -0.1602],\n","         [-0.3320, -0.3477, -0.3867,  ..., -0.1211, -0.1367, -0.1445],\n","         ...,\n","         [ 0.0664,  0.0586,  0.0430,  ..., -0.9961, -0.9961, -0.9961],\n","         [ 0.0586,  0.0508,  0.0352,  ..., -0.9961, -0.9961, -0.9961],\n","         [ 0.0586,  0.0508,  0.0352,  ..., -0.9961, -0.9961, -0.9961]],\n","\n","        [[-0.2930, -0.3086, -0.3633,  ..., -0.1523, -0.1680, -0.1680],\n","         [-0.3086, -0.3164, -0.3711,  ..., -0.1445, -0.1602, -0.1602],\n","         [-0.3320, -0.3477, -0.3867,  ..., -0.1211, -0.1367, -0.1445],\n","         ...,\n","         [ 0.0664,  0.0586,  0.0430,  ..., -0.9961, -0.9961, -0.9961],\n","         [ 0.0586,  0.0508,  0.0352,  ..., -0.9961, -0.9961, -0.9961],\n","         [ 0.0586,  0.0508,  0.0352,  ..., -0.9961, -0.9961, -0.9961]]]), tensor([[[-0.8633, -0.8633, -0.8633,  ..., -0.4102, -0.3242, -0.2695],\n","         [-0.8711, -0.8711, -0.8711,  ..., -0.3086, -0.2461, -0.2070],\n","         [-0.8711, -0.8711, -0.8711,  ..., -0.1992, -0.1602, -0.1445],\n","         ...,\n","         [-0.6523, -0.5430, -0.3789,  ..., -0.2227, -0.4336, -0.5898],\n","         [-0.7539, -0.6602, -0.5195,  ..., -0.4883, -0.6523, -0.7695],\n","         [-0.8164, -0.7383, -0.6133,  ..., -0.6914, -0.8086, -0.8945]],\n","\n","        [[-0.8945, -0.8945, -0.8945,  ..., -0.4258, -0.3398, -0.2852],\n","         [-0.9023, -0.9023, -0.9023,  ..., -0.3242, -0.2617, -0.2227],\n","         [-0.9023, -0.9023, -0.9023,  ..., -0.2070, -0.1758, -0.1602],\n","         ...,\n","         [-0.6367, -0.5273, -0.3633,  ..., -0.2227, -0.4336, -0.5898],\n","         [-0.7383, -0.6445, -0.5039,  ..., -0.4883, -0.6523, -0.7695],\n","         [-0.8008, -0.7227, -0.5977,  ..., -0.6914, -0.8086, -0.8945]],\n","\n","        [[-0.8867, -0.8867, -0.8867,  ..., -0.5195, -0.4414, -0.3867],\n","         [-0.8945, -0.8945, -0.8945,  ..., -0.4258, -0.3633, -0.3242],\n","         [-0.8945, -0.8945, -0.8945,  ..., -0.3242, -0.2930, -0.2695],\n","         ...,\n","         [-0.6758, -0.5742, -0.4102,  ..., -0.2070, -0.4180, -0.5742],\n","         [-0.7773, -0.6836, -0.5508,  ..., -0.4727, -0.6367, -0.7539],\n","         [-0.8398, -0.7617, -0.6445,  ..., -0.6758, -0.7930, -0.8789]]]), tensor([[[-0.9492, -0.9570, -0.9727,  ..., -0.3789, -0.3477, -0.2305],\n","         [-0.9492, -0.9570, -0.9805,  ..., -0.3867, -0.3555, -0.2539],\n","         [-0.9570, -0.9648, -0.9805,  ..., -0.3789, -0.3633, -0.2773],\n","         ...,\n","         [-0.6602, -0.6445, -0.6523,  ..., -0.7383, -0.6680, -0.6211],\n","         [-0.6523, -0.6289, -0.6211,  ..., -0.7695, -0.6914, -0.6445],\n","         [-0.6445, -0.6211, -0.6133,  ..., -0.7773, -0.7070, -0.6602]],\n","\n","        [[-0.9492, -0.9570, -0.9727,  ..., -0.5352, -0.5039, -0.3945],\n","         [-0.9492, -0.9570, -0.9805,  ..., -0.5430, -0.5117, -0.4180],\n","         [-0.9570, -0.9648, -0.9727,  ..., -0.5430, -0.5273, -0.4414],\n","         ...,\n","         [-0.6445, -0.6289, -0.6367,  ..., -0.7305, -0.6602, -0.6133],\n","         [-0.6367, -0.6133, -0.6055,  ..., -0.7617, -0.6836, -0.6289],\n","         [-0.6289, -0.6055, -0.5977,  ..., -0.7695, -0.6992, -0.6445]],\n","\n","        [[-0.9648, -0.9648, -0.9727,  ..., -0.7461, -0.7148, -0.6055],\n","         [-0.9570, -0.9570, -0.9648,  ..., -0.7539, -0.7227, -0.6367],\n","         [-0.9648, -0.9648, -0.9727,  ..., -0.7539, -0.7383, -0.6680],\n","         ...,\n","         [-0.7305, -0.7148, -0.7227,  ..., -0.7930, -0.7383, -0.7070],\n","         [-0.7227, -0.6992, -0.6914,  ..., -0.8242, -0.7617, -0.7227],\n","         [-0.7148, -0.6914, -0.6836,  ..., -0.8320, -0.7773, -0.7383]]]), tensor([[[-0.1914, -0.1914, -0.1914,  ...,  0.9102,  0.9023,  0.9023],\n","         [-0.2383, -0.2070, -0.1758,  ...,  0.9258,  0.9180,  0.9180],\n","         [-0.2930, -0.2148, -0.1602,  ...,  0.9414,  0.9336,  0.9258],\n","         ...,\n","         [-0.9961, -0.9961, -0.9961,  ..., -0.6133, -0.6289, -0.6289],\n","         [-0.9961, -0.9961, -0.9961,  ..., -0.6133, -0.6133, -0.5977],\n","         [-0.9961, -0.9961, -0.9961,  ..., -0.6133, -0.5977, -0.5664]],\n","\n","        [[ 0.1289,  0.1289,  0.1211,  ...,  0.9961,  0.9961,  0.9961],\n","         [ 0.0742,  0.0977,  0.1211,  ...,  0.9961,  0.9961,  0.9961],\n","         [ 0.0117,  0.0742,  0.1289,  ...,  0.9961,  0.9961,  0.9961],\n","         ...,\n","         [-0.9648, -0.9180, -0.8789,  ..., -0.4336, -0.4492, -0.4492],\n","         [-0.9570, -0.8945, -0.8555,  ..., -0.4180, -0.4180, -0.4102],\n","         [-0.9414, -0.8789, -0.8242,  ..., -0.4023, -0.3867, -0.3711]],\n","\n","        [[ 0.0664,  0.0664,  0.0586,  ...,  0.9961,  0.9961,  0.9961],\n","         [ 0.0117,  0.0430,  0.0664,  ...,  0.9961,  0.9961,  0.9961],\n","         [-0.0430,  0.0273,  0.0742,  ...,  0.9961,  0.9961,  0.9961],\n","         ...,\n","         [-0.8633, -0.8320, -0.7930,  ..., -0.3711, -0.3867, -0.3867],\n","         [-0.8633, -0.8086, -0.7695,  ..., -0.3711, -0.3633, -0.3477],\n","         [-0.8555, -0.7930, -0.7383,  ..., -0.3633, -0.3477, -0.3164]]]), tensor([[[ 0.5977,  0.6055,  0.5977,  ...,  0.9961,  0.9883,  0.9648],\n","         [ 0.5352,  0.5508,  0.5586,  ...,  0.9961,  0.9805,  0.9414],\n","         [ 0.4883,  0.5039,  0.5273,  ...,  0.9961,  0.9805,  0.9336],\n","         ...,\n","         [ 0.0117, -0.0352, -0.1211,  ..., -0.6914, -0.6758, -0.6523],\n","         [ 0.0430,  0.0117, -0.0664,  ..., -0.6992, -0.6836, -0.6602],\n","         [ 0.0430,  0.0430, -0.0039,  ..., -0.7070, -0.6914, -0.6680]],\n","\n","        [[-0.2148, -0.1992, -0.2070,  ...,  0.4570,  0.4336,  0.3789],\n","         [-0.2617, -0.2461, -0.2383,  ...,  0.4570,  0.4336,  0.3633],\n","         [-0.3008, -0.2852, -0.2539,  ...,  0.4570,  0.4336,  0.3633],\n","         ...,\n","         [-0.4727, -0.5273, -0.6133,  ..., -0.7305, -0.7148, -0.6914],\n","         [-0.4570, -0.5117, -0.5977,  ..., -0.7383, -0.7227, -0.6992],\n","         [-0.4570, -0.4961, -0.5664,  ..., -0.7461, -0.7305, -0.7070]],\n","\n","        [[-0.6836, -0.6680, -0.6758,  ..., -0.0508, -0.0586, -0.1133],\n","         [-0.7305, -0.7148, -0.7070,  ..., -0.0352, -0.0586, -0.1211],\n","         [-0.7695, -0.7461, -0.7148,  ..., -0.0273, -0.0508, -0.1211],\n","         ...,\n","         [-0.5664, -0.6133, -0.6836,  ..., -0.6758, -0.6602, -0.6367],\n","         [-0.5352, -0.5664, -0.6289,  ..., -0.6836, -0.6680, -0.6445],\n","         [-0.5039, -0.5195, -0.5664,  ..., -0.6914, -0.6758, -0.6523]]]), tensor([[[ 0.6523,  0.6602,  0.6602,  ...,  0.1523,  0.1523,  0.2148],\n","         [ 0.6211,  0.6289,  0.6289,  ...,  0.1680,  0.1680,  0.2305],\n","         [ 0.5820,  0.5898,  0.5898,  ...,  0.2305,  0.2461,  0.2930],\n","         ...,\n","         [-0.8477, -0.8086, -0.7617,  ..., -0.0742, -0.0195,  0.0352],\n","         [-0.7539, -0.6992, -0.6211,  ..., -0.0430,  0.0195,  0.0898],\n","         [-0.5508, -0.4648, -0.3633,  ..., -0.0195,  0.0508,  0.1211]],\n","\n","        [[ 0.1367,  0.1367,  0.1289,  ..., -0.1992, -0.1992, -0.1367],\n","         [ 0.1211,  0.1133,  0.1055,  ..., -0.1836, -0.1836, -0.1211],\n","         [ 0.0977,  0.0820,  0.0664,  ..., -0.1211, -0.1133, -0.0586],\n","         ...,\n","         [-0.9102, -0.8789, -0.8398,  ..., -0.5195, -0.4648, -0.4102],\n","         [-0.8242, -0.7773, -0.6992,  ..., -0.4883, -0.4258, -0.3633],\n","         [-0.6289, -0.5508, -0.4492,  ..., -0.4727, -0.4023, -0.3398]],\n","\n","        [[-0.1758, -0.1836, -0.1836,  ..., -0.5039, -0.5117, -0.4492],\n","         [-0.1914, -0.1992, -0.2070,  ..., -0.4883, -0.4961, -0.4414],\n","         [-0.2070, -0.2227, -0.2383,  ..., -0.4414, -0.4336, -0.3789],\n","         ...,\n","         [-0.9570, -0.9336, -0.8945,  ..., -0.6914, -0.6523, -0.5977],\n","         [-0.8867, -0.8320, -0.7695,  ..., -0.6680, -0.6133, -0.5586],\n","         [-0.6914, -0.6133, -0.5195,  ..., -0.6523, -0.5977, -0.5352]]]), tensor([[[ 0.1133,  0.0898,  0.0508,  ...,  0.2539,  0.2852,  0.3008],\n","         [ 0.0508,  0.0273, -0.0039,  ...,  0.2461,  0.2695,  0.2852],\n","         [-0.0195, -0.0430, -0.0742,  ...,  0.2383,  0.2539,  0.2695],\n","         ...,\n","         [-0.7305, -0.7305, -0.7383,  ..., -0.7539, -0.7461, -0.7461],\n","         [-0.7383, -0.7383, -0.7461,  ..., -0.7539, -0.7461, -0.7461],\n","         [-0.7461, -0.7461, -0.7461,  ..., -0.7539, -0.7461, -0.7461]],\n","\n","        [[ 0.0039, -0.0195, -0.0664,  ...,  0.1758,  0.2227,  0.2539],\n","         [-0.0508, -0.0742, -0.1211,  ...,  0.1602,  0.1992,  0.2227],\n","         [-0.1211, -0.1445, -0.1914,  ...,  0.1445,  0.1680,  0.1914],\n","         ...,\n","         [-0.6680, -0.6602, -0.6523,  ..., -0.5977, -0.6055, -0.6055],\n","         [-0.6680, -0.6602, -0.6523,  ..., -0.5977, -0.6055, -0.6055],\n","         [-0.6758, -0.6680, -0.6602,  ..., -0.5977, -0.6055, -0.6055]],\n","\n","        [[-0.2461, -0.2773, -0.3242,  ..., -0.0742, -0.0195,  0.0117],\n","         [-0.2930, -0.3242, -0.3711,  ..., -0.0898, -0.0430, -0.0195],\n","         [-0.3555, -0.3789, -0.4258,  ..., -0.1055, -0.0742, -0.0508],\n","         ...,\n","         [-0.5273, -0.5352, -0.5430,  ..., -0.5273, -0.5352, -0.5352],\n","         [-0.5273, -0.5352, -0.5430,  ..., -0.5273, -0.5273, -0.5273],\n","         [-0.5273, -0.5273, -0.5352,  ..., -0.5273, -0.5195, -0.5195]]]), tensor([[[ 0.6133,  0.6367,  0.6289,  ..., -0.8086, -0.8086, -0.8086],\n","         [ 0.6289,  0.6680,  0.6680,  ..., -0.8086, -0.8086, -0.8086],\n","         [ 0.6133,  0.6445,  0.6445,  ..., -0.8086, -0.8086, -0.8086],\n","         ...,\n","         [ 0.4180,  0.3945,  0.3711,  ...,  0.4727,  0.2773,  0.1133],\n","         [ 0.4336,  0.4102,  0.3867,  ...,  0.5742,  0.3945,  0.2539],\n","         [ 0.4414,  0.4180,  0.3945,  ...,  0.6680,  0.5273,  0.4023]],\n","\n","        [[ 0.5820,  0.6055,  0.5977,  ..., -0.6836, -0.6914, -0.6992],\n","         [ 0.5977,  0.6367,  0.6367,  ..., -0.6836, -0.6914, -0.6992],\n","         [ 0.5977,  0.6289,  0.6289,  ..., -0.6836, -0.6914, -0.6992],\n","         ...,\n","         [ 0.4492,  0.4414,  0.4414,  ...,  0.3711,  0.1836,  0.0273],\n","         [ 0.4570,  0.4492,  0.4492,  ...,  0.4883,  0.3164,  0.1758],\n","         [ 0.4648,  0.4570,  0.4570,  ...,  0.5898,  0.4492,  0.3242]],\n","\n","        [[ 0.8086,  0.8320,  0.8242,  ..., -0.3555, -0.3633, -0.3711],\n","         [ 0.8242,  0.8711,  0.8633,  ..., -0.3555, -0.3633, -0.3711],\n","         [ 0.8164,  0.8477,  0.8477,  ..., -0.3555, -0.3633, -0.3711],\n","         ...,\n","         [ 0.5898,  0.5898,  0.5977,  ...,  0.4727,  0.3086,  0.1758],\n","         [ 0.5898,  0.5977,  0.6055,  ...,  0.5742,  0.4258,  0.3008],\n","         [ 0.5977,  0.6055,  0.6055,  ...,  0.6602,  0.5352,  0.4336]]]), tensor([[[-0.1758, -0.2305, -0.2539,  ..., -0.5352, -0.5352, -0.5273],\n","         [-0.1914, -0.2227, -0.2305,  ..., -0.5664, -0.5664, -0.5664],\n","         [-0.3320, -0.3320, -0.3242,  ..., -0.6133, -0.6133, -0.6133],\n","         ...,\n","         [ 0.3398,  0.3633,  0.4258,  ..., -0.1602, -0.4727, -0.7539],\n","         [ 0.3555,  0.3633,  0.3945,  ...,  0.0664, -0.2461, -0.5820],\n","         [ 0.3945,  0.4180,  0.4180,  ...,  0.2383, -0.0430, -0.3711]],\n","\n","        [[-0.3242, -0.3945, -0.4180,  ..., -0.6836, -0.6914, -0.7070],\n","         [-0.3477, -0.3867, -0.4102,  ..., -0.7070, -0.7148, -0.7305],\n","         [-0.4883, -0.5039, -0.5039,  ..., -0.7539, -0.7539, -0.7695],\n","         ...,\n","         [ 0.2930,  0.3086,  0.3789,  ..., -0.2070, -0.5117, -0.7852],\n","         [ 0.3242,  0.3320,  0.3633,  ..., -0.0117, -0.3164, -0.6445],\n","         [ 0.3711,  0.3945,  0.3945,  ...,  0.1445, -0.1289, -0.4414]],\n","\n","        [[-0.3320, -0.3945, -0.4180,  ..., -0.7148, -0.7227, -0.7383],\n","         [-0.3555, -0.3945, -0.4102,  ..., -0.7461, -0.7539, -0.7695],\n","         [-0.5039, -0.5117, -0.5117,  ..., -0.8008, -0.8008, -0.8164],\n","         ...,\n","         [ 0.2852,  0.3008,  0.3633,  ..., -0.2930, -0.5742, -0.8242],\n","         [ 0.2930,  0.3086,  0.3320,  ..., -0.0898, -0.3867, -0.7070],\n","         [ 0.3320,  0.3555,  0.3477,  ...,  0.0664, -0.1992, -0.5117]]])]\n"]}]},{"cell_type":"markdown","source":["**Val**"],"metadata":{"id":"OLcbJ_pqy1LL"}},{"cell_type":"code","source":["data_dir = '/content/dataset/val'\n","dataset_val = datasets.ImageFolder(data_dir, transform=transforms.Resize((512,512)))\n","dataset_val.samples = [\n","    (p, p.replace(data_dir, data_dir + '_cropped'))\n","        for p, _ in dataset_val.samples\n","]"],"metadata":{"id":"9LfktOMpywEV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_loader = DataLoader(\n","    dataset_val,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    collate_fn=training.collate_pil\n",")\n","\n","for i, (x, y) in enumerate(val_loader):\n","    mtcnn(x, save_path=y)\n","    print('\\rBatch {} of {}'.format(i + 1, len(val_loader)), end='')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"-f27LwuSzBAl","executionInfo":{"status":"error","timestamp":1687100308050,"user_tz":-420,"elapsed":500,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"de51aaae-c775-48ee-d281-9134cf82e121"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-651aff304b31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m val_loader = DataLoader(\n\u001b[1;32m      2\u001b[0m     \u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_pil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'workers' is not defined"]}]},{"cell_type":"markdown","source":["**Adapt dataset**"],"metadata":{"id":"4L62qRxpzcsB"}},{"cell_type":"code","source":["optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n","scheduler = MultiStepLR(optimizer, [5, 10])\n","\n","trans = transforms.Compose([\n","    np.float32,\n","    transforms.ToTensor(),\n","    fixed_image_standardization\n","])\n","\n","train_dataset = datasets.ImageFolder('/content/dataset/train_cropped', transform=trans)\n","img_inds_train = np.arange(len(train_dataset))\n","np.random.shuffle(img_inds_train)\n","\n","val_dataset = datasets.ImageFolder('/content/dataset/val_cropped', transform=trans)\n","img_inds_val = np.arange(len(val_dataset))\n","np.random.shuffle(img_inds_val)\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    sampler=SubsetRandomSampler(img_inds_train)\n",")\n","val_loader = DataLoader(\n","    val_dataset,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    sampler=SubsetRandomSampler(img_inds_val)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"mVML-KinzeKr","executionInfo":{"status":"error","timestamp":1687100341428,"user_tz":-420,"elapsed":6,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"6c6db100-e856-406e-ffb0-4c243d339a31"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-2ec9b4891144>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m val_loader = DataLoader(\n\u001b[1;32m     25\u001b[0m     \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSubsetRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_inds_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'workers' is not defined"]}]},{"cell_type":"markdown","source":["**Training**"],"metadata":{"id":"-nTgRds-0ofZ"}},{"cell_type":"code","source":["loss_fn = torch.nn.CrossEntropyLoss()\n","metrics = {\n","    'fps': training.BatchTimer(),\n","    'acc': training.accuracy\n","}"],"metadata":{"id":"7yfkiCfG0bsO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["writer = SummaryWriter()\n","writer.iteration, writer.interval = 0, 10\n","\n","print('\\n\\nInitial')\n","print('-' * 10)\n","resnet.eval()\n","training.pass_epoch(\n","    resnet, loss_fn, val_loader,\n","    batch_metrics=metrics, show_running=True, device=device,\n","    writer=writer\n",")\n","\n","for epoch in range(epochs):\n","    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n","    print('-' * 10)\n","\n","    resnet.train()\n","    training.pass_epoch(\n","        resnet, loss_fn, train_loader, optimizer, scheduler,\n","        batch_metrics=metrics, show_running=True, device=device,\n","        writer=writer\n","    )\n","\n","    resnet.eval()\n","    training.pass_epoch(\n","        resnet, loss_fn, val_loader,\n","        batch_metrics=metrics, show_running=True, device=device,\n","        writer=writer\n","    )\n","\n","writer.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DoBBfsbP0qSX","executionInfo":{"status":"error","timestamp":1687097800609,"user_tz":-420,"elapsed":411625,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"8494828f-9c11-4f02-c3d9-218a1fc3e4e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Initial\n","----------\n","Valid |     1/1    | loss:    1.0637 | fps:    0.6090 | acc:    0.7692   \n","\n","Epoch 1/20\n","----------\n","Train |     3/3    | loss:    0.0054 | fps:    3.5250 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.9858 | fps:    7.6993 | acc:    0.8462   \n","\n","Epoch 2/20\n","----------\n","Train |     3/3    | loss:    0.0136 | fps:    3.5040 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.9429 | fps:    7.5642 | acc:    0.8846   \n","\n","Epoch 3/20\n","----------\n","Train |     3/3    | loss:    0.0113 | fps:    3.5109 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.8892 | fps:    7.2661 | acc:    0.8846   \n","\n","Epoch 4/20\n","----------\n","Train |     3/3    | loss:    0.0231 | fps:    3.5237 | acc:    0.9896   \n","Valid |     1/1    | loss:    0.8437 | fps:    6.6503 | acc:    0.8462   \n","\n","Epoch 5/20\n","----------\n","Train |     3/3    | loss:    0.0087 | fps:    3.6195 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.8212 | fps:    6.5171 | acc:    0.8462   \n","\n","Epoch 6/20\n","----------\n","Train |     3/3    | loss:    0.0051 | fps:    3.6324 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.8039 | fps:    6.2637 | acc:    0.8462   \n","\n","Epoch 7/20\n","----------\n","Train |     3/3    | loss:    0.0182 | fps:    3.5939 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.7872 | fps:    6.3275 | acc:    0.8462   \n","\n","Epoch 8/20\n","----------\n","Train |     3/3    | loss:    0.0027 | fps:    3.6236 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.7824 | fps:    6.2978 | acc:    0.8462   \n","\n","Epoch 9/20\n","----------\n","Train |     3/3    | loss:    0.0078 | fps:    3.6352 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.7849 | fps:    6.2669 | acc:    0.8462   \n","\n","Epoch 10/20\n","----------\n","Train |     3/3    | loss:    0.0071 | fps:    3.6601 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.7723 | fps:    5.9717 | acc:    0.8462   \n","\n","Epoch 11/20\n","----------\n","Train |     3/3    | loss:    0.0073 | fps:    3.6835 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.7611 | fps:    5.8461 | acc:    0.8462   \n","\n","Epoch 12/20\n","----------\n","Train |     3/3    | loss:    0.0041 | fps:    3.7573 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.7658 | fps:    5.6652 | acc:    0.8462   \n","\n","Epoch 13/20\n","----------\n","Train |     3/3    | loss:    0.0166 | fps:    3.8087 | acc:    1.0000   \n","Valid |     1/1    | loss:    0.7677 | fps:    5.4133 | acc:    0.8462   \n","\n","Epoch 14/20\n","----------\n","Train |     1/3    | loss:    0.0125 | fps:    2.2384 | acc:    1.0000   "]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-95-49d240c83dad>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     training.pass_epoch(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_running\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/facenet_pytorch/models/utils/training.py\u001b[0m in \u001b[0;36mpass_epoch\u001b[0;34m(model, loss_fn, loader, optimizer, scheduler, batch_metrics, show_running, device, writer)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mloss_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["resnet.eval()\n","predict = []\n","for x,y in val_loader:\n","  pred = resnet(x).detach().cpu()\n","  label = y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTAi2Qaz3YZY","executionInfo":{"status":"ok","timestamp":1687098161051,"user_tz":-420,"elapsed":5227,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"7420c3b2-c66e-4a13-d733-10c7801248b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["pred = np.argmax(pred,axis = 1)"],"metadata":{"id":"RqGsA8614Gug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import metrics\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"9uPWXOqb46P3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(label, pred, target_names=target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4NP5at24y1b","executionInfo":{"status":"ok","timestamp":1687098244663,"user_tz":-420,"elapsed":2,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"b231710b-9433-4a3a-b781-1ee29bd855e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                precision    recall  f1-score   support\n","\n","    ben_afflek       0.80      0.80      0.80         5\n","    elton_john       0.57      1.00      0.73         4\n","jerry_seinfeld       1.00      0.71      0.83         7\n","       madonna       1.00      0.80      0.89         5\n","  mindy_kaling       1.00      1.00      1.00         5\n","\n","      accuracy                           0.85        26\n","     macro avg       0.87      0.86      0.85        26\n","  weighted avg       0.90      0.85      0.85        26\n","\n"]}]},{"cell_type":"markdown","source":["#Faiss"],"metadata":{"id":"PLuhPRQfMNWN"}},{"cell_type":"markdown","source":["**AutoFaiss - CLIP + FAISS**"],"metadata":{"id":"wECxd0u35MlK"}},{"cell_type":"code","source":["!pip install clip-retrieval autofaiss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Cg5brF-m5Nqp","executionInfo":{"status":"ok","timestamp":1687098967484,"user_tz":-420,"elapsed":255790,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"a2b27b77-bda1-4e6e-b33f-6bbe57ff6898"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting clip-retrieval\n","  Downloading clip_retrieval-2.37.0-py3-none-any.whl (343 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.4/343.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting autofaiss\n","  Downloading autofaiss-2.15.8-py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting img2dataset<2,>=1.25.5 (from clip-retrieval)\n","  Downloading img2dataset-1.41.0-py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting clip-anytorch<3,>=2.5.0 (from clip-retrieval)\n","  Downloading clip_anytorch-2.5.2-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm<5,>=4.62.3 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (4.65.0)\n","Collecting fire<0.5.0,>=0.4.0 (from clip-retrieval)\n","  Downloading fire-0.4.0.tar.gz (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch<2,>=1.7.1 (from clip-retrieval)\n","  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m704.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (0.15.2+cu118)\n","Requirement already satisfied: numpy<2,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (1.22.4)\n","Collecting faiss-cpu<2,>=1.7.2 (from clip-retrieval)\n","  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flask<3,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (2.2.4)\n","Collecting flask-restful<1,>=0.3.9 (from clip-retrieval)\n","  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n","Collecting flask-cors<4,>=3.0.10 (from clip-retrieval)\n","  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n","Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (1.5.3)\n","Collecting pyarrow<8,>=6.0.1 (from clip-retrieval)\n","  Downloading pyarrow-7.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting webdataset<0.3,>=0.2 (from clip-retrieval)\n","  Downloading webdataset-0.2.48-py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: h5py<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (3.8.0)\n","Requirement already satisfied: prometheus-client<1,>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (0.16.0)\n","Collecting fsspec==2022.11.0 (from clip-retrieval)\n","  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentence-transformers<3,>=2.2.0 (from clip-retrieval)\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting wandb<0.13,>=0.12.10 (from clip-retrieval)\n","  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting open-clip-torch<3.0.0,>=2.0.0 (from clip-retrieval)\n","  Downloading open_clip_torch-2.20.0-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (2.27.1)\n","Collecting aiohttp<4,>=3.8.1 (from clip-retrieval)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multilingual-clip<2,>=1.0.10 (from clip-retrieval)\n","  Downloading multilingual_clip-1.0.10-py3-none-any.whl (20 kB)\n","Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from clip-retrieval) (1.26.15)\n","Collecting scipy<1.9.2 (from clip-retrieval)\n","  Downloading scipy-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting embedding-reader<2,>=1.5.1 (from autofaiss)\n","  Downloading embedding_reader-1.5.1-py3-none-any.whl (18 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4,>=3.8.1->clip-retrieval)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting ftfy (from clip-anytorch<3,>=2.5.0->clip-retrieval)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip-anytorch<3,>=2.5.0->clip-retrieval) (2022.10.31)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire<0.5.0,>=0.4.0->clip-retrieval) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire<0.5.0,>=0.4.0->clip-retrieval) (2.3.0)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask<3,>=2.0.3->clip-retrieval) (2.3.0)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask<3,>=2.0.3->clip-retrieval) (3.1.2)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask<3,>=2.0.3->clip-retrieval) (2.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask<3,>=2.0.3->clip-retrieval) (8.1.3)\n","Collecting aniso8601>=0.82 (from flask-restful<1,>=0.3.9->clip-retrieval)\n","  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from flask-restful<1,>=0.3.9->clip-retrieval) (2022.7.1)\n","Requirement already satisfied: opencv-python-headless<5,>=4.5.5.62 in /usr/local/lib/python3.10/dist-packages (from img2dataset<2,>=1.25.5->clip-retrieval) (4.7.0.72)\n","Collecting exifread-nocycle<4,>=3.0.1 (from img2dataset<2,>=1.25.5->clip-retrieval)\n","  Downloading ExifRead_nocycle-3.0.1-py3-none-any.whl (39 kB)\n","Requirement already satisfied: albumentations<2,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from img2dataset<2,>=1.25.5->clip-retrieval) (1.2.1)\n","Collecting dataclasses<1.0.0,>=0.6 (from img2dataset<2,>=1.25.5->clip-retrieval)\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Collecting transformers (from multilingual-clip<2,>=1.0.10->clip-retrieval)\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub (from open-clip-torch<3.0.0,>=2.0.0->clip-retrieval)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece (from open-clip-torch<3.0.0,>=2.0.0->clip-retrieval)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf<4 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch<3.0.0,>=2.0.0->clip-retrieval) (3.20.3)\n","Collecting timm (from open-clip-torch<3.0.0,>=2.0.0->clip-retrieval)\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->clip-retrieval) (2.8.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.1->clip-retrieval) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.1->clip-retrieval) (3.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3,>=2.2.0->clip-retrieval) (1.2.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3,>=2.2.0->clip-retrieval) (3.8.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2,>=1.7.1->clip-retrieval) (4.5.0)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2,>=1.7.1->clip-retrieval)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2,>=1.7.1->clip-retrieval)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2,>=1.7.1->clip-retrieval)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2,>=1.7.1->clip-retrieval)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7.1->clip-retrieval) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7.1->clip-retrieval) (0.40.0)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision<2,>=0.10.1 (from clip-retrieval)\n","  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<2,>=0.10.1->clip-retrieval) (8.4.0)\n","Collecting GitPython>=1.0.0 (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (2.3)\n","Collecting shortuuid>=0.5.0 (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading sentry_sdk-1.25.1-py2.py3-none-any.whl (206 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (6.0)\n","Collecting pathtools (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Collecting braceexpand (from webdataset<0.3,>=0.2->clip-retrieval)\n","  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (0.19.3)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (0.0.4)\n","Collecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->clip-retrieval) (3.12.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->clip-retrieval) (23.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask<3,>=2.0.3->clip-retrieval) (2.1.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->multilingual-clip<2,>=1.0.10->clip-retrieval)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->multilingual-clip<2,>=1.0.10->clip-retrieval)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip-anytorch<3,>=2.5.0->clip-retrieval) (0.2.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers<3,>=2.2.0->clip-retrieval) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3,>=2.2.0->clip-retrieval) (3.1.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13,>=0.12.10->clip-retrieval)\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (3.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (2.25.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (2023.4.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset<2,>=1.25.5->clip-retrieval) (1.4.1)\n","Building wheels for collected packages: fire, sentence-transformers, pathtools\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115927 sha256=5328f8bf44d8c081386a66beb0c358abf50cbe086562d7fbfe2ce0644d42ffef\n","  Stored in directory: /root/.cache/pip/wheels/26/9a/dd/2818b1b023daf077ec3e625c47ae446aca587a5abe48e05212\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=8477b9a43558ff9dac8a7a8eb145dac577510ca26d4f33fa3f387af46765c811\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=d2ac6cb3c8a9ea98906b96d2b00f04535d20fbc53b64c1963d2699bdde1478ac\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built fire sentence-transformers pathtools\n","Installing collected packages: tokenizers, sentencepiece, safetensors, pathtools, faiss-cpu, exifread-nocycle, dataclasses, braceexpand, aniso8601, webdataset, smmap, shortuuid, setproctitle, sentry-sdk, scipy, pyarrow, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, multidict, ftfy, fsspec, frozenlist, fire, docker-pycreds, async-timeout, yarl, nvidia-cudnn-cu11, huggingface-hub, gitdb, aiosignal, transformers, torch, GitPython, flask-restful, flask-cors, embedding-reader, aiohttp, wandb, torchvision, multilingual-clip, autofaiss, timm, sentence-transformers, img2dataset, clip-anytorch, open-clip-torch, clip-retrieval\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.10.1\n","    Uninstalling scipy-1.10.1:\n","      Successfully uninstalled scipy-1.10.1\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 9.0.0\n","    Uninstalling pyarrow-9.0.0:\n","      Successfully uninstalled pyarrow-9.0.0\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2023.4.0\n","    Uninstalling fsspec-2023.4.0:\n","      Successfully uninstalled fsspec-2023.4.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.1+cu118\n","    Uninstalling torch-2.0.1+cu118:\n","      Successfully uninstalled torch-2.0.1+cu118\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.15.2+cu118\n","    Uninstalling torchvision-0.15.2+cu118:\n","      Successfully uninstalled torchvision-0.15.2+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.31 aiohttp-3.8.4 aiosignal-1.3.1 aniso8601-9.0.1 async-timeout-4.0.2 autofaiss-2.15.8 braceexpand-0.1.7 clip-anytorch-2.5.2 clip-retrieval-2.37.0 dataclasses-0.6 docker-pycreds-0.4.0 embedding-reader-1.5.1 exifread-nocycle-3.0.1 faiss-cpu-1.7.4 fire-0.4.0 flask-cors-3.0.10 flask-restful-0.3.10 frozenlist-1.3.3 fsspec-2022.11.0 ftfy-6.1.1 gitdb-4.0.10 huggingface-hub-0.15.1 img2dataset-1.41.0 multidict-6.0.4 multilingual-clip-1.0.10 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 open-clip-torch-2.20.0 pathtools-0.1.2 pyarrow-7.0.0 safetensors-0.3.1 scipy-1.9.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 sentry-sdk-1.25.1 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 timm-0.9.2 tokenizers-0.13.3 torch-1.13.1 torchvision-0.14.1 transformers-4.30.2 wandb-0.12.21 webdataset-0.2.48 yarl-1.9.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses","fsspec","pyarrow","scipy","torch","torchvision"]}}},"metadata":{}}]},{"cell_type":"code","source":["!clip-retrieval inference --input_dataset /content/dataset/train_cropped --output_folder /content/dataset/train_embedding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4S0BD35u62bv","executionInfo":{"status":"ok","timestamp":1687099315156,"user_tz":-420,"elapsed":260914,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"0f7a41ec-0dc9-4c2a-c92d-45b19fba041b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples has been estimated to be 91\n","Starting the worker\n","dataset is 30\n","Starting work on task 0\n","100%|███████████████████████████████████████| 354M/354M [00:05<00:00, 59.4MiB/s]\n","warming up with batch size 256 on cpu\n","done warming up in 206.28352618217468s\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"," sample_per_sec 3 ; sample_count 91 "]}]},{"cell_type":"code","source":["!autofaiss build_index --embeddings=\"/content/dataset/train_embedding/img_emb\" \\\n","                    --index_path=\"/content/knn.index\" \\\n","                    --index_infos_path=\"/content/infos.json\" \\\n","                    --metric_type=\"ip\" \\\n","                    --max_index_query_time_ms=10 \\\n","                    --max_index_memory_usage=\"4GB\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1DGa62NS7oxA","executionInfo":{"status":"ok","timestamp":1687099347324,"user_tz":-420,"elapsed":5914,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"70d620d9-a201-42a8-9231-0bcb299b1651"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-18 14:42:24,239 [INFO]: Using 2 omp threads (processes), consider increasing --nb_cores if you have more\n","2023-06-18 14:42:24,240 [INFO]: Launching the whole pipeline 06/18/2023, 14:42:24\n","2023-06-18 14:42:24,240 [INFO]: Reading total number of vectors and dimension 06/18/2023, 14:42:24\n","100% 1/1 [00:00<00:00, 9776.93it/s]\n","2023-06-18 14:42:24,569 [INFO]: There are 91 embeddings of dim 512\n","2023-06-18 14:42:24,575 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.3298 secs\n","2023-06-18 14:42:24,576 [INFO]: \tCompute estimated construction time of the index 06/18/2023, 14:42:24\n","2023-06-18 14:42:24,577 [INFO]: \t\t-> Train: 16.7 minutes\n","2023-06-18 14:42:24,577 [INFO]: \t\t-> Add: 0.0 seconds\n","2023-06-18 14:42:24,577 [INFO]: \t\tTotal: 16.7 minutes\n","2023-06-18 14:42:24,584 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0007 secs\n","2023-06-18 14:42:24,585 [INFO]: \tChecking that your have enough memory available to create the index 06/18/2023, 14:42:24\n","2023-06-18 14:42:24,605 [INFO]: 200.2KB of memory will be needed to build the index (more might be used if you have more)\n","2023-06-18 14:42:24,606 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0209 secs\n","2023-06-18 14:42:24,614 [INFO]: \tSelecting most promising index types given data characteristics 06/18/2023, 14:42:24\n","2023-06-18 14:42:24,614 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0001 secs\n","2023-06-18 14:42:24,614 [INFO]: \tCreating the index 06/18/2023, 14:42:24\n","2023-06-18 14:42:24,615 [INFO]: \t\t-> Instanciate the index Flat 06/18/2023, 14:42:24\n","2023-06-18 14:42:24,627 [INFO]: \t\t>>> Finished \"-> Instanciate the index Flat\" in 0.0123 secs\n","2023-06-18 14:42:24,628 [INFO]: \t\t-> Adding the vectors to the index 06/18/2023, 14:42:24\n","2023-06-18 14:42:24,628 [INFO]: The memory available for adding the vectors is 32.0GB(total available - used by the index)\n","2023-06-18 14:42:24,629 [INFO]: Using a batch size of 488281 (memory overhead 953.7MB)\n","100% 1/1 [00:00<00:00,  2.64it/s]\n","2023-06-18 14:42:25,038 [INFO]: \tComputing best hyperparameters for index /content/knn.index 06/18/2023, 14:42:25\n","2023-06-18 14:42:25,038 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /content/knn.index\" in 0.0001 secs\n","2023-06-18 14:42:25,042 [INFO]: The best hyperparameters are: \n","2023-06-18 14:42:25,043 [INFO]: \tCompute fast metrics 06/18/2023, 14:42:25\n","  0% 0/1 [00:00<?, ?it/s]\n","2023-06-18 14:42:25,714 [INFO]: \t>>> Finished \"Compute fast metrics\" in 0.6711 secs\n","2023-06-18 14:42:25,714 [INFO]: \tSaving the index on local disk 06/18/2023, 14:42:25\n","2023-06-18 14:42:25,725 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.0105 secs\n","2023-06-18 14:42:25,727 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 1.0986 secs\n","2023-06-18 14:42:25,731 [INFO]: {\n","2023-06-18 14:42:25,731 [INFO]: \tindex_key: Flat\n","2023-06-18 14:42:25,732 [INFO]: \tindex_param: \n","2023-06-18 14:42:25,732 [INFO]: \tindex_path: /content/knn.index\n","2023-06-18 14:42:25,732 [INFO]: \tsize in bytes: 186413\n","2023-06-18 14:42:25,732 [INFO]: \tavg_search_speed_ms: 0.196892146857056\n","2023-06-18 14:42:25,732 [INFO]: \t99p_search_speed_ms: 4.986515099917602\n","2023-06-18 14:42:25,732 [INFO]: \treconstruction error %: 0.0\n","2023-06-18 14:42:25,732 [INFO]: \tnb vectors: 91\n","2023-06-18 14:42:25,732 [INFO]: \tvectors dimension: 512\n","2023-06-18 14:42:25,732 [INFO]: \tcompression ratio: 0.9997586005267873\n","2023-06-18 14:42:25,733 [INFO]: }\n","2023-06-18 14:42:25,733 [INFO]: \t>>> Finished \"Creating the index\" in 1.1181 secs\n","2023-06-18 14:42:25,733 [INFO]: >>> Finished \"Launching the whole pipeline\" in 1.4930 secs\n","(<faiss.swigfaiss_avx2.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x7f7f006008d0> >, {'index_key': 'Flat', 'index_param': '', 'index_path': '/content/knn.index', 'size in bytes': 186413, 'avg_search_speed_ms': 0.196892146857056, '99p_search_speed_ms': 4.986515099917602, 'reconstruction error %': 0.0, 'nb vectors': 91, 'vectors dimension': 512, 'compression ratio': 0.9997586005267873})\n"]}]},{"cell_type":"markdown","source":["**Search**"],"metadata":{"id":"xDBDiUTb9XM8"}},{"cell_type":"code","source":["import faiss\n","import torch\n","import clip\n","import os\n","import pandas as pd"],"metadata":{"id":"DKeTjJil9nVI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_parquet(\"/content/dataset/train_embedding/metadata/metadata_0.parquet\")\n","image_list = df[\"image_path\"].tolist()\n","ind = faiss.read_index(\"/content/knn.index\")"],"metadata":{"id":"aT_txYvj9Yye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)"],"metadata":{"id":"UDAo9OaJ-Ojj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Set up val dataset**"],"metadata":{"id":"1VmZGcXYBJls"}},{"cell_type":"code","source":["trans = transforms.Compose([\n","    np.float32,\n","    # transforms.ToTensor(),\n","    fixed_image_standardization\n","])\n","\n","val_dataset = datasets.ImageFolder('/content/dataset/val_cropped')\n","test_class_idx = val_dataset.class_to_idx"],"metadata":{"id":"eKXa4FJwBLgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx_to_class = {v: k for k, v in test_class_idx.items()}\n","print(idx_to_class)\n","\n","target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(target_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJ50o75lDo4I","executionInfo":{"status":"ok","timestamp":1687101070075,"user_tz":-420,"elapsed":3,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"70fa5888-59ff-4b1d-f621-fed3bbc17158"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'ben_afflek', 1: 'elton_john', 2: 'jerry_seinfeld', 3: 'madonna', 4: 'mindy_kaling'}\n","['ben_afflek', 'elton_john', 'jerry_seinfeld', 'madonna', 'mindy_kaling']\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","from collections import defaultdict"],"metadata":{"id":"lijM_xYA_f7V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_tensor = preprocess(Image.open('/content/dataset/val/ben_afflek/httpabsolumentgratuitfreefrimagesbenaffleckjpg.jpg'))\n","image_features = model.encode_image(torch.unsqueeze(image_tensor.to(device), dim=0))\n","image_features /= image_features.norm(dim=-1, keepdim=True)\n","image_embeddings = image_features.cpu().detach().numpy().astype('float32')\n","D, I = ind.search(image_embeddings, 5)"],"metadata":{"id":"wTm0BXJM_nj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = []\n","preds = []\n","k = 3\n","for x,y in val_dataset:\n","    image_tensor = preprocess(x)\n","    image_features = model.encode_image(torch.unsqueeze(image_tensor.to(device), dim=0))\n","    image_features /= image_features.norm(dim=-1, keepdim=True)\n","    image_embeddings = image_features.cpu().detach().numpy().astype('float32')\n","    D, I = ind.search(image_embeddings, k)\n","    # print(D,I)\n","    i_candidate = defaultdict(int)\n","    for D_ele,I_ele in zip(D[0],I[0]):\n","      if D_ele > 0.7:\n","        name = image_list[I_ele].split('/')[-2]\n","        i_candidate[test_class_idx[name]] += 1\n","    key_with_max_value = max(i_candidate, key=lambda k: i_candidate[k])\n","\n","    preds.append(key_with_max_value)\n","    labels.append(y)"],"metadata":{"id":"XVG8NMWrAEq5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import metrics\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"ywAIRXcFFlRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(labels, preds, target_names=target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAOKG4y1FoyF","executionInfo":{"status":"ok","timestamp":1687101699611,"user_tz":-420,"elapsed":3,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"4a08c8af-c6f7-4dbc-ee58-a6b67820fd59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                precision    recall  f1-score   support\n","\n","    ben_afflek       0.83      1.00      0.91         5\n","    elton_john       1.00      1.00      1.00         4\n","jerry_seinfeld       1.00      0.71      0.83         7\n","       madonna       1.00      1.00      1.00         5\n","  mindy_kaling       0.83      1.00      0.91         5\n","\n","      accuracy                           0.92        26\n","     macro avg       0.93      0.94      0.93        26\n","  weighted avg       0.94      0.92      0.92        26\n","\n"]}]},{"cell_type":"markdown","source":["**Facenet + Faiss**"],"metadata":{"id":"ZBHuB5GsG2CY"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","mtcnn = MTCNN(\n","    image_size=160, margin=0, min_face_size=20,\n","    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n","    device=device\n",")\n","\n","facenet = InceptionResnetV1(pretrained='vggface2').eval()\n","facenet = facenet.to(device)"],"metadata":{"id":"J9C51x8FG4up"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","# data = np.load('/content/dataset/train_embedding/img_emb/img_emb_0.npy')\n","#data.shape"],"metadata":{"id":"56gZDbS4H8cr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings, labels, class_to_idx = X_train, y_train, X_train_class_idx"],"metadata":{"id":"drQdi7pCJGib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -r facenet_emb\n","!mkdir facenet_emb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1akevQBJ-4I","executionInfo":{"status":"ok","timestamp":1687102698504,"user_tz":-420,"elapsed":460,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"28b1ec59-80e5-40dd-8343-1768cf09b974"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'facenet_emb': No such file or directory\n"]}]},{"cell_type":"code","source":["np.save('/content/facenet_emb/facenet.npy', embeddings)"],"metadata":{"id":"MXgsjMVWJVz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!autofaiss build_index --embeddings=\"/content/facenet_emb\" \\\n","                    --index_path=\"/content/knn_facenet.index\" \\\n","                    --index_infos_path=\"/content/infos_facenet.json\" \\\n","                    --metric_type=\"ip\" \\\n","                    --max_index_query_time_ms=10 \\\n","                    --max_index_memory_usage=\"4GB\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"As7HjYOsJq26","executionInfo":{"status":"ok","timestamp":1687102727362,"user_tz":-420,"elapsed":1416,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"291e5493-e7c6-4498-f787-b12da92fbcd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-18 15:38:47,479 [INFO]: Using 2 omp threads (processes), consider increasing --nb_cores if you have more\n","2023-06-18 15:38:47,479 [INFO]: Launching the whole pipeline 06/18/2023, 15:38:47\n","2023-06-18 15:38:47,479 [INFO]: Reading total number of vectors and dimension 06/18/2023, 15:38:47\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 14122.24it/s]\n","2023-06-18 15:38:47,519 [INFO]: There are 93 embeddings of dim 512\n","2023-06-18 15:38:47,519 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0394 secs\n","2023-06-18 15:38:47,519 [INFO]: \tCompute estimated construction time of the index 06/18/2023, 15:38:47\n","2023-06-18 15:38:47,519 [INFO]: \t\t-> Train: 16.7 minutes\n","2023-06-18 15:38:47,519 [INFO]: \t\t-> Add: 0.0 seconds\n","2023-06-18 15:38:47,519 [INFO]: \t\tTotal: 16.7 minutes\n","2023-06-18 15:38:47,519 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0002 secs\n","2023-06-18 15:38:47,519 [INFO]: \tChecking that your have enough memory available to create the index 06/18/2023, 15:38:47\n","2023-06-18 15:38:47,520 [INFO]: 204.6KB of memory will be needed to build the index (more might be used if you have more)\n","2023-06-18 15:38:47,521 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0014 secs\n","2023-06-18 15:38:47,521 [INFO]: \tSelecting most promising index types given data characteristics 06/18/2023, 15:38:47\n","2023-06-18 15:38:47,521 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n","2023-06-18 15:38:47,521 [INFO]: \tCreating the index 06/18/2023, 15:38:47\n","2023-06-18 15:38:47,521 [INFO]: \t\t-> Instanciate the index Flat 06/18/2023, 15:38:47\n","2023-06-18 15:38:47,528 [INFO]: \t\t>>> Finished \"-> Instanciate the index Flat\" in 0.0067 secs\n","2023-06-18 15:38:47,528 [INFO]: \t\t-> Adding the vectors to the index 06/18/2023, 15:38:47\n","2023-06-18 15:38:47,528 [INFO]: The memory available for adding the vectors is 32.0GB(total available - used by the index)\n","2023-06-18 15:38:47,528 [INFO]: Using a batch size of 488281 (memory overhead 953.7MB)\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 55.10it/s]\n","2023-06-18 15:38:47,551 [INFO]: \tComputing best hyperparameters for index /content/knn_facenet.index 06/18/2023, 15:38:47\n","2023-06-18 15:38:47,551 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /content/knn_facenet.index\" in 0.0000 secs\n","2023-06-18 15:38:47,556 [INFO]: The best hyperparameters are: \n","2023-06-18 15:38:47,556 [INFO]: \tCompute fast metrics 06/18/2023, 15:38:47\n","\r  0% 0/1 [00:00<?, ?it/s]\r  0% 0/1 [00:00<?, ?it/s]\n","2023-06-18 15:38:47,653 [INFO]: \t>>> Finished \"Compute fast metrics\" in 0.0973 secs\n","2023-06-18 15:38:47,653 [INFO]: \tSaving the index on local disk 06/18/2023, 15:38:47\n","2023-06-18 15:38:47,655 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.0013 secs\n","2023-06-18 15:38:47,655 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 0.1273 secs\n","2023-06-18 15:38:47,655 [INFO]: {\n","2023-06-18 15:38:47,655 [INFO]: \tindex_key: Flat\n","2023-06-18 15:38:47,655 [INFO]: \tindex_param: \n","2023-06-18 15:38:47,656 [INFO]: \tindex_path: /content/knn_facenet.index\n","2023-06-18 15:38:47,656 [INFO]: \tsize in bytes: 190509\n","2023-06-18 15:38:47,656 [INFO]: \tavg_search_speed_ms: 0.03383925611950883\n","2023-06-18 15:38:47,656 [INFO]: \t99p_search_speed_ms: 0.06603454958167275\n","2023-06-18 15:38:47,656 [INFO]: \treconstruction error %: 0.0\n","2023-06-18 15:38:47,656 [INFO]: \tnb vectors: 93\n","2023-06-18 15:38:47,656 [INFO]: \tvectors dimension: 512\n","2023-06-18 15:38:47,656 [INFO]: \tcompression ratio: 0.9997637906870541\n","2023-06-18 15:38:47,656 [INFO]: }\n","2023-06-18 15:38:47,656 [INFO]: \t>>> Finished \"Creating the index\" in 0.1351 secs\n","2023-06-18 15:38:47,656 [INFO]: >>> Finished \"Launching the whole pipeline\" in 0.1768 secs\n","(<faiss.swigfaiss_avx2.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x7f9c5c473db0> >, {'index_key': 'Flat', 'index_param': '', 'index_path': '/content/knn_facenet.index', 'size in bytes': 190509, 'avg_search_speed_ms': 0.03383925611950883, '99p_search_speed_ms': 0.06603454958167275, 'reconstruction error %': 0.0, 'nb vectors': 93, 'vectors dimension': 512, 'compression ratio': 0.9997637906870541})\n"]}]},{"cell_type":"code","source":["ind = faiss.read_index(\"/content/knn_facenet.index\")"],"metadata":{"id":"pMYhk3G7KMiT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4f62_9QgM9Nz","executionInfo":{"status":"ok","timestamp":1687103464331,"user_tz":-420,"elapsed":395,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"bb23f8c9-2e4b-41f6-af81-d7b80fa140d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"]}]},{"cell_type":"code","source":["preds = []\n","k = 5\n","for image_emb in X_test:\n","    D, I = ind.search(image_emb[np.newaxis, :], k)\n","    print(D,I)\n","    i_candidate = defaultdict(int)\n","    for D_ele,I_ele in zip(D[0],I[0]):\n","      # if D_ele > 0.7:\n","        cls = labels[I_ele]\n","        i_candidate[cls] += 1\n","    # try:\n","    key_with_max_value = max(i_candidate, key=lambda k: i_candidate[k])\n","    # except:\n","    #   print(\"error\")\n","    #   key_with_max_value = 0\n","\n","    preds.append(key_with_max_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAHNDbawKg-2","executionInfo":{"status":"ok","timestamp":1687103606494,"user_tz":-420,"elapsed":373,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"e0ae77f5-2d0f-41e8-a1cf-8c03dfa58971"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.741638   0.6051879  0.5813914  0.5583176  0.55315816]] [[ 9  0  8  1 11]]\n","[[0.7357062  0.72427213 0.7227762  0.6088511  0.57897544]] [[ 4  1  7 13  6]]\n","[[0.9597626  0.5537939  0.5434317  0.5231418  0.50185114]] [[ 5  9  7  1 11]]\n","[[0.4220538  0.34983587 0.29222882 0.25649747 0.2458143 ]] [[ 0  3  9 23  5]]\n","[[0.6477177  0.5769872  0.5712532  0.53599465 0.5292058 ]] [[10  9  5 11  0]]\n","[[0.8913976  0.8131018  0.7927766  0.69853055 0.68443555]] [[21 19 28 15 16]]\n","[[0.54405385 0.5172003  0.5000552  0.49620724 0.4705905 ]] [[30 57 29 54 69]]\n","[[0.72608554 0.7127732  0.6990677  0.66552025 0.6223527 ]] [[19 21 28 29 17]]\n","[[0.80854964 0.7383498  0.7068225  0.69533753 0.64965105]] [[21 16 19 28 29]]\n","[[0.56915426 0.5687758  0.54122424 0.49597368 0.4886323 ]] [[28 16 27 21 19]]\n","[[0.77401054 0.76777744 0.76656365 0.7366657  0.72549856]] [[31 49 35 48 45]]\n","[[0.7221998 0.7129574 0.6962161 0.680742  0.6761063]] [[37 45 43 40 33]]\n","[[0.7972399  0.79268676 0.79146916 0.78686774 0.7819207 ]] [[49 40 31 45 43]]\n","[[0.7731717  0.7705909  0.73045754 0.7239065  0.70735097]] [[31 49 37 34 48]]\n","[[0.8069067 0.7810869 0.7730789 0.7716552 0.7702409]] [[31 35 49 48 45]]\n","[[0.7426251  0.6071979  0.60161895 0.46554542 0.4566389 ]] [[53 56 52 68 70]]\n","[[0.6865746  0.63282084 0.6194694  0.55391514 0.53525925]] [[61 23 67 69 60]]\n","[[0.78430593 0.7716476  0.7337215  0.64537174 0.62677133]] [[53 52 56 70 54]]\n","[[0.7902976  0.70052904 0.6882029  0.6683689  0.66698587]] [[59 55 67 60 61]]\n","[[0.62853944 0.50820565 0.48293722 0.46322653 0.4583382 ]] [[60 53 55 57 67]]\n","[[0.6592228  0.64348453 0.63745284 0.60418904 0.596264  ]] [[72 92 89 74 86]]\n","[[0.773103   0.7441367  0.740299   0.71249604 0.71242595]] [[89 77 72 82 75]]\n","[[0.8055912  0.7385198  0.71543014 0.7074103  0.6967662 ]] [[88 76 82 77 72]]\n","[[0.780224   0.7648871  0.7462474  0.74605286 0.72418356]] [[74 75 82 77 92]]\n","[[0.75110483 0.6764641  0.6360776  0.6294644  0.62081265]] [[71 83 88 76 91]]\n"]}]},{"cell_type":"code","source":["preds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWkPYlDyNP4K","executionInfo":{"status":"ok","timestamp":1687103567513,"user_tz":-420,"elapsed":547,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"47dd4224-89d0-44ec-86c3-886529daf8d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4]"]},"metadata":{},"execution_count":151}]},{"cell_type":"code","source":["target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(y_test, preds, target_names=target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhFZGDzyNExn","executionInfo":{"status":"ok","timestamp":1687103612348,"user_tz":-420,"elapsed":1404,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"c5614e96-8da3-46b1-9e2c-e585ff8462ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                precision    recall  f1-score   support\n","\n","    ben_afflek       1.00      1.00      1.00         5\n","    elton_john       1.00      0.80      0.89         5\n","jerry_seinfeld       1.00      1.00      1.00         5\n","       madonna       0.83      1.00      0.91         5\n","  mindy_kaling       1.00      1.00      1.00         5\n","\n","      accuracy                           0.96        25\n","     macro avg       0.97      0.96      0.96        25\n","  weighted avg       0.97      0.96      0.96        25\n","\n"]}]},{"cell_type":"code","source":["preds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1cYjg9rFNaKA","executionInfo":{"status":"ok","timestamp":1687103591385,"user_tz":-420,"elapsed":558,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"13508221658046017690"}},"outputId":"5abff5bd-299f-4bdb-af97-fb68a39199cc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4]"]},"metadata":{},"execution_count":154}]},{"cell_type":"markdown","source":["**Alignment**"],"metadata":{"id":"hLjNfo9azK0y"}},{"cell_type":"code","source":["!pip install --upgrade imutils"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWVjoAsqzJr_","executionInfo":{"status":"ok","timestamp":1687163829925,"user_tz":-420,"elapsed":5615,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"93a73da7-d27b-46cb-8078-6f95e4eb3d29"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\n"]}]},{"cell_type":"code","source":["!pip install dlib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvQu-6AHzOvy","executionInfo":{"status":"ok","timestamp":1687163845394,"user_tz":-420,"elapsed":7923,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"14d16652-adf7-4976-ad53-37c830951494"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.1)\n"]}]},{"cell_type":"code","source":["from imutils import face_utils\n","import numpy as np\n","import argparse\n","import imutils\n","import dlib\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from imutils.face_utils import FaceAligner\n","from imutils.face_utils import rect_to_bb\n","import math\n","import matplotlib.pyplot as plt"],"metadata":{"id":"uolVosxuzQSM","executionInfo":{"status":"ok","timestamp":1687163959076,"user_tz":-420,"elapsed":406,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["detector = dlib.get_frontal_face_detector()\n","predictor = dlib.shape_predictor('/content/drive/MyDrive/faceRecognition/dlib/shape_predictor_68_face_landmarks.dat')\n","predictor2 = dlib.shape_predictor('/content/drive/MyDrive/faceRecognition/dlib/shape_predictor_81_face_landmarks.dat')"],"metadata":{"id":"qdva6HLdzvma","executionInfo":{"status":"ok","timestamp":1687163998538,"user_tz":-420,"elapsed":8478,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def facial_landmarks(image):\n","    try:\n","        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    except:\n","        grayscale_image = image\n","\n","    # array of rectangles surrounding faces detected\n","    rectangles = detector(grayscale_image, 1)\n","\n","    # If at least one face is detected, find its landmarks\n","    if len(rectangles) > 0:\n","        # Get 68 landmark points\n","        faceLandmarks = predictor(grayscale_image, rectangles[0])\n","        faceLandmarks = face_utils.shape_to_np(faceLandmarks)\n","        return faceLandmarks,rectangles\n","    else:\n","        return None"],"metadata":{"id":"MJKj8zoXz3R6","executionInfo":{"status":"ok","timestamp":1687164001235,"user_tz":-420,"elapsed":442,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from skimage import transform as trans\n","\n","__file__ = 'test'\n","\n","# reference facial points, a list of coordinates (x,y)\n","REFERENCE_FACIAL_POINTS = [\n","    [30.29459953, 51.69630051],\n","    [65.53179932, 51.50139999],\n","    [48.02519989, 71.73660278],\n","    [33.54930115, 92.3655014],\n","    [62.72990036, 92.20410156]\n","]\n","\n","DEFAULT_CROP_SIZE = (96, 112)\n","\n","\n","class FaceWarpException(Exception):\n","    def __str__(self):\n","        return 'In File {}:{}'.format(\n","            __file__, super.__str__(self))\n","\n","\n","def get_reference_facial_points(output_size=None,\n","                                inner_padding_factor=0.0,\n","                                outer_padding=(0, 0),\n","                                default_square=False):\n","    tmp_5pts = np.array(REFERENCE_FACIAL_POINTS)\n","    tmp_crop_size = np.array(DEFAULT_CROP_SIZE)\n","\n","    # 0) make the inner region a square\n","    if default_square:\n","        size_diff = max(tmp_crop_size) - tmp_crop_size\n","        tmp_5pts += size_diff / 2\n","        tmp_crop_size += size_diff\n","\n","    # print('---> default:')\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              reference_5pts = ', tmp_5pts)\n","\n","    if (output_size and\n","            output_size[0] == tmp_crop_size[0] and\n","            output_size[1] == tmp_crop_size[1]):\n","        print('output_size == DEFAULT_CROP_SIZE {}: return default reference points'.format(tmp_crop_size))\n","        return tmp_5pts\n","\n","    if (inner_padding_factor == 0 and\n","            outer_padding == (0, 0)):\n","        if output_size is None:\n","            print('No paddings to do: return default reference points')\n","            return tmp_5pts\n","        else:\n","            raise FaceWarpException(\n","                'No paddings to do, output_size must be None or {}'.format(tmp_crop_size))\n","\n","    # check output size\n","    if not (0 <= inner_padding_factor <= 1.0):\n","        raise FaceWarpException('Not (0 <= inner_padding_factor <= 1.0)')\n","\n","    if ((inner_padding_factor > 0 or outer_padding[0] > 0 or outer_padding[1] > 0)\n","            and output_size is None):\n","        output_size = tmp_crop_size * \\\n","                      (1 + inner_padding_factor * 2).astype(np.int32)\n","        output_size += np.array(outer_padding)\n","        print('              deduced from paddings, output_size = ', output_size)\n","\n","    if not (outer_padding[0] < output_size[0]\n","            and outer_padding[1] < output_size[1]):\n","        raise FaceWarpException('Not (outer_padding[0] < output_size[0]'\n","                                'and outer_padding[1] < output_size[1])')\n","\n","    # 1) pad the inner region according inner_padding_factor\n","    # print('---> STEP1: pad the inner region according inner_padding_factor')\n","    if inner_padding_factor > 0:\n","        size_diff = tmp_crop_size * inner_padding_factor * 2\n","        tmp_5pts += size_diff / 2\n","        tmp_crop_size += np.round(size_diff).astype(np.int32)\n","\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              reference_5pts = ', tmp_5pts)\n","\n","    # 2) resize the padded inner region\n","    # print('---> STEP2: resize the padded inner region')\n","    size_bf_outer_pad = np.array(output_size) - np.array(outer_padding) * 2\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              size_bf_outer_pad = ', size_bf_outer_pad)\n","\n","    if size_bf_outer_pad[0] * tmp_crop_size[1] != size_bf_outer_pad[1] * tmp_crop_size[0]:\n","        raise FaceWarpException('Must have (output_size - outer_padding)'\n","                                '= some_scale * (crop_size * (1.0 + inner_padding_factor)')\n","\n","    scale_factor = size_bf_outer_pad[0].astype(np.float32) / tmp_crop_size[0]\n","    # print('              resize scale_factor = ', scale_factor)\n","    tmp_5pts = tmp_5pts * scale_factor\n","    #    size_diff = tmp_crop_size * (scale_factor - min(scale_factor))\n","    #    tmp_5pts = tmp_5pts + size_diff / 2\n","    tmp_crop_size = size_bf_outer_pad\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              reference_5pts = ', tmp_5pts)\n","\n","    # 3) add outer_padding to make output_size\n","    reference_5point = tmp_5pts + np.array(outer_padding)\n","    tmp_crop_size = output_size\n","    # print('---> STEP3: add outer_padding to make output_size')\n","    # print('              crop_size = ', tmp_crop_size)\n","    # print('              reference_5pts = ', tmp_5pts)\n","    #\n","    # print('===> end get_reference_facial_points\\n')\n","\n","    return reference_5point\n","\n","\n","def get_affine_transform_matrix(src_pts, dst_pts):\n","    tfm = np.float32([[1, 0, 0], [0, 1, 0]])\n","    n_pts = src_pts.shape[0]\n","    ones = np.ones((n_pts, 1), src_pts.dtype)\n","    src_pts_ = np.hstack([src_pts, ones])\n","    dst_pts_ = np.hstack([dst_pts, ones])\n","\n","    A, res, rank, s = np.linalg.lstsq(src_pts_, dst_pts_)\n","\n","    if rank == 3:\n","        tfm = np.float32([\n","            [A[0, 0], A[1, 0], A[2, 0]],\n","            [A[0, 1], A[1, 1], A[2, 1]]\n","        ])\n","    elif rank == 2:\n","        tfm = np.float32([\n","            [A[0, 0], A[1, 0], 0],\n","            [A[0, 1], A[1, 1], 0]\n","        ])\n","\n","    return tfm\n","\n","\n","def warp_and_crop_face(src_img,\n","                       facial_pts,\n","                       reference_pts=None,\n","                       crop_size=(96, 112),\n","                       align_type='smilarity'):\n","    if reference_pts is None:\n","        if crop_size[0] == 96 and crop_size[1] == 112:\n","            reference_pts = REFERENCE_FACIAL_POINTS\n","        else:\n","            default_square = False\n","            inner_padding_factor = 0\n","            outer_padding = (0, 0)\n","            output_size = crop_size\n","\n","            reference_pts = get_reference_facial_points(output_size,\n","                                                        inner_padding_factor,\n","                                                        outer_padding,\n","                                                        default_square)\n","\n","    ref_pts = np.float32(reference_pts)\n","    ref_pts_shp = ref_pts.shape\n","    if max(ref_pts_shp) < 3 or min(ref_pts_shp) != 2:\n","        raise FaceWarpException(\n","            'reference_pts.shape must be (K,2) or (2,K) and K>2')\n","\n","    if ref_pts_shp[0] == 2:\n","        ref_pts = ref_pts.T\n","\n","    src_pts = np.float32(facial_pts)\n","    src_pts_shp = src_pts.shape\n","    if max(src_pts_shp) < 3 or min(src_pts_shp) != 2:\n","        raise FaceWarpException(\n","            'facial_pts.shape must be (K,2) or (2,K) and K>2')\n","\n","    if src_pts_shp[0] == 2:\n","        src_pts = src_pts.T\n","\n","    if src_pts.shape != ref_pts.shape:\n","        raise FaceWarpException(\n","            'facial_pts and reference_pts must have the same shape')\n","\n","    if align_type == 'cv2_affine':\n","        tfm = cv2.getAffineTransform(src_pts[0:3], ref_pts[0:3])\n","    #        print('cv2.getAffineTransform() returns tfm=\\n' + str(tfm))\n","    elif align_type == 'affine':\n","        tfm = get_affine_transform_matrix(src_pts, ref_pts)\n","    #        print('get_affine_transform_matrix() returns tfm=\\n' + str(tfm))\n","    else:\n","        # tfm = get_similarity_transform_for_cv2(src_pts, ref_pts)\n","        tform = trans.SimilarityTransform()\n","        tform.estimate(src_pts, ref_pts)\n","        tfm = tform.params[0:2, :]\n","\n","    face_img = cv2.warpAffine(src_img, tfm, (crop_size[0], crop_size[1]))\n","\n","    return face_img"],"metadata":{"id":"rIRmpfEZz66c","executionInfo":{"status":"ok","timestamp":1687164021617,"user_tz":-420,"elapsed":541,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["!mkdir dataset/train_crop\n","!mkdir dataset/val_crop"],"metadata":{"id":"KQHE80kz012M","executionInfo":{"status":"ok","timestamp":1687164261971,"user_tz":-420,"elapsed":408,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["for path in os.listdir('/content/dataset/train'):\n","  # try:\n","  #   os.chdir(f'/content/dataset/train_crop/{path}')\n","  # except:\n","  os.mkdir(f'/content/dataset/train_crop/{path}')\n","  os.chdir(f'/content/dataset/train_crop/{path}')\n","  for link in os.listdir('/content/dataset/train/' + path):\n","    try:\n","      originalImage = cv2.imread(f'/content/dataset/train/{path}/{link}')\n","      landmarks,rec = facial_landmarks(originalImage)\n","      (x, y, w, h) = face_utils.rect_to_bb(rec[0])\n","      a = [landmarks[17],landmarks[26],landmarks[33],landmarks[4],landmarks[12]]\n","      img = warp_and_crop_face(originalImage,a,reference_pts=None,crop_size=(96,112),align_type='')\n","      cv2.imwrite(f'{link}', img)\n","    except:\n","      continue"],"metadata":{"id":"HtdDMEm10GKf","executionInfo":{"status":"ok","timestamp":1687164658789,"user_tz":-420,"elapsed":2842,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["for path in os.listdir('/content/dataset/val'):\n","  # try:\n","  #   os.chdir(f'/content/dataset/train_crop/{path}')\n","  # except:\n","  os.mkdir(f'/content/dataset/val_crop/{path}')\n","  os.chdir(f'/content/dataset/val_crop/{path}')\n","  for link in os.listdir('/content/dataset/val/' + path):\n","    try:\n","      originalImage = cv2.imread(f'/content/dataset/val/{path}/{link}')\n","      landmarks,rec = facial_landmarks(originalImage)\n","      (x, y, w, h) = face_utils.rect_to_bb(rec[0])\n","      a = [landmarks[17],landmarks[26],landmarks[33],landmarks[4],landmarks[12]]\n","      img = warp_and_crop_face(originalImage,a,reference_pts=None,crop_size=(96,112),align_type='')\n","      cv2.imwrite(f'{link}', img)\n","    except:\n","      continue"],"metadata":{"id":"j_fbOYfl2jb6","executionInfo":{"status":"ok","timestamp":1687164730028,"user_tz":-420,"elapsed":1147,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["dataset_train = datasets.ImageFolder(root=\"/content/dataset/train_crop\")\n","dataset_val = datasets.ImageFolder(root=\"/content/dataset/val_crop\")\n","\n","X_train, y_train = dataset_to_embeddings(dataset_train, mtcnn, facenet)\n","X_test, y_test = dataset_to_embeddings(dataset_val, mtcnn, facenet)\n","\n","X_train_class_idx = dataset_train.class_to_idx\n","X_test_class_idx = dataset_val.class_to_idx\n","\n","embeddings, labels, class_to_idx = X_train, y_train, X_train_class_idx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kMND1OSU2402","executionInfo":{"status":"ok","timestamp":1687164960120,"user_tz":-420,"elapsed":16098,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"90c8d827-bd6e-418e-ba6a-e69358ddad0b"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/dataset/train_crop/ben_afflek/httpcsvkmeuaeccjpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpimagesfandangocomrImageRendererredesignstaticimgnoxportraitjpgpcpcpcimagesmasterrepositoryperformerimagespjpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpssmediacacheakpinimgcomxdbbdbbbececacdecdcdfjpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpssmediacacheakpinimgcomxdfdfadcfeabjpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpssmediacacheakpinimgcomxedaedabcbefbcbabbjpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpssmediacacheakpinimgcomxeebdfdbaaajpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpsuploadwikimediaorgwikipediacommonsthumbddBenAffleckbyGageSkidmorejpgpxBenAffleckbyGageSkidmorejpg.jpg\n","/content/dataset/train_crop/ben_afflek/httptrwebimgacstanetcxbdddmediasnmediajpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpwwwaceshowbizcomimagesphotobenaffleckjpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpwwwallposterscomimagesPostersPFjpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpwwwfilmscoopitcgibinattoriBENAFFLECKBENAFFLECKjpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpwwwhillsindcomstorebenjpg.jpg\n","/content/dataset/train_crop/ben_afflek/httpwwwrumenewscomwpcontentuploadsbenafflekxpng.jpg\n","/content/dataset/train_crop/elton_john/httpftqncomymusicLxZeltonjohnjpg.jpg\n","/content/dataset/train_crop/elton_john/httpiamediaimdbcomimagesMMVBMTAxNDUMzUwOTdeQTJeQWpwZBbWUMDUOTAyNTIVUXCRALjpg.jpg\n","/content/dataset/train_crop/elton_john/httpimagesmtvcomurimgidfiledocrootvhcomsitewideflipbooksimgdailyjpgenlargefalsemattetruematteColorblackquality.jpg\n","/content/dataset/train_crop/elton_john/httpimggalpmdstaticnetfithttpAFFwwwEgalaEfrFvarFgalFstorageFimagesFmediaFmultiuploaddufevrierFeltonjohnFfreFRFeltonjohnEjpgxqualityeltonjohnjpg.jpg\n","/content/dataset/train_crop/elton_john/httpmediacacheecpinimgcomxffeffbcefjpg.jpg\n","/content/dataset/train_crop/elton_john/httpmediapopsugarassetscomfilescbffewltonjpg.jpg\n","/content/dataset/train_crop/elton_john/httpspmctvlinefileswordpresscomeltonjohnjpg.jpg\n","/content/dataset/train_crop/elton_john/httpssmediacacheakpinimgcomxfcfcbaeaeddabbjpg.jpg\n","/content/dataset/train_crop/elton_john/httpssmediacacheakpinimgcomxfecfecaefaadfebejpg.jpg\n","/content/dataset/train_crop/elton_john/httpssmediacacheakpinimgcomxfefdacfbfdeadajpg.jpg\n","/content/dataset/train_crop/elton_john/httpsuploadwikimediaorgwikipediacommonsthumbEltonJohninsjpgpxEltonJohninsjpg.jpg\n","/content/dataset/train_crop/elton_john/httpwwwdeadlinecomvimgnetwpcontentuploadseltonjpg.jpg\n","/content/dataset/train_crop/elton_john/httpwwweonlinecomresizewwweonlinecomeolimagesEntireSitersxEltonJohnJRjpg.jpg\n","/content/dataset/train_crop/elton_john/httpwwwjohnpauljonesarenacomeventimagesEltonCalendarVjpg.jpg\n","/content/dataset/train_crop/elton_john/httpwwwlautdeEltonJohneltonjohnjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpgraphicsnytimescomimagessectionmoviesfilmographyWireImagejpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpikinjaimgcomgawkermediaimageuploadsWmIuhdsrcedidjpgjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpimagescontactmusiccomnewsimagesjerryseinfeldjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpimagesrottentomatoescomimagesspotlightsnewsjerryseinfeldjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpimgthedrumcomsfspublicnewstmpjerryseinfeldjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpimgtimeincnetpeopleinewsjerryseinfeldjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpmediapopsugarassetscomfilesusersxlargejpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httppfodcompchannelslegacyprofilejerryseinfeldpodcastjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpsmedialivenationcomartiststapjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpsrobertaccetturacomwpcontentuploadsjerryseinfeldheadshotjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpstatictherichestimagescomcdncwpcontentuploadsJerrySeinfeldjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpsticketmnettmenusdbimagesajpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpsuploadwikimediaorgwikipediacommonsthumbbJerrySeinfeldjpgpxJerrySeinfeldjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpsuswestamazonawscomblogsprodmediausuploadsJerrySeinfeldkidsxjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpswwwticketscomuploadsartistsjerryseinfeldbilletsjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpwwweonlinecomresizewwweonlinecomeolimagesEntireSitersxjerryseinfeldjulialouisdreyfusjwjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpwwwnndbcompeoplejerryseinfeldaujpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpwwwpictureszimbiocomgpJerrySeinfeldJessicaSeinfeldmarriedBczSipMdNMQljpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpwwwpsychiatrictimescomsitesdefaultfilesimagesmediaPTMofficSeinfeldjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpwwwslatecomcontentdamslateblogsbrowbeatseinfeldjpgCROParticlemediumjpg.jpg\n","/content/dataset/train_crop/jerry_seinfeld/httpwwwwatchidcomsitesdefaultfilesuploadsightingBreitlingwatchJerrySeinfeldjpg.jpg\n","/content/dataset/train_crop/madonna/httpiamediaimdbcomimagesMMVBMTANDQNTAxNDVeQTJeQWpwZBbWUMDIMjQOTYVUXCRALjpg.jpg\n","/content/dataset/train_crop/madonna/httpimagegaladevcmseamadonnaprivatdetektivsquaretopsquarejpgv.jpg\n","/content/dataset/train_crop/madonna/httpimgclosermagfrvarclosermagstorageimagesactupeoplebiodestarsmadonnamadonnafreFRmadonnaexactxljpg.jpg\n","Multiple faces detected for /content/dataset/train_crop/madonna/httpimgclosermagfrvarclosermagstorageimagesactupeoplebiodestarsmadonnamadonnafreFRmadonnaexactxljpg.jpg, taking one with highest probability\n","/content/dataset/train_crop/madonna/httpmediavoguecomrwblondesdarkbrowsmadonnajpg.jpg\n","/content/dataset/train_crop/madonna/httpresizeparismatchladmediafrrffffffcentermiddleimgvarnewsstorageimagesparismatchpeopleazmadonnafreFRMadonnajpg.jpg\n","/content/dataset/train_crop/madonna/httpssmediacacheakpinimgcomxaaeaaeecccaedfebdbjpg.jpg\n","/content/dataset/train_crop/madonna/httpssmediacacheakpinimgcomxdcfdcfedfaedadjpg.jpg\n","/content/dataset/train_crop/madonna/httpssmediacacheakpinimgcomxfeebfdccajpg.jpg\n","/content/dataset/train_crop/madonna/httpssmediacacheakpinimgcomxffabffabbbcfbceaedjpg.jpg\n","/content/dataset/train_crop/madonna/httpssmediacacheakpinimgcomxffeabacaaejpg.jpg\n","/content/dataset/train_crop/madonna/httpssmediacacheakpinimgcomxffecfafddjpg.jpg\n","/content/dataset/train_crop/madonna/httpssvagalumecommadonnaimagesmadonnajpg.jpg\n","/content/dataset/train_crop/madonna/httpsuploadwikimediaorgwikipediacommonsMadonnathAnnualGoldenGlobesAwardscroppedjpg.jpg\n","/content/dataset/train_crop/madonna/httpsuploadwikimediaorgwikipediacommonsthumbaaMadonnaatthepremiereofIAmBecauseWeArejpgpxMadonnaatthepremiereofIAmBecauseWeArejpg.jpg\n","/content/dataset/train_crop/madonna/httpuploadwikimediaorgwikipediacommonsthumbaMadonnaRotterdamjpgpxMadonnaRotterdamjpg.jpg\n","/content/dataset/train_crop/madonna/httpwwwbeastiemaniacomwhoismadonnamadonnajpg.jpg\n","/content/dataset/train_crop/madonna/httpwwwblackdogfilmscomwordpresswpcontentuploadsmadonnacelebrationxjpg.jpg\n","/content/dataset/train_crop/madonna/httpwwwetonlinecomphotomadonnabdayjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpgonetworthcomwpcontentuploadsthumbsjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpgraphicsnytimescomimagesmagazinekalingkalingarticleInlinejpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpimagesnymagcomimagesdailymindykalingxjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpmediaonsugarcomfilesbabaaaaaMindyInterviewxxxlargejpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httppixelnymagcomimgsfashiondailymindykalingwhjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpsamazonawscomkidzworldphotoimagesbccdddaaadgallerymindykalinggalleryjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpsimagesnasslimagesamazoncomimagesIAQdwKvFILUXjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpsimgbuzzfeedcombuzzfeedstaticstaticcampaignimageswebdradorableetsyitemsallmindykalingfansneedbigjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpssmediacacheakpinimgcomxafaffbbbbdbcbdddjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpssmediacacheakpinimgcomxbabafeacbaaacabffjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpssmediacacheakpinimgcomxbbfcbdedfbfdbjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpssmediacacheakpinimgcomxcccccddbbfedabjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpssmediacacheakpinimgcomxccccdabeaadjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpssmediacacheakpinimgcomxededbebccdajpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpsuploadwikimediaorgwikipediacommonsthumbMindyBKalingBPaleyFestBNewBYorkBBMindyBIqZgUWvbljpgpxMindyBKalingBPaleyFestBNewBYorkBBMindyBIqZgUWvbljpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpwwwadweekcomfilesimagecachenodeinlinemindykalingprofilehedjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpwwwbooksforbetterlivingcomwpcontentuploadsMindyKalingAuthorPhotojpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpwwweonlinecomeolimagesEntireSiteregKalingCancermhjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpwwwimpropercomimagesuploadsusercontentimagesSoroffMindyjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpwwwpictureszimbiocomfpMindyKalingfBgaONVFDFmjpg.jpg\n","/content/dataset/train_crop/mindy_kaling/httpwwwpictureszimbiocomgiMindyKalingLkvEhTwDeJmjpg.jpg\n","/content/dataset/val_crop/ben_afflek/httpabsolumentgratuitfreefrimagesbenaffleckjpg.jpg\n","/content/dataset/val_crop/ben_afflek/httpafilesbiographycomimageuploadcfillcssrgbdprgfacehqwMTENDgMDUODczNDcNTcjpg.jpg\n","/content/dataset/val_crop/ben_afflek/httpbpblogspotcomedLMjVpRGkSWexgsXjkNIAAAAAAAADWgFFtAUqBlhAsjpg.jpg\n","/content/dataset/val_crop/ben_afflek/httpcsvkmeuadecafjpg.jpg\n","/content/dataset/val_crop/elton_john/httpafilesbiographycomimageuploadcfillcssrgbdprgfacehqwMTEODAOTcxNjcMjczMjkzjpg.jpg\n","/content/dataset/val_crop/elton_john/httpcdncdnjustjaredcomwpcontentuploadsheadlineseltonjohnemmysperformancewatchnowjpg.jpg\n","/content/dataset/val_crop/elton_john/httpcdncdnjustjaredcomwpcontentuploadsheadlineseltonjohnstillstandingbrooklynnewyearsjpg.jpg\n","/content/dataset/val_crop/elton_john/httpcdncdnjustjaredcomwpcontentuploadsheadlineseltonjohnsupportsbrucejennerstransitiontowomanjpg.jpg\n","/content/dataset/val_crop/elton_john/httpcdnlyricssongonlyricsnetwpcontentuploadsEltonJohnDiscographyCDreleasesjpg.jpg\n","/content/dataset/val_crop/jerry_seinfeld/httpafilesbiographycomimageuploadcfillcssrgbdprgfacehqwMTIwNjANjMMDINzIxNjcjpg.jpg\n","/content/dataset/val_crop/jerry_seinfeld/httpaurorasblogcomwpcontentuploadsjerryseinfeldpublicityshotjpg.jpg\n","/content/dataset/val_crop/jerry_seinfeld/httpblognjcomentertainmentimpactcelebritiesmediumjerrybjpg.jpg\n","/content/dataset/val_crop/jerry_seinfeld/httpcdncdnjustjaredcomwpcontentuploadsheadlinesjerryseinfeldmakesbrianwilliamsjokejpg.jpg\n","/content/dataset/val_crop/jerry_seinfeld/httpcdnssninsidercomwpcontentuploadsjerryseinfeldxjpg.jpg\n","/content/dataset/val_crop/madonna/httpassetsrollingstonecomassetsarticlemadonnadavidbowiechangedthecourseofmylifeforeversmallsquarexmadonnabowiejpg.jpg\n","/content/dataset/val_crop/madonna/httpassetsrollingstonecomassetsimagesalbumreviewaffaceabdcccaeedjpg.jpg\n","/content/dataset/val_crop/madonna/httpcdncdnjustjaredcomwpcontentuploadsheadlinesmadonnatalksparisattackstearsjpg.jpg\n","/content/dataset/val_crop/madonna/httpcdnfuncheapcomwpcontentuploadsVOGUEjpg.jpg\n","/content/dataset/val_crop/madonna/httpecximagesamazoncomimagesIfmaBKWLACULSRjpg.jpg\n","/content/dataset/val_crop/mindy_kaling/httpafilesbiographycomimageuploadcfillcssrgbdprgfacehqwMTIOTcwODQNTUzNjQMzcjpg.jpg\n","/content/dataset/val_crop/mindy_kaling/httpcdncdnjustjaredcomwpcontentuploadsheadlinesmindykalingcomedypilotjpg.jpg\n","/content/dataset/val_crop/mindy_kaling/httpcdnpastemagazinecomwwwarticlesmindykalingndbookjpg.jpg\n","/content/dataset/val_crop/mindy_kaling/httpcdnpastemagazinecomwwwarticlesmindyprojectjpg.jpg\n","/content/dataset/val_crop/mindy_kaling/httpdbrbzkkbdsdcloudfrontnetwpcontentuploadsMindyKalingjpg.jpg\n"]}]},{"cell_type":"code","source":["!rm -r /content/facenet_emb\n","!mkdir /content/facenet_emb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pN_Ggw73lJ1","executionInfo":{"status":"ok","timestamp":1687165129244,"user_tz":-420,"elapsed":322,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"acaeb4ce-294f-429f-9fee-54d96d93bf08"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/facenet_emb': No such file or directory\n"]}]},{"cell_type":"code","source":["np.save('/content/facenet_emb/facenet.npy', embeddings)"],"metadata":{"id":"IGShk0M13o1w","executionInfo":{"status":"ok","timestamp":1687165136003,"user_tz":-420,"elapsed":303,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["!pip install autofaiss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-p4Gyw-4V8n","executionInfo":{"status":"ok","timestamp":1687165179061,"user_tz":-420,"elapsed":9154,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"734c3db8-dfac-49ac-fb2f-c653db9b5517"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting autofaiss\n","  Downloading autofaiss-2.15.8-py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fire<0.5.0,>=0.4.0 (from autofaiss)\n","  Downloading fire-0.4.0.tar.gz (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy<2,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (1.22.4)\n","Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (1.5.3)\n","Requirement already satisfied: pyarrow<13,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (9.0.0)\n","Requirement already satisfied: tqdm<5,>=4.62.3 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (4.65.0)\n","Requirement already satisfied: fsspec>=2022.1.0 in /usr/local/lib/python3.10/dist-packages (from autofaiss) (2023.4.0)\n","Collecting embedding-reader<2,>=1.5.1 (from autofaiss)\n","  Downloading embedding_reader-1.5.1-py3-none-any.whl (18 kB)\n","Collecting faiss-cpu<2,>=1 (from autofaiss)\n","  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire<0.5.0,>=0.4.0->autofaiss) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire<0.5.0,>=0.4.0->autofaiss) (2.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->autofaiss) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->autofaiss) (2022.7.1)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115927 sha256=f5a289ca1ee4aba6fc5b8dcb390a17147eedc6d40bc2749f91f82d5e275387b7\n","  Stored in directory: /root/.cache/pip/wheels/26/9a/dd/2818b1b023daf077ec3e625c47ae446aca587a5abe48e05212\n","Successfully built fire\n","Installing collected packages: faiss-cpu, fire, embedding-reader, autofaiss\n","Successfully installed autofaiss-2.15.8 embedding-reader-1.5.1 faiss-cpu-1.7.4 fire-0.4.0\n"]}]},{"cell_type":"code","source":["!autofaiss build_index --embeddings=\"/content/facenet_emb\" \\\n","                    --index_path=\"/content/knn_facenet.index\" \\\n","                    --index_infos_path=\"/content/infos_facenet.json\" \\\n","                    --metric_type=\"ip\" \\\n","                    --max_index_query_time_ms=10 \\\n","                    --max_index_memory_usage=\"4GB\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lI0fBIAQ3td2","executionInfo":{"status":"ok","timestamp":1687165192299,"user_tz":-420,"elapsed":1395,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"66abf008-5870-41ef-e567-d58e50ca2215"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-19 08:59:51,070 [INFO]: Using 2 omp threads (processes), consider increasing --nb_cores if you have more\n","2023-06-19 08:59:51,075 [INFO]: Launching the whole pipeline 06/19/2023, 08:59:51\n","2023-06-19 08:59:51,075 [INFO]: Reading total number of vectors and dimension 06/19/2023, 08:59:51\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 16644.06it/s]\n","2023-06-19 08:59:51,118 [INFO]: There are 88 embeddings of dim 512\n","2023-06-19 08:59:51,118 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0435 secs\n","2023-06-19 08:59:51,119 [INFO]: \tCompute estimated construction time of the index 06/19/2023, 08:59:51\n","2023-06-19 08:59:51,119 [INFO]: \t\t-> Train: 16.7 minutes\n","2023-06-19 08:59:51,120 [INFO]: \t\t-> Add: 0.0 seconds\n","2023-06-19 08:59:51,120 [INFO]: \t\tTotal: 16.7 minutes\n","2023-06-19 08:59:51,120 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0003 secs\n","2023-06-19 08:59:51,120 [INFO]: \tChecking that your have enough memory available to create the index 06/19/2023, 08:59:51\n","2023-06-19 08:59:51,121 [INFO]: 193.6KB of memory will be needed to build the index (more might be used if you have more)\n","2023-06-19 08:59:51,121 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0009 secs\n","2023-06-19 08:59:51,121 [INFO]: \tSelecting most promising index types given data characteristics 06/19/2023, 08:59:51\n","2023-06-19 08:59:51,121 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n","2023-06-19 08:59:51,121 [INFO]: \tCreating the index 06/19/2023, 08:59:51\n","2023-06-19 08:59:51,121 [INFO]: \t\t-> Instanciate the index Flat 06/19/2023, 08:59:51\n","2023-06-19 08:59:51,121 [INFO]: \t\t>>> Finished \"-> Instanciate the index Flat\" in 0.0002 secs\n","2023-06-19 08:59:51,121 [INFO]: \t\t-> Adding the vectors to the index 06/19/2023, 08:59:51\n","2023-06-19 08:59:51,121 [INFO]: The memory available for adding the vectors is 32.0GB(total available - used by the index)\n","2023-06-19 08:59:51,121 [INFO]: Using a batch size of 488281 (memory overhead 953.7MB)\n","\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 14.54it/s]\n","2023-06-19 08:59:51,195 [INFO]: \tComputing best hyperparameters for index /content/knn_facenet.index 06/19/2023, 08:59:51\n","2023-06-19 08:59:51,195 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /content/knn_facenet.index\" in 0.0000 secs\n","2023-06-19 08:59:51,195 [INFO]: The best hyperparameters are: \n","2023-06-19 08:59:51,195 [INFO]: \tCompute fast metrics 06/19/2023, 08:59:51\n","\r  0% 0/1 [00:00<?, ?it/s]\r  0% 0/1 [00:00<?, ?it/s]\n","2023-06-19 08:59:51,263 [INFO]: \t>>> Finished \"Compute fast metrics\" in 0.0681 secs\n","2023-06-19 08:59:51,263 [INFO]: \tSaving the index on local disk 06/19/2023, 08:59:51\n","2023-06-19 08:59:51,264 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.0009 secs\n","2023-06-19 08:59:51,264 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 0.1432 secs\n","2023-06-19 08:59:51,265 [INFO]: {\n","2023-06-19 08:59:51,265 [INFO]: \tindex_key: Flat\n","2023-06-19 08:59:51,265 [INFO]: \tindex_param: \n","2023-06-19 08:59:51,265 [INFO]: \tindex_path: /content/knn_facenet.index\n","2023-06-19 08:59:51,265 [INFO]: \tsize in bytes: 180269\n","2023-06-19 08:59:51,265 [INFO]: \tavg_search_speed_ms: 0.025341795950525058\n","2023-06-19 08:59:51,265 [INFO]: \t99p_search_speed_ms: 0.04651319994991354\n","2023-06-19 08:59:51,265 [INFO]: \treconstruction error %: 0.0\n","2023-06-19 08:59:51,265 [INFO]: \tnb vectors: 88\n","2023-06-19 08:59:51,265 [INFO]: \tvectors dimension: 512\n","2023-06-19 08:59:51,265 [INFO]: \tcompression ratio: 0.9997503730536033\n","2023-06-19 08:59:51,265 [INFO]: }\n","2023-06-19 08:59:51,265 [INFO]: \t>>> Finished \"Creating the index\" in 0.1441 secs\n","2023-06-19 08:59:51,265 [INFO]: >>> Finished \"Launching the whole pipeline\" in 0.1902 secs\n","(<faiss.swigfaiss_avx2.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x7f013c490840> >, {'index_key': 'Flat', 'index_param': '', 'index_path': '/content/knn_facenet.index', 'size in bytes': 180269, 'avg_search_speed_ms': 0.025341795950525058, '99p_search_speed_ms': 0.04651319994991354, 'reconstruction error %': 0.0, 'nb vectors': 88, 'vectors dimension': 512, 'compression ratio': 0.9997503730536033})\n"]}]},{"cell_type":"code","source":["import faiss\n","import torch\n","import os\n","import pandas as pd\n","from collections import defaultdict"],"metadata":{"id":"kAtN9C1N4Cde","executionInfo":{"status":"ok","timestamp":1687165262291,"user_tz":-420,"elapsed":2,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["ind = faiss.read_index(\"/content/knn_facenet.index\")"],"metadata":{"id":"3DFMfuWg4ckM","executionInfo":{"status":"ok","timestamp":1687165222496,"user_tz":-420,"elapsed":317,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["print(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61phmK1j4kDx","executionInfo":{"status":"ok","timestamp":1687165227600,"user_tz":-420,"elapsed":534,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"ffda8bb3-ac01-4bd8-bc85-3b06f52523d7"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"]}]},{"cell_type":"code","source":["preds = []\n","k = 5\n","for image_emb in X_test:\n","    D, I = ind.search(image_emb[np.newaxis, :], k)\n","    print(D,I)\n","    i_candidate = defaultdict(int)\n","    for D_ele,I_ele in zip(D[0],I[0]):\n","      # if D_ele > 0.7:\n","        cls = labels[I_ele]\n","        i_candidate[cls] += 1\n","    # try:\n","    key_with_max_value = max(i_candidate, key=lambda k: i_candidate[k])\n","    # except:\n","    #   print(\"error\")\n","    #   key_with_max_value = 0\n","\n","    preds.append(key_with_max_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BEbXIwh4mA3","executionInfo":{"status":"ok","timestamp":1687167195968,"user_tz":-420,"elapsed":327,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"7cd89bf5-68ca-472c-8dce-4d84717556dc"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.63068545 0.6127332  0.5875063  0.57932496 0.51305753]] [[8 9 0 5 1]]\n","[[0.73820114 0.69468373 0.6858582  0.6767702  0.65058136]] [[ 6  1  9 12  4]]\n","[[0.8709558  0.61281395 0.6028794  0.57177866 0.5668622 ]] [[ 5  1  8  7 11]]\n","[[0.6204376  0.61376035 0.56622636 0.53639716 0.510901  ]] [[ 5  9  8 12  7]]\n","[[0.77670765 0.7425232  0.73400223 0.6831244  0.6380301 ]] [[19 18 14 25 16]]\n","[[0.6597472  0.62895    0.6266669  0.62017846 0.61965   ]] [[27 26 17 25 18]]\n","[[0.82673585 0.80561006 0.7178046  0.7118243  0.6581554 ]] [[18 19 25 26 14]]\n","[[0.7062645  0.6421448  0.60937154 0.60728216 0.5656978 ]] [[25 15 14 19 18]]\n","[[0.69252634 0.64726985 0.6102964  0.6094415  0.53007907]] [[18 19 14 25 15]]\n","[[0.76592195 0.7546424  0.7489149  0.7452918  0.6926686 ]] [[32 46 28 45 30]]\n","[[0.70690143 0.6419416  0.6360741  0.62480783 0.6215017 ]] [[30 42 28 44 40]]\n","[[0.8042834  0.79666233 0.77691996 0.7667087  0.7624382 ]] [[30 33 40 32 42]]\n","[[0.80708575 0.7890631  0.7653747  0.73917973 0.72221076]] [[28 31 36 32 46]]\n","[[0.79021776 0.7384831  0.7290894  0.7265099  0.72078824]] [[28 46 30 32 42]]\n","[[0.7566931  0.691024   0.616025   0.4256423  0.41799846]] [[50 53 49 64 59]]\n","[[0.7295561 0.7112799 0.6099361 0.5674832 0.5672144]] [[57 21 23 26 27]]\n","[[0.80879116 0.7809932  0.7579564  0.6326591  0.56173396]] [[53 49 50 59 66]]\n","[[0.81743276 0.7458898  0.70547116 0.69036746 0.6727818 ]] [[55 57 56 54 63]]\n","[[0.5715395  0.47814244 0.47069603 0.46695194 0.46630734]] [[56 23 52 54 57]]\n","[[0.73420614 0.68710655 0.6838681  0.68298304 0.68202186]] [[67 84 82 71 87]]\n","[[0.879969   0.8784351  0.8638642  0.76664174 0.7625413 ]] [[84 71 68 70 86]]\n","[[0.8391449  0.79140365 0.7596763  0.7575969  0.7532422 ]] [[83 72 68 82 71]]\n","[[0.8007841  0.7873894  0.7714335  0.76540285 0.7613369 ]] [[71 84 70 78 86]]\n","[[0.6815539 0.6271018 0.624369  0.5957341 0.5746479]] [[67 70 72 83 71]]\n"]}]},{"cell_type":"code","source":["from sklearn import metrics\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"1l6Lj5YS469p","executionInfo":{"status":"ok","timestamp":1687165319685,"user_tz":-420,"elapsed":414,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["test_class_idx = dataset_val.class_to_idx\n","\n","idx_to_class = {v: k for k, v in test_class_idx.items()}\n","print(idx_to_class)\n","\n","target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(target_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DaWdZtRe5Cio","executionInfo":{"status":"ok","timestamp":1687165650019,"user_tz":-420,"elapsed":4,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"57f72048-b52e-46a3-e67a-27ad88c0749b"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'ben_afflek', 1: 'elton_john', 2: 'jerry_seinfeld', 3: 'madonna', 4: 'mindy_kaling'}\n","['ben_afflek', 'elton_john', 'jerry_seinfeld', 'madonna', 'mindy_kaling']\n"]}]},{"cell_type":"code","source":["target_names = list(map(lambda i: i[1], sorted(idx_to_class.items(), key=lambda i: i[0])))\n","print(metrics.classification_report(y_test, preds, target_names=target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W3UaSyOr4q_V","executionInfo":{"status":"ok","timestamp":1687167200312,"user_tz":-420,"elapsed":359,"user":{"displayName":"PHÚ NGUYỄN ĐẮC HOÀNG","userId":"05644926397827677174"}},"outputId":"3a9ac4a5-f399-45ab-abd3-15e4123c44b0"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["                precision    recall  f1-score   support\n","\n","    ben_afflek       1.00      1.00      1.00         4\n","    elton_john       0.83      1.00      0.91         5\n","jerry_seinfeld       1.00      1.00      1.00         5\n","       madonna       1.00      0.80      0.89         5\n","  mindy_kaling       1.00      1.00      1.00         5\n","\n","      accuracy                           0.96        24\n","     macro avg       0.97      0.96      0.96        24\n","  weighted avg       0.97      0.96      0.96        24\n","\n"]}]}]}